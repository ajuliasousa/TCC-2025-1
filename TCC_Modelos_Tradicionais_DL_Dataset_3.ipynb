{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajuliasousa/TCC-2025-1/blob/main/TCC_Modelos_Tradicionais_DL_Dataset_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN-GPL6RJ9D6"
      },
      "source": [
        "### **Panorama Geral do Trabalho**\n",
        "\n",
        "**Título:**\n",
        "\n",
        "Avaliação Comparativa de Modelos de Machine Learning e Deep Learning para Detecção de Discurso de Ódio com Diferentes Técnicas de Representação e Balanceamento de Dados\n",
        "\n",
        "**Objetivo Geral:**\n",
        "\n",
        "Avaliar e comparar o desempenho de diferentes algoritmos de classificação, incluindo modelos tradicionais de Machine Learning (ML) e arquiteturas de Deep Learning (DL), na tarefa de detecção de discurso de ódio em dados textuais, analisando o impacto de distintas abordagens de representação vetorial e técnicas de balanceamento de classes.\n",
        "\n",
        "**Objetivos Específicos:**\n",
        "\n",
        "1.\tRealizar o pré-processamento e a limpeza de um corpus textual rotulado para discurso de ódio.\n",
        "2.\tRepresentar os dados textuais por meio de TF-IDF e embeddings (pré-treinados ou ajustados).\n",
        "3.\tTreinar e avaliar modelos de ML com TF-IDF: Naive Bayes, Regressão Logística, SVM, Random Forest, LightGBM, MLP.\n",
        "4.\tPreparar dados com tokenização, vocabulário e padding para modelos DL com embeddings.\n",
        "5.\tTreinar redes profundas: CNN e LSTM com embeddings.\n",
        "6.\tAplicar diferentes estratégias de balanceamento de dados: oversampling, undersampling e uso de class_weight.\n",
        "7.\tAvaliar os modelos usando métricas: F1-score ponderado, AUC e acurácia.\n",
        "8.\tComparar sistematicamente o desempenho dos modelos em diferentes cenários e técnicas de representação.\n",
        "9.\tDiscutir os impactos do tipo de vetorização e do balanceamento nos resultados obtidos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUUTFj-_KATf"
      },
      "source": [
        "### **Dataset 3 : Hate Speech Detection**\n",
        "\n",
        "**Link:** https://www.kaggle.com/datasets/waalbannyantudre/hate-speech-detection-curated-dataset/data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este projeto também utiliza o **\"Hate Speech Detection Curated Dataset\"**, disponibilizado no Kaggle, que foi elaborado com o objetivo de refletir as atuais tendências de linguagem presentes em plataformas de redes sociais, onde a propagação de discursos de ódio ocorre frequentemente por meio de conteúdo textual. O dataset reúne sentenças curtas em inglês anotadas em duas classes: **\"0\" para conteúdo não odioso e \"1\" para conteúdo considerado discurso de ódio**.\n",
        "\n",
        "Um dos principais diferenciais deste conjunto de dados é a incorporação de elementos típicos da comunicação online, como emojis, emoticons, hashtags, gírias e contrações — o que o torna especialmente relevante para aplicações práticas em ambientes digitais contemporâneos. Por ser um corpus curado e filtrado, ele pode ser utilizado com segurança em pesquisas e sistemas de detecção automática sem expor informações sensíveis ou identificáveis.\n",
        "\n",
        "Assim, o dataset representa uma fonte valiosa para o treinamento e a avaliação de modelos de aprendizado de máquina e técnicas de Processamento de Linguagem Natural (PLN) voltadas à detecção de discurso de ódio, além de poder servir como benchmark em estudos futuros sobre o tema."
      ],
      "metadata": {
        "id": "M8Cp-zfIjuB6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Uv2Rw1KNah"
      },
      "source": [
        "### **Bibliotecas e Visão do Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4hyC2lUKVdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a4447fe-d4e2-4510-d657-1abb24a71582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# bibliotecas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/TCC/Datasets/Hate Speech Detection/HateSpeechDataset.csv'\n",
        "df = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knalaEcsKXso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5269096f-0b53-4351-e57a-a30f99c9fab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset:\n",
            "Primeiras linhas:\n",
            "                                             Content Label  \\\n",
            "0  denial of normal the con be asked to comment o...     1   \n",
            "1  just by being able to tweet this insufferable ...     1   \n",
            "2  that is retarded you too cute to be single tha...     1   \n",
            "3  thought of a real badass mongol style declarat...     1   \n",
            "4                                afro american basho     1   \n",
            "\n",
            "                                         Content_int  \n",
            "0  [146715, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,...  \n",
            "1  [146715, 14, 15, 16, 17, 7, 18, 19, 20, 21, 22...  \n",
            "2  [146715, 28, 29, 30, 26, 31, 32, 7, 5, 33, 28,...  \n",
            "3  [146715, 35, 1, 24, 36, 37, 38, 39, 40, 1, 41,...  \n",
            "4                       [146715, 46, 47, 48, 146714]  \n",
            "\n",
            "Informações:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 440906 entries, 0 to 440905\n",
            "Data columns (total 3 columns):\n",
            " #   Column       Non-Null Count   Dtype \n",
            "---  ------       --------------   ----- \n",
            " 0   Content      440906 non-null  object\n",
            " 1   Label        440906 non-null  object\n",
            " 2   Content_int  440906 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 10.1+ MB\n",
            "\n",
            "Descrição estatística:\n",
            "        Content   Label             Content_int\n",
            "count    440906  440906                  440906\n",
            "unique   417561       3                  417561\n",
            "top     content       0  [146715, 7139, 146714]\n",
            "freq          7  361594                       7\n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset:\")\n",
        "print(\"Primeiras linhas:\")\n",
        "print(df.head())\n",
        "print(\"\\nInformações:\")\n",
        "df.info()\n",
        "print(\"\\nDescrição estatística:\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O dataset analisado é composto por **440.906 observações**, cada uma representando uma sentença textual (coluna `Content`) anotada com um rótulo de classificação (`Label`) e uma representação numérica tokenizada (`Content_int`).\n",
        "\n",
        "A coluna `Label` possui **três valores distintos**, com predominância da classe `0`, que representa **conteúdos não ofensivos**, totalizando **361.594 ocorrências** (cerca de 82% dos dados). As colunas `Content` e `Content_int` apresentam **417.561 valores únicos**, indicando que a maior parte das mensagens e suas representações inteiras são distintas entre si, refletindo a diversidade textual do corpus.\n",
        "\n"
      ],
      "metadata": {
        "id": "oeUZNEC2kOFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# verificação de label\n",
        "print(\"\\nValores únicos na coluna 'Label':\", df['Label'].unique())\n",
        "\n",
        "# tratamento dos dados\n",
        "df = df[df['Label'].isin(['0', '1'])].copy() # mantém apenas linhas com labels '0' ou '1'\n",
        "\n",
        "# conversão\n",
        "df['Label'] = df['Label'].astype(int)\n",
        "\n",
        "print(\"\\nInformações do DataFrame após tratar a coluna 'Label':\")\n",
        "df.info()\n",
        "print(\"\\nContagem de valores na coluna 'Label' após tratamento:\")\n",
        "print(df['Label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8IK5NSL2BtT",
        "outputId": "1a167c6d-0b00-4d6c-d897-a36a30dfb54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Valores únicos na coluna 'Label': ['1' '0' 'Label']\n",
            "\n",
            "Informações do DataFrame após tratar a coluna 'Label':\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 440899 entries, 0 to 440905\n",
            "Data columns (total 3 columns):\n",
            " #   Column       Non-Null Count   Dtype \n",
            "---  ------       --------------   ----- \n",
            " 0   Content      440899 non-null  object\n",
            " 1   Label        440899 non-null  int64 \n",
            " 2   Content_int  440899 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 13.5+ MB\n",
            "\n",
            "Contagem de valores na coluna 'Label' após tratamento:\n",
            "Label\n",
            "0    361594\n",
            "1     79305\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes do tratamento, a coluna `Label` continha três valores únicos: `'1'`, `'0'` e `'Label'`, indicando a presença de uma string de cabeçalho indevidamente lida como dado. Após a correção, o dataset passou a conter **440.899 entradas válidas**, com os rótulos devidamente convertidos para o tipo `int64`.\n",
        "\n",
        "A distribuição das classes mostra uma forte desproporcionalidade: **361.594 amostras (≈82%)** foram rotuladas como **não odiosas (classe 0)**, enquanto **79.305 amostras (≈18%)** foram classificadas como **discurso de ódio (classe 1)**. Esse desequilíbrio justifica a necessidade do uso de técnicas de balanceamento durante o treinamento dos modelos."
      ],
      "metadata": {
        "id": "YYwU7Tpokmwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado o grande volume de dados presentes no dataset original — com mais de 440 mil observações —, optou-se pela criação de um **subconjunto estratificado de 25.000 amostras** para o desenvolvimento e experimentação dos modelos de machine learning.\n",
        "\n",
        "Essa abordagem visa **reduzir o custo computacional** durante as etapas de pré-processamento, vetorização e treinamento, permitindo **execução mais rápida** e iterativa dos experimentos, sem comprometer significativamente a representatividade das classes.\n",
        "\n",
        "O uso do parâmetro `stratify` ao aplicar `train_test_split` garante que o subconjunto preserve a **mesma proporção entre instâncias de discurso de ódio e conteúdo não odioso** encontrada no dataset completo. No subconjunto criado, foram selecionadas exatamente **20.503 instâncias da classe 0 (não odioso)** e **4.497 instâncias da classe 1 (discurso de ódio)**, refletindo fielmente a distribuição original das classes.\n"
      ],
      "metadata": {
        "id": "-gbpHLqwlOn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# recorte de subset por limitação computacional\n",
        "subset_size = 25000\n",
        "\n",
        "# verificação\n",
        "if subset_size > len(df):\n",
        "    print(\"O tamanho do subconjunto desejado é maior que o número total de linhas.\")\n",
        "    subset_size = len(df)\n",
        "    print(f\"Usando o tamanho total do dataset: {subset_size}\")\n",
        "\n",
        "# uso do train_test_split para selecionar um subconjunto estratificado.\n",
        "# divisão o dataset original em um conjunto de treino (que será nosso 'df_subset') e um conjunto de teste\n",
        "# a proporção para o conjunto de treino é subset_size / len(df).\n",
        "# uso do stratify=df['Label'] para garantir que a proporção das classes seja mantida.\n",
        "\n",
        "# uso do random_state para reprodutibilidade e stratify para garantir a proporção das classes.\n",
        "df_subset, df_rest = train_test_split(\n",
        "    df,\n",
        "    train_size=subset_size,\n",
        "    random_state=42,\n",
        "    stratify=df['Label']\n",
        ")\n",
        "\n",
        "# reindeixação\n",
        "df_subset = df_subset.reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nSubconjunto criado com {len(df_subset)} observações.\")\n",
        "print(\"\\nDistribuição das classes no subconjunto:\")\n",
        "print(df_subset['Label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgpqHrhZ2nr-",
        "outputId": "9d39fc62-f1f3-42c5-dc43-4037e027442f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Subconjunto criado com 25000 observações.\n",
            "\n",
            "Distribuição das classes no subconjunto:\n",
            "Label\n",
            "0    20503\n",
            "1     4497\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLZryRJdNwoJ"
      },
      "source": [
        "### **Pré-processamento Textual**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Limpeza com clean_text()**\n",
        "\n",
        "Função que aplica regras para \"limpar\" os textos brutos:\n",
        "\n",
        "    Remove URLs: tira links da internet (ex: http://...).\n",
        "\n",
        "    Remove menções: elimina nomes de usuários (@usuario).\n",
        "\n",
        "    Remove hashtags: exclui palavras precedidas de # (ou poderia apenas remover o símbolo).\n",
        "\n",
        "    Remove caracteres não alfabéticos: exclui números, pontuações e símbolos, mantendo letras e acentuação.\n",
        "\n",
        "    Converte para minúsculas: uniformiza o texto.\n",
        "\n",
        "    Remove espaços extras: com strip().\n",
        "\n",
        "Resultado: uma versão mais \"limpa\" do tweet.\n",
        "\n",
        "**2. Tokenização e Processamento Avançado**\n",
        "\n",
        "Com a função preprocess_text_advanced():\n",
        "\n",
        "    Tokenização: divide o texto em palavras (tokens) com word_tokenize.\n",
        "\n",
        "    Remoção de stopwords: elimina palavras muito comuns em inglês (ex: the, and, is) que pouco contribuem para a análise.\n",
        "\n",
        "    Lematização: reduz as palavras à sua forma base (ex: running vira run, cars vira car), usando o WordNetLemmatizer."
      ],
      "metadata": {
        "id": "zIBw1hlkmLdP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PGZ5tU4KnOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "056c75ad-bf0c-4bdb-b9f5-ff3aef694cea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Exemplo de conteúdo limpo (df_subset):\n",
            "                                             Content  \\\n",
            "0  that is what it is being years since i read th...   \n",
            "1  czar baldy bald does not have enough info to g...   \n",
            "2  i ve seen some discussion that there are simil...   \n",
            "3  they are weak and subhuman the problem is that...   \n",
            "4  hitler what a slut the perfect guy in wwii ser...   \n",
            "\n",
            "                                     cleaned_content  \n",
            "0  that is what it is being years since i read th...  \n",
            "1  czar baldy bald does not have enough info to g...  \n",
            "2  i ve seen some discussion that there are simil...  \n",
            "3  they are weak and subhuman the problem is that...  \n",
            "4  hitler what a slut the perfect guy in wwii ser...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 'punkt_tab'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'punkt_tab' downloaded.\n",
            "\n",
            "Exemplo de tokens processados (df_subset):\n",
            "                                             Content  \\\n",
            "0  that is what it is being years since i read th...   \n",
            "1  czar baldy bald does not have enough info to g...   \n",
            "2  i ve seen some discussion that there are simil...   \n",
            "3  they are weak and subhuman the problem is that...   \n",
            "4  hitler what a slut the perfect guy in wwii ser...   \n",
            "\n",
            "                                    processed_tokens  \n",
            "0  [year, since, read, thing, book, felt, could, ...  \n",
            "1    [czar, baldy, bald, enough, info, get, article]  \n",
            "2  [seen, discussion, similarity, anon, tactic, c...  \n",
            "3  [weak, subhuman, problem, monopoly, finance, m...  \n",
            "4  [hitler, slut, perfect, guy, wwii, seriously, ...  \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# regras de limpeza\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text) # remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text) # remove menções\n",
        "    text = re.sub(r'#\\w+', '', text) # remove hashtags (ou pode mantê-las sem o #)\n",
        "    # removendo acentos para simplificar a lematização com WordNetLemmatizer\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A) # remove caracteres não alfabéticos\n",
        "    text = text.lower() # converte para minúsculas\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# aplicação\n",
        "df_subset['cleaned_content'] = df_subset['Content'].apply(clean_text)\n",
        "\n",
        "print(\"\\nExemplo de conteúdo limpo (df_subset):\")\n",
        "print(df_subset[['Content', 'cleaned_content']].head())\n",
        "\n",
        "\n",
        "# recursos de lematização\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "# tratativa de exceção\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    print(\"Downloading 'punkt_tab'...\")\n",
        "    nltk.download('punkt_tab')\n",
        "    print(\"'punkt_tab' downloaded.\")\n",
        "\n",
        "\n",
        "# inicializa lematizador e stop words para inglês\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text_advanced(text):\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return []\n",
        "    tokens = word_tokenize(text)\n",
        "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word.isalpha()] # Adicionado isalpha() para garantir que são palavras\n",
        "    return processed_tokens\n",
        "\n",
        "# aplica pré-processamento\n",
        "df_subset['processed_tokens'] = df_subset['cleaned_content'].apply(preprocess_text_advanced)\n",
        "\n",
        "print(\"\\nExemplo de tokens processados (df_subset):\")\n",
        "print(df_subset[['Content', 'processed_tokens']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGL9oMu7bH88"
      },
      "source": [
        "### **Vetorização com  TF-IDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para transformar os textos dos tweets em uma representação numérica adequada aos algoritmos de aprendizado de máquina, foi utilizado o método de vetorização **TF-IDF (Term Frequency-Inverse Document Frequency).**\n",
        "\n",
        "Esse processo converte cada tweet em um vetor que reflete a relevância de cada termo no contexto do corpus, penalizando palavras muito frequentes e destacando termos mais informativos. Antes da vetorização, os tokens de cada tweet foram recombinados em strings, já que o `TfidfVectorizer` opera sobre textos contínuos.\n",
        "\n",
        "Após a definição do subconjunto com 25.000 observações, foi realizado o processo de **vetorização textual utilizando a técnica TF-IDF (Term Frequency-Inverse Document Frequency)**, com um limite de 5.000 features. Esse método transforma o conteúdo textual em uma matriz numérica esparsa, onde cada linha representa um documento (neste caso, um tweet) e cada coluna representa um termo relevante do vocabulário.\n",
        "\n",
        "A matriz resultante do `df_subset` apresentou a forma **(25000, 5000)**, indicando 25.000 documentos e 5.000 termos distintos. Em seguida, essa matriz foi dividida em conjuntos de treino e teste utilizando `train_test_split`, com 80% dos dados destinados ao treinamento e 20% à avaliação. O conjunto de treino resultou em uma matriz de **(20000, 5000)** e o de teste em **(5000, 5000)**. As variáveis alvo (labels) correspondentes a cada conjunto apresentaram dimensões **(20000,)** e **(5000,)**, respectivamente.\n"
      ],
      "metadata": {
        "id": "HI9-L-5Kmqkp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ktc9o8kwLjHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "882c8e58-5c74-464f-b305-26ce8ad3de17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forma da matriz TF-IDF do df_subset:\n",
            "(25000, 5000)\n",
            "\n",
            "Forma da matriz TF-IDF de Treino após o split:\n",
            "(20000, 5000)\n",
            "\n",
            "Forma da matriz TF-IDF de Teste após o split:\n",
            "(5000, 5000)\n",
            "\n",
            "Forma dos labels de Treino após o split:\n",
            "(20000,)\n",
            "\n",
            "Forma dos labels de Teste após o split:\n",
            "(5000,)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# verificação de 'processed_tokens' e 'Label'\n",
        "texts_for_tfidf = df_subset['processed_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "labels = df_subset['Label']\n",
        "\n",
        "# inicializa o TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# aplica o vetorizador\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(texts_for_tfidf)\n",
        "\n",
        "print(\"\\nForma da matriz TF-IDF do df_subset:\")\n",
        "print(tfidf_matrix.shape)\n",
        "\n",
        "# --- divide os dados vetorizados em treino e teste ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    tfidf_matrix,\n",
        "    labels,\n",
        "    test_size=0.20,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "print(\"\\nForma da matriz TF-IDF de Treino após o split:\")\n",
        "print(X_train.shape)\n",
        "print(\"\\nForma da matriz TF-IDF de Teste após o split:\")\n",
        "print(X_test.shape)\n",
        "print(\"\\nForma dos labels de Treino após o split:\")\n",
        "print(y_train.shape)\n",
        "print(\"\\nForma dos labels de Teste após o split:\")\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Aplicação em dataset desbalanceado**"
      ],
      "metadata": {
        "id": "ReKqtJ_gr2qu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6G441n_bOqA"
      },
      "source": [
        "#### **Divisão em treino/ teste**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após a vetorização dos dados, o conjunto TF-IDF foi dividido em dois subconjuntos: **80% para treino e 20% para teste**. O conjunto de treino possui **20.000 amostras**, enquanto o conjunto de teste contém **5.000 amostras**.\n",
        "\n",
        "Ambos mantêm a mesma dimensionalidade de 5.000 features, correspondente aos termos mais relevantes extraídos do vocabulário do corpus. Essa divisão permite avaliar o desempenho dos modelos de forma justa, utilizando dados não vistos durante o treinamento.\n"
      ],
      "metadata": {
        "id": "3BURTHbOniL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzsuXIjDMZpT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64f8d798-ea12-4e9e-879a-916f443554c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma dos dados de treino (X_train, y_train): (20000, 5000) (20000,)\n",
            "Forma dos dados de teste (X_test, y_test): (5000, 5000) (5000,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# divisão em, treino/teste\n",
        "X_split = tfidf_matrix\n",
        "y_split = labels\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_split, y_split, test_size=0.20, random_state=42, stratify=y_split\n",
        ")\n",
        "\n",
        "print(\"Forma dos dados de treino (X_train, y_train):\", X_train.shape, y_train.shape)\n",
        "print(\"Forma dos dados de teste (X_test, y_test):\", X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-oIDIWZbtPh"
      },
      "source": [
        "#### **Modelos Tradicionais**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quatro modelos de classificação supervisionada foram treinados para prever categorias de tweets (discurso de ódio, linguagem ofensiva ou nenhum dos dois), utilizando a matriz TF-IDF como entrada. Os modelos testados foram: **Regressão Logística, Naive Bayes Multinomial, Support Vector Machine (SVM com kernel linear) e Random Forest**. Cada modelo foi ajustado com os dados de treino e avaliado com os dados de teste usando métricas como **acurácia, F1-score (ponderado) e AUC (curva ROC, ponderada)**. Os resultados foram armazenados em um dicionário para facilitar a comparação de desempenho entre os classificadores.\n",
        "\n",
        "**Regressão Logística**\n",
        "\n",
        "A Regressão Logística foi utilizada como um modelo linear de base para classificação . Ela estima a probabilidade de um tweet pertencer a cada uma das classes com base nas palavras mais relevantes (extraídas via TF-IDF). O modelo foi treinado com um número maior de iterações (max_iter=1000) para garantir a convergência, dado o tamanho da matriz. Por oferecer suporte ao método predict_proba, foi possível calcular a métrica AUC ponderada (one-vs-rest), o que fornece uma medida robusta da capacidade do modelo em distinguir entre as classes.\n",
        "\n",
        "**Naive Bayes Multinomial**\n",
        "\n",
        "O modelo Naive Bayes Multinomial é especialmente adequado para tarefas de classificação de texto, por assumir que as características (neste caso, palavras) ocorrem de forma independente. Ele é simples, eficiente e frequentemente usado como forte baseline em PLN. No experimento, ele também permitiu a geração de probabilidades de classe (predict_proba), o que possibilitou o cálculo da AUC. Apesar de suas suposições simplificadas, o Naive Bayes costuma ter desempenho competitivo quando os dados estão bem vetorizados.\n",
        "\n",
        "**Support Vector Machine (SVM)**\n",
        "\n",
        "O SVM foi utilizado com kernel linear, uma configuração comum e eficaz para dados textuais de alta dimensionalidade, como é o caso da matriz TF-IDF. Foi ativada a opção probability=True para possibilitar o cálculo da AUC, embora isso torne o treinamento mais custoso computacionalmente. O SVM busca encontrar hiperplanos que melhor separam as classes, sendo especialmente útil quando há margens claras entre categorias. Apesar de não ser naturalmente probabilístico, sua robustez o torna uma escolha frequente em tarefas de classificação com múltiplas classes.\n",
        "\n",
        "**Random Forest**\n",
        "\n",
        "O modelo Random Forest foi treinado com 100 árvores de decisão, combinando os resultados de várias árvores para aumentar a estabilidade e a precisão da predição. Como um modelo de ensemble, ele lida bem com dados complexos e é menos sensível a overfitting do que uma única árvore. Também oferece suporte a predict_proba, permitindo calcular a AUC ponderada. Sua capacidade de capturar interações não lineares entre os termos dos textos pode ser vantajosa em relação a modelos lineares, especialmente quando o texto contém padrões mais sutis."
      ],
      "metadata": {
        "id": "UZrVNtQOnuWN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAGE1eTyMTTM",
        "outputId": "c6b68f0e-8a43-4b23-c84a-aa2ef99b88f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando Regressão Logística...\n",
            "Avaliação da Regressão Logística:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.97      0.91      4101\n",
            "           1       0.68      0.32      0.44       899\n",
            "\n",
            "    accuracy                           0.85      5000\n",
            "   macro avg       0.77      0.64      0.68      5000\n",
            "weighted avg       0.83      0.85      0.83      5000\n",
            "\n",
            "Acurácia:  0.8506\n",
            "Weighted F1-score: 0.8281429780388009\n",
            "AUC: 0.8609071175293256\n",
            "--------------------------------------------------\n",
            "Treinando Naive Bayes Multinomial...\n",
            "Avaliação do Naive Bayes Multinomial:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.97      0.91      4101\n",
            "           1       0.66      0.28      0.39       899\n",
            "\n",
            "    accuracy                           0.84      5000\n",
            "   macro avg       0.76      0.62      0.65      5000\n",
            "weighted avg       0.82      0.84      0.82      5000\n",
            "\n",
            "Acurácia:  0.8446\n",
            "Weighted F1-score: 0.8175401737151381\n",
            "AUC: 0.8493548197230171\n",
            "--------------------------------------------------\n",
            "Treinando SVM (Kernel Linear)...\n",
            "Avaliação do SVM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.91      4101\n",
            "           1       0.65      0.39      0.49       899\n",
            "\n",
            "    accuracy                           0.85      5000\n",
            "   macro avg       0.76      0.67      0.70      5000\n",
            "weighted avg       0.84      0.85      0.84      5000\n",
            "\n",
            "Acurácia:  0.8522\n",
            "Weighted F1-score: 0.8371962132264463\n",
            "AUC: 0.8339295144649871\n",
            "--------------------------------------------------\n",
            "Treinando Random Forest...\n",
            "Avaliação do Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.95      0.91      4101\n",
            "           1       0.61      0.37      0.46       899\n",
            "\n",
            "    accuracy                           0.84      5000\n",
            "   macro avg       0.74      0.66      0.69      5000\n",
            "weighted avg       0.83      0.84      0.83      5000\n",
            "\n",
            "Acurácia:  0.844\n",
            "Weighted F1-score: 0.8283386445741102\n",
            "AUC: 0.8386757726689197\n",
            "\n",
            "Resultados acumulados até agora:\n",
            "{'Logistic Regression': {'accuracy': 0.8506, 'precision (macro)': 0.7721839968602056, 'recall (macro)': 0.6444651579866437, 'f1-score (macro)': 0.6754733722643643, 'f1-score (weighted)': 0.8281429780388009, 'auc': np.float64(0.8609071175293256)}, 'Multinomial NB': {'accuracy': 0.8446, 'precision (macro)': 0.7604801283944512, 'recall (macro)': 0.6234374317666898, 'f1-score (macro)': 0.6512339092926278, 'f1-score (weighted)': 0.8175401737151381, 'auc': np.float64(0.8493548197230171)}, 'SVM': {'accuracy': 0.8522, 'precision (macro)': 0.7619668536821658, 'recall (macro)': 0.6727984085923859, 'f1-score (macro)': 0.7010995411331927, 'f1-score (weighted)': 0.8371962132264463, 'auc': np.float64(0.8339295144649871)}, 'Random Forest': {'accuracy': 0.844, 'precision (macro)': 0.7407205519399869, 'recall (macro)': 0.659548838979288, 'f1-score (macro)': 0.6850596157921123, 'f1-score (weighted)': 0.8283386445741102, 'auc': np.float64(0.8386757726689197)}}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "results = {}\n",
        "\n",
        "# 1. Regressão Logística\n",
        "print(\"Treinando Regressão Logística...\")\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "lr_predictions_proba = lr_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação da Regressão Logística:\")\n",
        "print(classification_report(y_test, lr_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lr_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, lr_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, lr_predictions_proba))\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "lr_report = classification_report(y_test, lr_predictions, output_dict=True)\n",
        "results['Logistic Regression'] = {\n",
        "    'accuracy': accuracy_score(y_test, lr_predictions),\n",
        "    'precision (macro)': lr_report['macro avg']['precision'],\n",
        "    'recall (macro)': lr_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lr_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lr_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, lr_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 2. Naive Bayes Multinomial\n",
        "print(\"Treinando Naive Bayes Multinomial...\")\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_predictions = nb_model.predict(X_test)\n",
        "nb_predictions_proba = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do Naive Bayes Multinomial:\")\n",
        "print(classification_report(y_test, nb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, nb_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, nb_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, nb_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "nb_report = classification_report(y_test, nb_predictions, output_dict=True)\n",
        "results['Multinomial NB'] = {\n",
        "    'accuracy': accuracy_score(y_test, nb_predictions),\n",
        "    'precision (macro)': nb_report['macro avg']['precision'],\n",
        "    'recall (macro)': nb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': nb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, nb_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, nb_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 3. Support Vector Machine (SVM)\n",
        "print(\"Treinando SVM (Kernel Linear)...\")\n",
        "svm_model = SVC(kernel='linear', probability=True)\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_predictions = svm_model.predict(X_test)\n",
        "svm_predictions_proba = svm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do SVM:\")\n",
        "print(classification_report(y_test, svm_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, svm_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, svm_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, svm_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "svm_report = classification_report(y_test, svm_predictions, output_dict=True)\n",
        "results['SVM'] = {\n",
        "    'accuracy': accuracy_score(y_test, svm_predictions),\n",
        "    'precision (macro)': svm_report['macro avg']['precision'],\n",
        "    'recall (macro)': svm_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': svm_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, svm_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, svm_predictions_proba)\n",
        "}\n",
        "\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 4. Random Forest Classifier\n",
        "print(\"Treinando Random Forest...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "rf_predictions_proba = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do Random Forest:\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, rf_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, rf_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, rf_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "rf_report = classification_report(y_test, rf_predictions, output_dict=True)\n",
        "results['Random Forest'] = {\n",
        "    'accuracy': accuracy_score(y_test, rf_predictions),\n",
        "    'precision (macro)': rf_report['macro avg']['precision'],\n",
        "    'recall (macro)': rf_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': rf_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, rf_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, rf_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"\\nResultados acumulados até agora:\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oirSDRZibcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9811f2d8-50a6-4ac6-c392-3411e4bb9ade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# instalação\n",
        "!pip install tensorflow\n",
        "\n",
        "# importações de bibliotecas necessárias para a CNN\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0C8wz0nQSuc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6879889e-52fa-4c08-8b34-4703203eb3b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\n",
            "\n",
            "==================================================\n",
            "Treinando LightGBM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3598, number of negative: 16402\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.454492 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 88850\n",
            "[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 2680\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.179900 -> initscore=-1.517025\n",
            "[LightGBM] [Info] Start training from score -1.517025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avaliação do LightGBM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.96      0.91      4101\n",
            "           1       0.63      0.35      0.45       899\n",
            "\n",
            "    accuracy                           0.85      5000\n",
            "   macro avg       0.75      0.65      0.68      5000\n",
            "weighted avg       0.83      0.85      0.83      5000\n",
            "\n",
            "Acurácia:  0.8456\n",
            "Weighted F1-score: 0.8268504750781127\n",
            "AUC: 0.8413867965137236\n",
            "--------------------------------------------------\n",
            "Treinando Rede Neural Densa (MLP)...\n",
            "\n",
            "Treinando MLP...\n",
            "Epoch 1/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.8092 - loss: 0.4551 - val_accuracy: 0.8500 - val_loss: 0.3493\n",
            "Epoch 2/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.8683 - loss: 0.2976 - val_accuracy: 0.8518 - val_loss: 0.3542\n",
            "Epoch 3/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.8996 - loss: 0.2382 - val_accuracy: 0.8490 - val_loss: 0.3746\n",
            "Epoch 4/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.9213 - loss: 0.1992 - val_accuracy: 0.8450 - val_loss: 0.4338\n",
            "Epoch 5/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9469 - loss: 0.1447 - val_accuracy: 0.8378 - val_loss: 0.4834\n",
            "Epoch 6/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.9666 - loss: 0.0962 - val_accuracy: 0.8390 - val_loss: 0.5639\n",
            "Epoch 7/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 28ms/step - accuracy: 0.9775 - loss: 0.0688 - val_accuracy: 0.8380 - val_loss: 0.6617\n",
            "Epoch 8/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.9824 - loss: 0.0506 - val_accuracy: 0.8292 - val_loss: 0.7257\n",
            "Epoch 9/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - accuracy: 0.9861 - loss: 0.0410 - val_accuracy: 0.8362 - val_loss: 0.7958\n",
            "Epoch 10/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9881 - loss: 0.0337 - val_accuracy: 0.8382 - val_loss: 0.8471\n",
            "Epoch 11/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - accuracy: 0.9924 - loss: 0.0239 - val_accuracy: 0.8352 - val_loss: 0.8796\n",
            "Epoch 12/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 28ms/step - accuracy: 0.9919 - loss: 0.0238 - val_accuracy: 0.8382 - val_loss: 0.9386\n",
            "Epoch 13/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.9907 - loss: 0.0234 - val_accuracy: 0.8350 - val_loss: 0.9656\n",
            "Epoch 14/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.9926 - loss: 0.0205 - val_accuracy: 0.8296 - val_loss: 1.0125\n",
            "Epoch 15/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.9928 - loss: 0.0202 - val_accuracy: 0.8390 - val_loss: 1.0319\n",
            "Epoch 16/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.9949 - loss: 0.0166 - val_accuracy: 0.8330 - val_loss: 1.0587\n",
            "Epoch 17/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.9934 - loss: 0.0178 - val_accuracy: 0.8376 - val_loss: 1.1010\n",
            "Epoch 18/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9964 - loss: 0.0121 - val_accuracy: 0.8404 - val_loss: 1.0967\n",
            "Epoch 19/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step - accuracy: 0.9946 - loss: 0.0155 - val_accuracy: 0.8364 - val_loss: 1.1441\n",
            "Epoch 20/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.9948 - loss: 0.0145 - val_accuracy: 0.8434 - val_loss: 1.1396\n",
            "\n",
            "Resumo do Modelo MLP:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,280,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,939,845\u001b[0m (15.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,939,845</span> (15.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,281\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,281</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,626,564\u001b[0m (10.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,626,564</span> (10.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando MLP no conjunto de teste...\n",
            "\n",
            "Acurácia da MLP no conjunto de teste: 0.8434\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "\n",
            "Avaliação completa da MLP:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.93      0.91      4101\n",
            "           1       0.58      0.44      0.51       899\n",
            "\n",
            "    accuracy                           0.84      5000\n",
            "   macro avg       0.73      0.69      0.71      5000\n",
            "weighted avg       0.83      0.84      0.83      5000\n",
            "\n",
            "Weighted F1-score: 0.8347655047605053\n",
            "AUC: 0.8199473310044838\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# --- Modelos Avançados com TF-IDF ---\n",
        "\n",
        "# 5. LightGBM (Gradient Boosting)\n",
        "!pip install lightgbm\n",
        "\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Treinando LightGBM...\")\n",
        "\n",
        "# o número de classes será 2\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# converte matriz esparsa para o formato LightGBM\n",
        "lgb_model = lgb.LGBMClassifier(objective='binary', random_state=42)\n",
        "lgb_model.fit(X_train, y_train)\n",
        "lgb_predictions = lgb_model.predict(X_test)\n",
        "\n",
        "# obtém probabilidades da classe positiva para AUC\n",
        "lgb_predictions_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do LightGBM:\")\n",
        "print(classification_report(y_test, lgb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lgb_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, lgb_predictions, average='weighted'))\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    lgb_auc_score = roc_auc_score(y_test, lgb_predictions_proba)\n",
        "    print(\"AUC:\", lgb_auc_score)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    lgb_auc_score = None\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "lgb_report = classification_report(y_test, lgb_predictions, output_dict=True)\n",
        "results['LightGBM'] = {\n",
        "    'accuracy': accuracy_score(y_test, lgb_predictions),\n",
        "    'precision (macro)': lgb_report['macro avg']['precision'],\n",
        "    'recall (macro)': lgb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lgb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lgb_predictions, average='weighted'),\n",
        "    'auc': lgb_auc_score #\n",
        "}\n",
        "\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 6. Rede Neural Densa (MLP) com TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "print(\"Treinando Rede Neural Densa (MLP)...\")\n",
        "\n",
        "num_classes_mlp = len(np.unique(y_train))\n",
        "\n",
        "# constrói o Modelo MLP\n",
        "mlp_model = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "mlp_model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# treino do Modelo MLP\n",
        "epochs_mlp = 20\n",
        "batch_size_mlp = 64\n",
        "\n",
        "\n",
        "print(\"\\nTreinando MLP...\")\n",
        "history_mlp = mlp_model.fit(X_train, y_train,\n",
        "                          epochs=epochs_mlp,\n",
        "                          batch_size=batch_size_mlp,\n",
        "                          validation_data=(X_test, y_test),\n",
        "                          verbose=1)\n",
        "\n",
        "\n",
        "print(\"\\nResumo do Modelo MLP:\")\n",
        "mlp_model.summary()\n",
        "\n",
        "# avaliação do Modelo MLP\n",
        "print(\"\\nAvaliando MLP no conjunto de teste...\")\n",
        "loss_mlp, accuracy_mlp = mlp_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da MLP no conjunto de teste: {accuracy_mlp:.4f}\")\n",
        "\n",
        "# gera previsões de probabilidade\n",
        "mlp_predictions_proba_positive = mlp_model.predict(X_test)\n",
        "\n",
        "# obtém as classes preditas\n",
        "mlp_predictions = (mlp_predictions_proba_positive >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da MLP:\")\n",
        "print(classification_report(y_test, mlp_predictions))\n",
        "\n",
        "# cálculo de F1-score ponderado\n",
        "mlp_weighted_f1 = f1_score(y_test, mlp_predictions, average='weighted')\n",
        "print(\"Weighted F1-score:\", mlp_weighted_f1)\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    mlp_auc_score = roc_auc_score(y_test, mlp_predictions_proba_positive)\n",
        "    print(\"AUC:\", mlp_auc_score)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    mlp_auc_score = None\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "mlp_report = classification_report(y_test, mlp_predictions, output_dict=True)\n",
        "results['MLP (TF-IDF)'] = {\n",
        "    'accuracy': accuracy_mlp,\n",
        "    'precision (macro)': mlp_report['macro avg']['precision'],\n",
        "    'recall (macro)': mlp_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': mlp_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': mlp_weighted_f1,\n",
        "    'auc': mlp_auc_score\n",
        "}\n",
        "\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaGFQTW_dhin"
      },
      "source": [
        "#### **Modelos de DL**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para explorar abordagens mais profundas de aprendizado, foi implementada **uma rede neural convolucional (CNN)** voltada para a classificação de textos. Como esse tipo de modelo trabalha melhor com sequências de palavras em vez de vetores TF-IDF, os tweets foram tokenizados e convertidos em **sequências inteiras**, com padding para garantir um comprimento fixo.\n",
        "\n",
        "O modelo foi construído com uma camada de embedding (para mapear palavras em vetores densos), seguida por uma **camada convolucional 1D** que captura padrões locais no texto e uma **camada de pooling** que extrai as informações mais relevantes. Camadas densas e dropout foram adicionadas para refinar o aprendizado e reduzir overfitting. O modelo foi treinado por 10 épocas e avaliado com base em métricas como acurácia, F1-score e AUC. Essa arquitetura permite capturar melhor a estrutura local e semântica dos textos, sendo especialmente útil para dados curtos e ruidosos como tweets."
      ],
      "metadata": {
        "id": "bS5B8Puhp1rv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeetVhS1NMOM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9834955f-b13d-4600-d7f4-51493891404b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Exemplo de sequências padded (df_subset completo):\n",
            "[[  57   84   90   47  156 1215   28 2589  146 3160 2199  101    1 2413\n",
            "  2589  101 2200   14  113  681  680 4581    1    6 2589  101 1262   48\n",
            "   194  348 3522    6 1263 3803    6   19  277 8319  997  942 2525   82\n",
            "    35   23 1801   62  303  198  249    8  282   60 1361    8  583 8319\n",
            "   375  363 8319 2161  198  370 1392 1008  363  252   47    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    1 7440  224  285   19    2    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [ 307   64 3058 6811 2360 1291  185  250   91   64    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [1769    1  101 6812 3804  195 8320  114    1  447  356  728 4822   13\n",
            "    97 1009 3389    1 2201 2730   32 1960  329   52    1  147 1532  728\n",
            "   418  211 8321 5121 1248 3389   15 3389   19  329   89 1802    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [1629    3 1079  164 4823  662   19 2202   13   75    1  663  129    7\n",
            "  1414 1561    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "\n",
            "Forma das sequências padded (df_subset completo): (25000, 100)\n",
            "\n",
            "Labels do df_subset: [0 1]\n",
            "\n",
            "Forma dos dados de treino para CNN: (15000, 100) (15000,)\n",
            "Forma dos dados de validação para CNN: (5000, 100) (5000,)\n",
            "Forma dos dados de teste FINAL para CNN: (5000, 100) (5000,)\n",
            "\n",
            "Treinando CNN...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 45ms/step - accuracy: 0.8145 - loss: 0.4512 - val_accuracy: 0.8510 - val_loss: 0.3292\n",
            "Epoch 2/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 47ms/step - accuracy: 0.8933 - loss: 0.2582 - val_accuracy: 0.8502 - val_loss: 0.3319\n",
            "Epoch 3/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 44ms/step - accuracy: 0.9363 - loss: 0.1584 - val_accuracy: 0.8530 - val_loss: 0.3927\n",
            "Epoch 4/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 45ms/step - accuracy: 0.9739 - loss: 0.0730 - val_accuracy: 0.8406 - val_loss: 0.5711\n",
            "Epoch 5/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.9876 - loss: 0.0367 - val_accuracy: 0.8266 - val_loss: 0.6974\n",
            "Epoch 6/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.9960 - loss: 0.0130 - val_accuracy: 0.8270 - val_loss: 0.7919\n",
            "Epoch 7/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 47ms/step - accuracy: 0.9975 - loss: 0.0083 - val_accuracy: 0.8404 - val_loss: 1.0197\n",
            "Epoch 8/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - accuracy: 0.9982 - loss: 0.0065 - val_accuracy: 0.8356 - val_loss: 0.9934\n",
            "Epoch 9/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 45ms/step - accuracy: 0.9983 - loss: 0.0069 - val_accuracy: 0.8386 - val_loss: 1.1244\n",
            "Epoch 10/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 48ms/step - accuracy: 0.9975 - loss: 0.0086 - val_accuracy: 0.8414 - val_loss: 1.1126\n",
            "\n",
            "Resumo do Modelo CNN:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m49,280\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,037,765\u001b[0m (15.40 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,037,765</span> (15.40 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,345,921\u001b[0m (5.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,345,921</span> (5.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,691,844\u001b[0m (10.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,691,844</span> (10.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando CNN no conjunto de teste FINAL...\n",
            "\n",
            "Acurácia da CNN no conjunto de teste FINAL: 0.8328\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
            "\n",
            "Avaliação completa da CNN no conjunto de teste FINAL:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.93      0.90      4101\n",
            "           1       0.55      0.38      0.45       899\n",
            "\n",
            "    accuracy                           0.83      5000\n",
            "   macro avg       0.71      0.66      0.68      5000\n",
            "weighted avg       0.82      0.83      0.82      5000\n",
            "\n",
            "Weighted F1-score: 0.8205822285173583\n",
            "AUC: 0.8076487218315944\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# --- pré-processamento para CNN/LSTM  ---\n",
        "\n",
        "max_words = 10000\n",
        "max_sequence_length = 100\n",
        "\n",
        "all_texts = df_subset['processed_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "all_labels = df_subset['Label']\n",
        "\n",
        "# --- tokenização ---\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(all_texts)\n",
        "\n",
        "# converte textos para sequências de inteiros\n",
        "all_sequences = tokenizer.texts_to_sequences(all_texts)\n",
        "\n",
        "# --- padding das Sequências ---\n",
        "all_padded_sequences = pad_sequences(all_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "print(\"\\nExemplo de sequências padded (df_subset completo):\")\n",
        "print(all_padded_sequences[:5])\n",
        "print(\"\\nForma das sequências padded (df_subset completo):\", all_padded_sequences.shape)\n",
        "\n",
        "# --- prepara labels para CNN/LSTM ---\n",
        "# as labels do df_subset já devem ser inteiros (0 ou 1) devido ao pré-processamento anterior.\n",
        "encoded_all_labels = all_labels.values\n",
        "\n",
        "print(\"\\nLabels do df_subset:\", np.unique(encoded_all_labels))\n",
        "\n",
        "# --- divisão dos dados para CNN/LSTM (a partir do df_subset completo) ---\n",
        "X_train_val, X_test_cnn_final, y_train_val, y_test_cnn_final = train_test_split(\n",
        "    all_padded_sequences, encoded_all_labels, test_size=0.20, random_state=42, stratify=encoded_all_labels\n",
        ")\n",
        "\n",
        "X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val\n",
        "    # test_size=0.25 para 25% do conjunto train_val, que é 20% do total original (0.25 * 0.80 = 0.20)\n",
        "    # ao final aproximadamente 60% Treino, 20% Validação, 20% Teste.\n",
        ")\n",
        "\n",
        "\n",
        "print(\"\\nForma dos dados de treino para CNN:\", X_train_cnn.shape, y_train_cnn.shape)\n",
        "print(\"Forma dos dados de validação para CNN:\", X_val_cnn.shape, y_val_cnn.shape)\n",
        "print(\"Forma dos dados de teste FINAL para CNN:\", X_test_cnn_final.shape, y_test_cnn_final.shape)\n",
        "\n",
        "\n",
        "# --- contrói o Modelo CNN ---\n",
        "\n",
        "embedding_dim = 128\n",
        "filter_sizes = [3]\n",
        "num_filters = 128\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    Conv1D(filters=num_filters, kernel_size=filter_sizes[0], activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# --- treino do Modelo CNN ---\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "print(\"\\nTreinando CNN...\")\n",
        "history = model.fit(X_train_cnn, y_train_cnn,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(X_val_cnn, y_val_cnn),\n",
        "                    verbose=1)\n",
        "\n",
        "print(\"\\nResumo do Modelo CNN:\")\n",
        "model.summary()\n",
        "\n",
        "# ---avaliação do Modelo CNN ---\n",
        "print(\"\\nAvaliando CNN no conjunto de teste FINAL...\")\n",
        "loss_cnn_test, accuracy_cnn_test = model.evaluate(X_test_cnn_final, y_test_cnn_final, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da CNN no conjunto de teste FINAL: {accuracy_cnn_test:.4f}\")\n",
        "\n",
        "# gera previsões e probabilidades\n",
        "cnn_predictions_proba_positive_test = model.predict(X_test_cnn_final)\n",
        "cnn_predictions_test = (cnn_predictions_proba_positive_test >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da CNN no conjunto de teste FINAL:\")\n",
        "print(classification_report(y_test_cnn_final, cnn_predictions_test))\n",
        "\n",
        "# cálculo de F1-score ponderado e AUC\n",
        "cnn_weighted_f1_test = f1_score(y_test_cnn_final, cnn_predictions_test, average='weighted')\n",
        "print(\"Weighted F1-score:\", cnn_weighted_f1_test)\n",
        "\n",
        "try:\n",
        "    cnn_auc_score_test = roc_auc_score(y_test_cnn_final, cnn_predictions_proba_positive_test)\n",
        "    print(\"AUC:\", cnn_auc_score_test)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    cnn_auc_score_test = None\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "# dicionário de resultados\n",
        "\n",
        "cnn_report_test = classification_report(y_test_cnn_final, cnn_predictions_test, output_dict=True)\n",
        "results['CNN (Embedding+Seq) Test'] = {\n",
        "    'accuracy': accuracy_cnn_test,\n",
        "    'precision (macro)': cnn_report_test['macro avg']['precision'],\n",
        "    'recall (macro)': cnn_report_test['macro avg']['recall'],\n",
        "    'f1-score (macro)': cnn_report_test['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': cnn_weighted_f1_test,\n",
        "    'auc': cnn_auc_score_test\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Rswna5mV5bO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "195a8bea-e81a-4e45-fa96-b75a6665d576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Construindo o Modelo LSTM...\n",
            "\n",
            "Treinando LSTM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 227ms/step - accuracy: 0.8182 - loss: 0.4988 - val_accuracy: 0.8202 - val_loss: 0.4721\n",
            "Epoch 2/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 220ms/step - accuracy: 0.8214 - loss: 0.4798 - val_accuracy: 0.8202 - val_loss: 0.4709\n",
            "Epoch 3/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 238ms/step - accuracy: 0.8236 - loss: 0.4720 - val_accuracy: 0.8202 - val_loss: 0.4689\n",
            "Epoch 4/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 217ms/step - accuracy: 0.8151 - loss: 0.4821 - val_accuracy: 0.8204 - val_loss: 0.4669\n",
            "Epoch 5/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 220ms/step - accuracy: 0.8240 - loss: 0.4653 - val_accuracy: 0.8204 - val_loss: 0.4684\n",
            "Epoch 6/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 217ms/step - accuracy: 0.8195 - loss: 0.4637 - val_accuracy: 0.8202 - val_loss: 0.3822\n",
            "Epoch 7/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 223ms/step - accuracy: 0.8205 - loss: 0.3574 - val_accuracy: 0.8202 - val_loss: 0.3407\n",
            "Epoch 8/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 227ms/step - accuracy: 0.8566 - loss: 0.2880 - val_accuracy: 0.8460 - val_loss: 0.3573\n",
            "Epoch 9/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 229ms/step - accuracy: 0.8901 - loss: 0.2368 - val_accuracy: 0.8522 - val_loss: 0.3738\n",
            "Epoch 10/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 253ms/step - accuracy: 0.9222 - loss: 0.1840 - val_accuracy: 0.8310 - val_loss: 0.3851\n",
            "Epoch 11/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 222ms/step - accuracy: 0.9355 - loss: 0.1628 - val_accuracy: 0.8456 - val_loss: 0.4483\n",
            "Epoch 12/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 222ms/step - accuracy: 0.9462 - loss: 0.1408 - val_accuracy: 0.7970 - val_loss: 0.4972\n",
            "Epoch 13/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 241ms/step - accuracy: 0.9511 - loss: 0.1274 - val_accuracy: 0.8394 - val_loss: 0.4864\n",
            "Epoch 14/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 217ms/step - accuracy: 0.9638 - loss: 0.1026 - val_accuracy: 0.8454 - val_loss: 0.5998\n",
            "Epoch 15/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 217ms/step - accuracy: 0.9686 - loss: 0.0940 - val_accuracy: 0.8286 - val_loss: 0.5682\n",
            "\n",
            "Resumo do Modelo LSTM:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,259,717\u001b[0m (16.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,259,717</span> (16.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,419,905\u001b[0m (5.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,419,905</span> (5.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,839,812\u001b[0m (10.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,839,812</span> (10.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando LSTM no conjunto de teste FINAL...\n",
            "\n",
            "Acurácia da LSTM no conjunto de teste FINAL: 0.8198\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step\n",
            "\n",
            "Avaliação completa da LSTM no conjunto de teste FINAL:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89      4101\n",
            "           1       0.50      0.51      0.50       899\n",
            "\n",
            "    accuracy                           0.82      5000\n",
            "   macro avg       0.70      0.70      0.70      5000\n",
            "weighted avg       0.82      0.82      0.82      5000\n",
            "\n",
            "Weighted F1-score: 0.820303736403071\n",
            "AUC: 0.8144487941978937\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# --- reutiliza dados preparados para a CNN ---\n",
        "\n",
        "# --- constrói o Modelo LSTM ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Construindo o Modelo LSTM...\")\n",
        "\n",
        "# como o problema é binário, a última camada da LSTM deve ter 1 unidade com ativação 'sigmoid'.\n",
        "\n",
        "\n",
        "model_lstm = Sequential([\n",
        "    # camada de Embedding: Reutiliza os mesmos parâmetros da CNN para consistência na base de embedding\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    # camada LSTM\n",
        "    LSTM(128),\n",
        "    # camada de Dropout\n",
        "    Dropout(0.5),\n",
        "    # camada densa (Fully Connected)\n",
        "    Dense(64, activation='relu'), # Camada densa adicional\n",
        "    Dropout(0.5),\n",
        "    # camada de saída: 1 unidade com ativação 'sigmoid' para classificação binária\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_lstm.compile(optimizer='adam',\n",
        "                   loss='binary_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# --- treino do Modelo LSTM ---\n",
        "epochs_lstm = 15\n",
        "batch_size_lstm = 32\n",
        "\n",
        "print(\"\\nTreinando LSTM...\")\n",
        "\n",
        "history_lstm = model_lstm.fit(X_train_cnn, y_train_cnn,\n",
        "                              epochs=epochs_lstm,\n",
        "                              batch_size=batch_size_lstm,\n",
        "                              validation_data=(X_val_cnn, y_val_cnn),\n",
        "                              verbose=1)\n",
        "\n",
        "print(\"\\nResumo do Modelo LSTM:\")\n",
        "model_lstm.summary()\n",
        "\n",
        "# --- avaliação do Modelo LSTM  ---\n",
        "print(\"\\nAvaliando LSTM no conjunto de teste FINAL...\")\n",
        "loss_lstm_test, accuracy_lstm_test = model_lstm.evaluate(X_test_cnn_final, y_test_cnn_final, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da LSTM no conjunto de teste FINAL: {accuracy_lstm_test:.4f}\")\n",
        "\n",
        "# gera previsões (classes) e probabilidades para calcular métricas\n",
        "lstm_predictions_proba_positive_test = model_lstm.predict(X_test_cnn_final)\n",
        "\n",
        "# obtém as classes preditas\n",
        "lstm_predictions_test = (lstm_predictions_proba_positive_test >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da LSTM no conjunto de teste FINAL:\")\n",
        "print(classification_report(y_test_cnn_final, lstm_predictions_test))\n",
        "\n",
        "\n",
        "# cálculo de F1-score ponderado\n",
        "lstm_weighted_f1_test = f1_score(y_test_cnn_final, lstm_predictions_test, average='weighted')\n",
        "print(\"Weighted F1-score:\", lstm_weighted_f1_test)\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    lstm_auc_score_test = roc_auc_score(y_test_cnn_final, lstm_predictions_proba_positive_test)\n",
        "    print(\"AUC:\", lstm_auc_score_test)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    lstm_auc_score_test = None\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "# dicionário de resultados\n",
        "lstm_report_test = classification_report(y_test_cnn_final, lstm_predictions_test, output_dict=True)\n",
        "\n",
        "results['LSTM (Embedding+Seq) Test'] = {\n",
        "    'accuracy': accuracy_lstm_test,\n",
        "    'precision (macro)': lstm_report_test['macro avg']['precision'],\n",
        "    'recall (macro)': lstm_report_test['macro avg']['recall'],\n",
        "    'f1-score (macro)': lstm_report_test['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': lstm_weighted_f1_test,\n",
        "    'auc': lstm_auc_score_test\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGQsnDqrwSlJ"
      },
      "source": [
        "### **Balanceamento do dataset e retreino dos modelos**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Class_weight**"
      ],
      "metadata": {
        "id": "DC2dbirgsFGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta etapa do experimento, foi aplicado o **balanceamento de classes por meio do parâmetro class_weight**, com o objetivo de mitigar o impacto do desbalanceamento dos dados na performance dos modelos.\n",
        "\n",
        "Utilizando a função `compute_class_weight` da biblioteca `sklearn`, foram calculados pesos proporcionais à frequência das classes no conjunto de treinamento. Esses pesos foram incorporados diretamente ao processo de treinamento em modelos que suportam esse recurso, como **Regressão Logística, SVM, Random Forest, LightGBM e Redes Neurais com Keras**.\n",
        "\n",
        "A inclusão dos pesos penaliza erros cometidos em classes minoritárias, forçando os algoritmos a considerarem com maior atenção esses exemplos menos frequentes. Essa estratégia é especialmente eficaz em cenários de classificação desbalanceada, como o de detecção de discursos ofensivos, contribuindo para **melhorias em métricas como recall e F1-score das classes minoritárias, sem necessariamente comprometer a acurácia geral**.\n",
        "\n",
        "Modelos como o Naive Bayes Multinomial, que não oferecem suporte direto a class_weight, foram mantidos como referência sem ajuste neste momento."
      ],
      "metadata": {
        "id": "wB7Xok1CsTHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, f1_score\n",
        "import pandas as pd\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"Treinando modelos tradicionais COM Class Weight...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 1. Regressão Logística\n",
        "print(\"Treinando Regressão Logística (Class Weight)...\")\n",
        "lr_model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "lr_predictions_proba = lr_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação da Regressão Logística (Class Weight):\")\n",
        "print(classification_report(y_test, lr_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lr_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, lr_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, lr_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "lr_report = classification_report(y_test, lr_predictions, output_dict=True)\n",
        "results['Logistic Regression (Class Weight)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lr_predictions),\n",
        "    'precision (macro)': lr_report['macro avg']['precision'],\n",
        "    'recall (macro)': lr_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lr_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lr_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, lr_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 2. Naive Bayes Multinomial\n",
        "# Nota: MultinomialNB NÃO tem o parâmetro class_weight.\n",
        "# Naive Bayes já lida com a frequência das classes inerentemente na sua formulação de probabilidade.\n",
        "\n",
        "print(\"Treinando Naive Bayes Multinomial (Não suporta class_weight)...\")\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_predictions = nb_model.predict(X_test)\n",
        "nb_predictions_proba = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do Naive Bayes Multinomial:\")\n",
        "print(classification_report(y_test, nb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, nb_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, nb_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, nb_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "nb_report = classification_report(y_test, nb_predictions, output_dict=True)\n",
        "results['Multinomial NB'] = {\n",
        "    'accuracy': accuracy_score(y_test, nb_predictions),\n",
        "    'precision (macro)': nb_report['macro avg']['precision'],\n",
        "    'recall (macro)': nb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': nb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, nb_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, nb_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# 3. Support Vector Machine (SVM)\n",
        "print(\"Treinando SVM (Kernel Linear) (Class Weight)...\")\n",
        "svm_model = SVC(kernel='linear', probability=True, class_weight='balanced', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_predictions = svm_model.predict(X_test)\n",
        "svm_predictions_proba = svm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do SVM (Class Weight):\")\n",
        "print(classification_report(y_test, svm_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, svm_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, svm_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, svm_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "svm_report = classification_report(y_test, svm_predictions, output_dict=True)\n",
        "results['SVM (Class Weight)'] = {\n",
        "    'accuracy': accuracy_score(y_test, svm_predictions),\n",
        "    'precision (macro)': svm_report['macro avg']['precision'],\n",
        "    'recall (macro)': svm_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': svm_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, svm_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, svm_predictions_proba)\n",
        "}\n",
        "\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 4. Random Forest Classifier\n",
        "print(\"Treinando Random Forest (Class Weight)...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "rf_predictions_proba = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do Random Forest (Class Weight):\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, rf_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, rf_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, rf_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "rf_report = classification_report(y_test, rf_predictions, output_dict=True)\n",
        "results['Random Forest (Class Weight)'] = {\n",
        "    'accuracy': accuracy_score(y_test, rf_predictions),\n",
        "    'precision (macro)': rf_report['macro avg']['precision'],\n",
        "    'recall (macro)': rf_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': rf_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, rf_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, rf_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"\\nResultados acumulados (incluindo Class Weight) até agora:\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "waFbBJrlhJ1L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7bcc069-cc01-41b3-ef02-9ea80a6184d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando modelos tradicionais COM Class Weight...\n",
            "--------------------------------------------------\n",
            "Treinando Regressão Logística (Class Weight)...\n",
            "Avaliação da Regressão Logística (Class Weight):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.82      0.87      4101\n",
            "           1       0.47      0.72      0.57       899\n",
            "\n",
            "    accuracy                           0.81      5000\n",
            "   macro avg       0.70      0.77      0.72      5000\n",
            "weighted avg       0.85      0.81      0.82      5000\n",
            "\n",
            "Acurácia:  0.8054\n",
            "Weighted F1-score: 0.819503615198162\n",
            "AUC: 0.8599667353712529\n",
            "--------------------------------------------------\n",
            "Treinando Naive Bayes Multinomial (Não suporta class_weight)...\n",
            "Avaliação do Naive Bayes Multinomial:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.97      0.91      4101\n",
            "           1       0.66      0.28      0.39       899\n",
            "\n",
            "    accuracy                           0.84      5000\n",
            "   macro avg       0.76      0.62      0.65      5000\n",
            "weighted avg       0.82      0.84      0.82      5000\n",
            "\n",
            "Acurácia:  0.8446\n",
            "Weighted F1-score: 0.8175401737151381\n",
            "AUC: 0.8493548197230171\n",
            "--------------------------------------------------\n",
            "Treinando SVM (Kernel Linear) (Class Weight)...\n",
            "Avaliação do SVM (Class Weight):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.82      0.87      4101\n",
            "           1       0.46      0.71      0.55       899\n",
            "\n",
            "    accuracy                           0.80      5000\n",
            "   macro avg       0.69      0.76      0.71      5000\n",
            "weighted avg       0.84      0.80      0.81      5000\n",
            "\n",
            "Acurácia:  0.7954\n",
            "Weighted F1-score: 0.8108717136472083\n",
            "AUC: 0.8462418211570526\n",
            "--------------------------------------------------\n",
            "Treinando Random Forest (Class Weight)...\n",
            "Avaliação do Random Forest (Class Weight):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.92      0.90      4101\n",
            "           1       0.52      0.42      0.46       899\n",
            "\n",
            "    accuracy                           0.83      5000\n",
            "   macro avg       0.70      0.67      0.68      5000\n",
            "weighted avg       0.81      0.83      0.82      5000\n",
            "\n",
            "Acurácia:  0.826\n",
            "Weighted F1-score: 0.8183874465417764\n",
            "AUC: 0.8357359595681783\n",
            "\n",
            "Resultados acumulados (incluindo Class Weight) até agora:\n",
            "{'Logistic Regression (Class Weight)': {'accuracy': 0.8054, 'precision (macro)': 0.7013703048018993, 'recall (macro)': 0.7706361263524266, 'f1-score (macro)': 0.7219663023158407, 'f1-score (weighted)': 0.819503615198162, 'auc': np.float64(0.8599667353712529)}, 'Multinomial NB': {'accuracy': 0.8446, 'precision (macro)': 0.7604801283944512, 'recall (macro)': 0.6234374317666898, 'f1-score (macro)': 0.6512339092926278, 'f1-score (weighted)': 0.8175401737151381, 'auc': np.float64(0.8493548197230171)}, 'SVM (Class Weight)': {'accuracy': 0.7954, 'precision (macro)': 0.6910059382725489, 'recall (macro)': 0.7601975317884159, 'f1-score (macro)': 0.7103840319061108, 'f1-score (weighted)': 0.8108717136472083, 'auc': np.float64(0.8462418211570526)}, 'Random Forest (Class Weight)': {'accuracy': 0.826, 'precision (macro)': 0.6988866758443875, 'recall (macro)': 0.6668144913785645, 'f1-score (macro)': 0.679890877375688, 'f1-score (weighted)': 0.8183874465417764, 'auc': np.float64(0.8357359595681783)}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Modelos Avançados com TF-IDF (com Class Weight) ---\n",
        "\n",
        "# importação de bibliotecas\n",
        "import lightgbm as lgb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "# --- cálculo de Class Weights ---\n",
        "classes = np.unique(y_train)\n",
        "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
        "class_weight_dict = dict(zip(classes, class_weights))\n",
        "\n",
        "print(\"\\nPesos das classes para TF-IDF (Class Weight):\")\n",
        "print(class_weight_dict)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Treinando Modelos Avançados com TF-IDF usando CLASS WEIGHT...\")\n",
        "\n",
        "\n",
        "# 5. LightGBM (Gradient Boosting) (com Class Weight)\n",
        "print(\"Treinando LightGBM com Class Weight...\")\n",
        "\n",
        "lgb_model_cw = lgb.LGBMClassifier(objective='binary', random_state=42, class_weight=class_weight_dict)\n",
        "lgb_model_cw.fit(X_train, y_train)\n",
        "lgb_predictions_cw = lgb_model_cw.predict(X_test)\n",
        "lgb_predictions_proba_cw = lgb_model_cw.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do LightGBM (Class Weight):\")\n",
        "print(classification_report(y_test, lgb_predictions_cw))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lgb_predictions_cw))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, lgb_predictions_cw, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, lgb_predictions_proba_cw))\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "lgb_report_cw = classification_report(y_test, lgb_predictions_cw, output_dict=True)\n",
        "results['LightGBM (Class Weight)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lgb_predictions_cw),\n",
        "    'precision (macro)': lgb_report_cw['macro avg']['precision'],\n",
        "    'recall (macro)': lgb_report_cw['macro avg']['recall'],\n",
        "    'f1-score (macro)': lgb_report_cw['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lgb_predictions_cw, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, lgb_predictions_proba_cw)\n",
        "}\n",
        "\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 6. Rede Neural Densa (MLP) com TensorFlow/Keras (com Class Weight)\n",
        "print(\"Treinando Rede Neural Densa (MLP) com Class Weight...\")\n",
        "\n",
        "# o número de classes será 2 para a saída sigmoid\n",
        "num_classes_mlp_cw = len(np.unique(y_train))\n",
        "print(f\"Número de classes em y_train para MLP (Class Weight): {num_classes_mlp_cw}\")\n",
        "\n",
        "mlp_model_cw = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "mlp_model_cw.compile(optimizer='adam',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# treino do Modelo MLP\n",
        "epochs_mlp_cw = 20\n",
        "batch_size_mlp_cw = 64\n",
        "\n",
        "\n",
        "print(\"\\nTreinando MLP (Class Weight)...\")\n",
        "history_mlp_cw = mlp_model_cw.fit(X_train, y_train,\n",
        "                                  epochs=epochs_mlp_cw,\n",
        "                                  batch_size=batch_size_mlp_cw,\n",
        "                                  validation_data=(X_test, y_test),\n",
        "                                  class_weight=class_weight_dict,\n",
        "                                  verbose=1)\n",
        "\n",
        "\n",
        "print(\"\\nResumo do Modelo MLP (Class Weight):\")\n",
        "mlp_model_cw.summary()\n",
        "\n",
        "# avaliação do modelo\n",
        "print(\"\\nAvaliando MLP (Class Weight) no conjunto de teste...\")\n",
        "loss_mlp_cw, accuracy_mlp_cw = mlp_model_cw.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da MLP (Class Weight) no conjunto de teste: {accuracy_mlp_cw:.4f}\")\n",
        "\n",
        "# gera previsões de probabilidade e classes\n",
        "mlp_predictions_proba_positive_cw = mlp_model_cw.predict(X_test)\n",
        "mlp_predictions_cw = (mlp_predictions_proba_positive_cw >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da MLP (Class Weight):\")\n",
        "print(classification_report(y_test, mlp_predictions_cw))\n",
        "\n",
        "mlp_weighted_f1_cw = f1_score(y_test, mlp_predictions_cw, average='weighted')\n",
        "print(\"Weighted F1-score:\", mlp_weighted_f1_cw)\n",
        "\n",
        "try:\n",
        "    mlp_auc_score_cw = roc_auc_score(y_test, mlp_predictions_proba_positive_cw)\n",
        "    print(\"AUC:\", mlp_auc_score_cw)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    mlp_auc_score_cw = None\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "mlp_report_cw = classification_report(y_test, mlp_predictions_cw, output_dict=True)\n",
        "results['MLP (Class Weight)'] = {\n",
        "    'accuracy': accuracy_mlp_cw,\n",
        "    'precision (macro)': mlp_report_cw['macro avg']['precision'],\n",
        "    'recall (macro)': mlp_report_cw['macro avg']['recall'],\n",
        "    'f1-score (macro)': mlp_report_cw['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': mlp_weighted_f1_cw,\n",
        "    'auc': mlp_auc_score_cw\n",
        "}\n",
        "\n",
        "\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "SX_cvIpLkYMf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9c46213c-91aa-4bd1-8b47-6f8d9ebb2182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pesos das classes para TF-IDF (Class Weight):\n",
            "{np.int64(0): np.float64(0.6096817461285209), np.int64(1): np.float64(2.7793218454697053)}\n",
            "\n",
            "==================================================\n",
            "Treinando Modelos Avançados com TF-IDF usando CLASS WEIGHT...\n",
            "Treinando LightGBM com Class Weight...\n",
            "[LightGBM] [Info] Number of positive: 3598, number of negative: 16402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.389267 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 88850\n",
            "[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 2680\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avaliação do LightGBM (Class Weight):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.82      0.87      4101\n",
            "           1       0.45      0.68      0.54       899\n",
            "\n",
            "    accuracy                           0.79      5000\n",
            "   macro avg       0.68      0.75      0.70      5000\n",
            "weighted avg       0.84      0.79      0.81      5000\n",
            "\n",
            "Acurácia:  0.7928\n",
            "Weighted F1-score: 0.8075698695340502\n",
            "AUC: 0.8360408310840922\n",
            "--------------------------------------------------\n",
            "Treinando Rede Neural Densa (MLP) com Class Weight...\n",
            "Número de classes em y_train para MLP (Class Weight): 2\n",
            "\n",
            "Treinando MLP (Class Weight)...\n",
            "Epoch 1/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.7197 - loss: 0.6056 - val_accuracy: 0.7458 - val_loss: 0.4974\n",
            "Epoch 2/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 26ms/step - accuracy: 0.8105 - loss: 0.3940 - val_accuracy: 0.7614 - val_loss: 0.4880\n",
            "Epoch 3/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.8560 - loss: 0.2986 - val_accuracy: 0.8078 - val_loss: 0.4585\n",
            "Epoch 4/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - accuracy: 0.9093 - loss: 0.2070 - val_accuracy: 0.8028 - val_loss: 0.5124\n",
            "Epoch 5/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 44ms/step - accuracy: 0.9383 - loss: 0.1420 - val_accuracy: 0.8196 - val_loss: 0.5373\n",
            "Epoch 6/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9606 - loss: 0.0994 - val_accuracy: 0.8238 - val_loss: 0.6011\n",
            "Epoch 7/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.9712 - loss: 0.0783 - val_accuracy: 0.8256 - val_loss: 0.6654\n",
            "Epoch 8/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9779 - loss: 0.0594 - val_accuracy: 0.8274 - val_loss: 0.7261\n",
            "Epoch 9/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 26ms/step - accuracy: 0.9803 - loss: 0.0527 - val_accuracy: 0.8238 - val_loss: 0.7871\n",
            "Epoch 10/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 26ms/step - accuracy: 0.9860 - loss: 0.0377 - val_accuracy: 0.8270 - val_loss: 0.8035\n",
            "Epoch 11/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - accuracy: 0.9876 - loss: 0.0342 - val_accuracy: 0.8272 - val_loss: 0.8854\n",
            "Epoch 12/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 26ms/step - accuracy: 0.9899 - loss: 0.0279 - val_accuracy: 0.8292 - val_loss: 0.9774\n",
            "Epoch 13/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.9898 - loss: 0.0303 - val_accuracy: 0.8276 - val_loss: 0.9152\n",
            "Epoch 14/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 45ms/step - accuracy: 0.9915 - loss: 0.0290 - val_accuracy: 0.8270 - val_loss: 0.9911\n",
            "Epoch 15/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 30ms/step - accuracy: 0.9917 - loss: 0.0259 - val_accuracy: 0.8266 - val_loss: 0.9923\n",
            "Epoch 16/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.9924 - loss: 0.0232 - val_accuracy: 0.8210 - val_loss: 1.0256\n",
            "Epoch 17/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9934 - loss: 0.0204 - val_accuracy: 0.8128 - val_loss: 1.0805\n",
            "Epoch 18/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 26ms/step - accuracy: 0.9918 - loss: 0.0230 - val_accuracy: 0.8252 - val_loss: 1.1091\n",
            "Epoch 19/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 26ms/step - accuracy: 0.9936 - loss: 0.0194 - val_accuracy: 0.8260 - val_loss: 1.0808\n",
            "Epoch 20/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.9934 - loss: 0.0177 - val_accuracy: 0.8156 - val_loss: 1.2256\n",
            "\n",
            "Resumo do Modelo MLP (Class Weight):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,280,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,939,845\u001b[0m (15.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,939,845</span> (15.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,281\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,281</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,626,564\u001b[0m (10.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,626,564</span> (10.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando MLP (Class Weight) no conjunto de teste...\n",
            "\n",
            "Acurácia da MLP (Class Weight) no conjunto de teste: 0.8156\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
            "\n",
            "Avaliação completa da MLP (Class Weight):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.87      0.89      4101\n",
            "           1       0.49      0.55      0.52       899\n",
            "\n",
            "    accuracy                           0.82      5000\n",
            "   macro avg       0.69      0.71      0.70      5000\n",
            "weighted avg       0.82      0.82      0.82      5000\n",
            "\n",
            "Weighted F1-score: 0.8195258191616567\n",
            "AUC: 0.8155262871667266\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# --- pré-processamento para CNN ---\n",
        "\n",
        "# tokenização para CNN (Sequências de Inteiros)\n",
        "max_words = 10000\n",
        "max_sequence_length = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df_subset['processed_tokens'].apply(lambda tokens: ' '.join(tokens)))\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df_subset['processed_tokens'].apply(lambda tokens: ' '.join(tokens)))\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "print(\"\\nExemplo de sequências padded:\")\n",
        "print(padded_sequences[:5])\n",
        "print(\"\\nForma das sequências padded:\", padded_sequences.shape)\n",
        "\n",
        "# --- prepara labels para CNN ---\n",
        "encoded_y = df_subset['Label'].values\n",
        "\n",
        "print(\"\\nLabels originais (do df_subset):\", df_subset['Label'].unique())\n",
        "print(\"Labels usadas para treino (codificadas):\", np.unique(encoded_y))\n",
        "\n",
        "# --- divide dados para CNN (usando as sequências e labels codificadas) ---\n",
        "X_cnn = padded_sequences\n",
        "y_cnn = encoded_y\n",
        "\n",
        "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(\n",
        "    X_cnn, y_cnn, test_size=0.20, random_state=42, stratify=y_cnn\n",
        ")\n",
        "\n",
        "print(\"\\nForma dos dados de treino para CNN:\", X_train_cnn.shape, y_train_cnn.shape)\n",
        "print(\"Forma dos dados de teste para CNN:\", X_test_cnn.shape, y_test_cnn.shape)\n",
        "\n",
        "# --- cálculo de Class Weights para a CNN (baseado em y_train_cnn) ---\n",
        "classes_cnn = np.unique(y_train_cnn)\n",
        "class_weights_cnn = compute_class_weight(class_weight='balanced', classes=classes_cnn, y=y_train_cnn)\n",
        "class_weight_dict_cnn = dict(zip(classes_cnn, class_weights_cnn))\n",
        "\n",
        "print(\"\\nPesos das classes calculados para CNN (baseado em y_train_cnn):\")\n",
        "print(class_weight_dict_cnn)\n",
        "\n",
        "\n",
        "# --- constrói o Modelo CNN ---\n",
        "embedding_dim = 128\n",
        "filter_sizes = [3]\n",
        "num_filters = 128\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    Conv1D(filters=num_filters, kernel_size=filter_sizes[0], activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# --- treino do Modelo CNN (Com class_weight) ---\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "print(\"\\nTreinando CNN com class_weight...\")\n",
        "history = model.fit(X_train_cnn, y_train_cnn,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_split=0.1,\n",
        "                    verbose=1,\n",
        "                    class_weight=class_weight_dict_cnn)\n",
        "\n",
        "# --- avaliação do Modelo CNN ---\n",
        "print(\"\\nAvaliando CNN no conjunto de teste...\")\n",
        "loss, accuracy = model.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da CNN no conjunto de teste: {accuracy:.4f}\")\n",
        "\n",
        "# classification report para CNN\n",
        "cnn_predictions_probs = model.predict(X_test_cnn)\n",
        "cnn_predictions = (cnn_predictions_probs >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da CNN:\")\n",
        "print(classification_report(y_test_cnn, cnn_predictions))\n",
        "\n",
        "# cálculo de F1-score ponderado e AUC para a CNN\n",
        "cnn_f1_weighted = f1_score(y_test_cnn, cnn_predictions, average='weighted')\n",
        "\n",
        "try:\n",
        "    cnn_auc = roc_auc_score(y_test_cnn, cnn_predictions_probs)\n",
        "except ValueError as e:\n",
        "    cnn_auc = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "except Exception as e:\n",
        "    cnn_auc = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "\n",
        "\n",
        "print(\"F1-score ponderado (CNN):\", cnn_f1_weighted)\n",
        "if isinstance(cnn_auc, (float, int)):\n",
        "    print(\"AUC:\", cnn_auc)\n",
        "else:\n",
        "    print(\"AUC:\", cnn_auc)\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "try:\n",
        "    cnn_report = classification_report(y_test_cnn, cnn_predictions, output_dict=True)\n",
        "    results['CNN (Embedding+Seq, Class_Weight)'] = {\n",
        "        'accuracy': accuracy_score(y_test_cnn, cnn_predictions),\n",
        "        'precision (macro)': cnn_report['macro avg']['precision'],\n",
        "        'recall (macro)': cnn_report['macro avg']['recall'],\n",
        "        'f1-score (macro)': cnn_report['macro avg']['f1-score'],\n",
        "        'f1-score (weighted)': cnn_f1_weighted,\n",
        "        'auc': cnn_auc if isinstance(cnn_auc, (float, int)) else None\n",
        "    }\n",
        "except NameError:\n",
        "    print(\"Dicionário 'results' não encontrado. Os resultados da CNN (com class_weight) não foram armazenados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YxQ_zP_5hrEv",
        "outputId": "a86d4817-3010-4806-c812-ff79e014c65b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Exemplo de sequências padded:\n",
            "[[  57   84   90   47  156 1215   28 2589  146 3160 2199  101    1 2413\n",
            "  2589  101 2200   14  113  681  680 4581    1    6 2589  101 1262   48\n",
            "   194  348 3522    6 1263 3803    6   19  277 8319  997  942 2525   82\n",
            "    35   23 1801   62  303  198  249    8  282   60 1361    8  583 8319\n",
            "   375  363 8319 2161  198  370 1392 1008  363  252   47    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    1 7440  224  285   19    2    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [ 307   64 3058 6811 2360 1291  185  250   91   64    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [1769    1  101 6812 3804  195 8320  114    1  447  356  728 4822   13\n",
            "    97 1009 3389    1 2201 2730   32 1960  329   52    1  147 1532  728\n",
            "   418  211 8321 5121 1248 3389   15 3389   19  329   89 1802    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [1629    3 1079  164 4823  662   19 2202   13   75    1  663  129    7\n",
            "  1414 1561    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "\n",
            "Forma das sequências padded: (25000, 100)\n",
            "\n",
            "Labels originais (do df_subset): [0 1]\n",
            "Labels usadas para treino (codificadas): [0 1]\n",
            "\n",
            "Forma dos dados de treino para CNN: (20000, 100) (20000,)\n",
            "Forma dos dados de teste para CNN: (5000, 100) (5000,)\n",
            "\n",
            "Pesos das classes calculados para CNN (baseado em y_train_cnn):\n",
            "{np.int64(0): np.float64(0.6096817461285209), np.int64(1): np.float64(2.7793218454697053)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_1          │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_1          │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando CNN com class_weight...\n",
            "Epoch 1/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 41ms/step - accuracy: 0.6588 - loss: 0.5978 - val_accuracy: 0.7785 - val_loss: 0.4713\n",
            "Epoch 2/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 34ms/step - accuracy: 0.8581 - loss: 0.3207 - val_accuracy: 0.7785 - val_loss: 0.4866\n",
            "Epoch 3/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 36ms/step - accuracy: 0.9215 - loss: 0.1796 - val_accuracy: 0.8165 - val_loss: 0.4869\n",
            "Epoch 4/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.9647 - loss: 0.0884 - val_accuracy: 0.8365 - val_loss: 0.6431\n",
            "Epoch 5/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 36ms/step - accuracy: 0.9813 - loss: 0.0463 - val_accuracy: 0.8295 - val_loss: 0.8108\n",
            "Epoch 6/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 33ms/step - accuracy: 0.9903 - loss: 0.0239 - val_accuracy: 0.8120 - val_loss: 0.8975\n",
            "Epoch 7/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 36ms/step - accuracy: 0.9917 - loss: 0.0217 - val_accuracy: 0.8255 - val_loss: 1.0384\n",
            "Epoch 8/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 35ms/step - accuracy: 0.9938 - loss: 0.0141 - val_accuracy: 0.8100 - val_loss: 1.0357\n",
            "Epoch 9/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 38ms/step - accuracy: 0.9955 - loss: 0.0124 - val_accuracy: 0.8235 - val_loss: 1.1835\n",
            "Epoch 10/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.9953 - loss: 0.0128 - val_accuracy: 0.8100 - val_loss: 1.1451\n",
            "\n",
            "Avaliando CNN no conjunto de teste...\n",
            "\n",
            "Acurácia da CNN no conjunto de teste: 0.8134\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n",
            "\n",
            "Avaliação completa da CNN:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.86      0.88      4101\n",
            "           1       0.48      0.59      0.53       899\n",
            "\n",
            "    accuracy                           0.81      5000\n",
            "   macro avg       0.69      0.72      0.71      5000\n",
            "weighted avg       0.83      0.81      0.82      5000\n",
            "\n",
            "F1-score ponderado (CNN): 0.8200126650439746\n",
            "AUC: 0.8235961602463275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# --- reutiliza dados preparados na célula anterior para CNN ---\n",
        "\n",
        "# uso do class_weight_dict_cnn calculado na célula anterior\n",
        "class_weight_dict_lstm = class_weight_dict_cnn\n",
        "\n",
        "print(\"\\nPesos das classes calculados para LSTM (reutilizados de y_train_cnn):\")\n",
        "print(class_weight_dict_lstm)\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Construindo e Treinando o Modelo LSTM com CLASS WEIGHT...\")\n",
        "\n",
        "# --- constrói o Modelo LSTM ---\n",
        "\n",
        "embedding_dim = 128\n",
        "model_lstm_cw = Sequential([\n",
        "    # camada de Embedding: Reutiliza os mesmos parâmetros da CNN\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    # camada LSTM\n",
        "    LSTM(128),\n",
        "    # camada de Dropout\n",
        "    Dropout(0.5),\n",
        "    # camada densa (Fully Connected)\n",
        "    Dense(64, activation='relu'), # Camada densa adicional\n",
        "    Dropout(0.5),\n",
        "    # camada de saída: 1 unidade com ativação 'sigmoid' para classificação binária\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_lstm_cw.compile(optimizer='adam',\n",
        "                   loss='binary_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nResumo do Modelo LSTM (Class Weight):\")\n",
        "model_lstm_cw.summary()\n",
        "\n",
        "\n",
        "# --- treino do Modelo LSTM (Com class_weight) ---\n",
        "epochs_lstm_cw = 15\n",
        "batch_size_lstm_cw = 32\n",
        "\n",
        "print(\"\\nTreinando LSTM (Class Weight)...\")\n",
        "\n",
        "history_lstm_cw = model_lstm_cw.fit(X_train_cnn, y_train_cnn,\n",
        "                                  epochs=epochs_lstm_cw,\n",
        "                                  batch_size=batch_size_lstm_cw,\n",
        "                                  validation_data=(X_val_cnn, y_val_cnn),\n",
        "                                  class_weight=class_weight_dict_lstm,\n",
        "                                  verbose=1)\n",
        "\n",
        "\n",
        "# --- avaliação do Modelo LSTM  ---\n",
        "print(\"\\nAvaliando LSTM (Class Weight) no conjunto de teste FINAL...\")\n",
        "loss_lstm_test_cw, accuracy_lstm_test_cw = model_lstm_cw.evaluate(X_test_cnn_final, y_test_cnn_final, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da LSTM (Class Weight) no conjunto de teste FINAL: {accuracy_lstm_test_cw:.4f}\")\n",
        "\n",
        "# gera previsões (classes) e probabilidades para calcular métricas\n",
        "lstm_predictions_proba_positive_test_cw = model_lstm_cw.predict(X_test_cnn_final)\n",
        "\n",
        "# obtém as classes preditas\n",
        "lstm_predictions_test_cw = (lstm_predictions_proba_positive_test_cw >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da LSTM (Class Weight) no conjunto de teste FINAL:\")\n",
        "print(classification_report(y_test_cnn_final, lstm_predictions_test_cw))\n",
        "\n",
        "# cálculo de F1-score ponderado para LSTM\n",
        "lstm_weighted_f1_test_cw = f1_score(y_test_cnn_final, lstm_predictions_test_cw, average='weighted')\n",
        "print(\"Weighted F1-score:\", lstm_weighted_f1_test_cw)\n",
        "\n",
        "# cálculo de AUC para LSTM\n",
        "try:\n",
        "    lstm_auc_score_test_cw = roc_auc_score(y_test_cnn_final, lstm_predictions_proba_positive_test_cw)\n",
        "    print(\"AUC:\", lstm_auc_score_test_cw)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    lstm_auc_score_test_cw = None\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "# dicionário de resultados\n",
        "\n",
        "lstm_report_test_cw = classification_report(y_test_cnn_final, lstm_predictions_test_cw, output_dict=True)\n",
        "\n",
        "try:\n",
        "    results['LSTM (Embedding+Seq, Class_Weight) Test'] = {\n",
        "        'accuracy': accuracy_lstm_test_cw,\n",
        "        'precision (macro)': lstm_report_test_cw['macro avg']['precision'],\n",
        "        'recall (macro)': lstm_report_test_cw['macro avg']['recall'],\n",
        "        'f1-score (macro)': lstm_report_test_cw['macro avg']['f1-score'],\n",
        "        'f1-score (weighted)': lstm_weighted_f1_test_cw,\n",
        "        'auc': lstm_auc_score_test_cw\n",
        "    }\n",
        "except NameError:\n",
        "     print(\"Dicionário 'results' não encontrado. Os resultados da LSTM (com class_weight) não foram armazenados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rKPiYf2Libe-",
        "outputId": "f7ca2e20-9dd8-4198-91b9-8ee1da910e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pesos das classes calculados para LSTM (reutilizados de y_train_cnn):\n",
            "{np.int64(0): np.float64(0.6097065279245589), np.int64(1): np.float64(2.7788069655427936)}\n",
            "\n",
            "==================================================\n",
            "Construindo e Treinando o Modelo LSTM com CLASS WEIGHT...\n",
            "\n",
            "Resumo do Modelo LSTM (Class Weight):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando LSTM (Class Weight)...\n",
            "Epoch 1/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 181ms/step - accuracy: 0.5565 - loss: 0.6899 - val_accuracy: 0.8174 - val_loss: 0.6914\n",
            "Epoch 2/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 184ms/step - accuracy: 0.4515 - loss: 0.7014 - val_accuracy: 0.2286 - val_loss: 0.6796\n",
            "Epoch 3/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 186ms/step - accuracy: 0.4730 - loss: 0.7272 - val_accuracy: 0.8204 - val_loss: 0.6646\n",
            "Epoch 4/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 180ms/step - accuracy: 0.5088 - loss: 0.6941 - val_accuracy: 0.8202 - val_loss: 0.6880\n",
            "Epoch 5/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 180ms/step - accuracy: 0.4873 - loss: 0.6908 - val_accuracy: 0.2040 - val_loss: 0.6962\n",
            "Epoch 6/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 184ms/step - accuracy: 0.4680 - loss: 0.6793 - val_accuracy: 0.2088 - val_loss: 0.6964\n",
            "Epoch 7/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 187ms/step - accuracy: 0.2244 - loss: 0.6863 - val_accuracy: 0.2206 - val_loss: 0.6865\n",
            "Epoch 8/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 186ms/step - accuracy: 0.2856 - loss: 0.6882 - val_accuracy: 0.2064 - val_loss: 0.6860\n",
            "Epoch 9/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 184ms/step - accuracy: 0.3289 - loss: 0.6827 - val_accuracy: 0.2076 - val_loss: 0.6867\n",
            "Epoch 10/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 186ms/step - accuracy: 0.3836 - loss: 0.6824 - val_accuracy: 0.2088 - val_loss: 0.7138\n",
            "Epoch 11/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 184ms/step - accuracy: 0.2371 - loss: 0.6765 - val_accuracy: 0.2030 - val_loss: 0.7341\n",
            "Epoch 12/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 186ms/step - accuracy: 0.3656 - loss: 0.6630 - val_accuracy: 0.5848 - val_loss: 0.5968\n",
            "Epoch 13/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 183ms/step - accuracy: 0.7194 - loss: 0.4907 - val_accuracy: 0.7466 - val_loss: 0.5964\n",
            "Epoch 14/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 181ms/step - accuracy: 0.8289 - loss: 0.3596 - val_accuracy: 0.7688 - val_loss: 0.4066\n",
            "Epoch 15/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 188ms/step - accuracy: 0.8828 - loss: 0.2519 - val_accuracy: 0.8086 - val_loss: 0.4932\n",
            "\n",
            "Avaliando LSTM (Class Weight) no conjunto de teste FINAL...\n",
            "\n",
            "Acurácia da LSTM (Class Weight) no conjunto de teste FINAL: 0.8020\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step\n",
            "\n",
            "Avaliação completa da LSTM (Class Weight) no conjunto de teste FINAL:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.83      0.87      4101\n",
            "           1       0.47      0.68      0.55       899\n",
            "\n",
            "    accuracy                           0.80      5000\n",
            "   macro avg       0.69      0.75      0.71      5000\n",
            "weighted avg       0.84      0.80      0.82      5000\n",
            "\n",
            "Weighted F1-score: 0.8150811212227593\n",
            "AUC: 0.8320351882486678\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Oversampling**"
      ],
      "metadata": {
        "id": "vmNe7p3YsJPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como segunda abordagem para tratar o desbalanceamento entre as classes, foi utilizado o método de oversampling sintético **SMOTE (Synthetic Minority Over-sampling Technique)**. Essa técnica atua gerando novos exemplos sintéticos para as classes minoritárias com base na interpolação entre amostras reais vizinhas no espaço de atributos, equilibrando a distribuição sem simplesmente duplicar instâncias existentes.\n",
        "\n",
        "O SMOTE foi aplicado exclusivamente ao conjunto de treino, preservando a distribuição real nos dados de teste. Com isso, as duas classes (0 e 1) passaram a ter exatamente o mesmo número de instâncias (16.402), conforme e.videnciado após o reamostramento.\n",
        "\n",
        "Esse balanceamento favorece o aprendizado de modelos como Regressão Logística, SVM, Random Forest, LightGBM e MLP, aumentando sua capacidade de generalização para exemplos das classes originalmente sub-representadas. A técnica é especialmente vantajosa por evitar overfitting comum em métodos de duplicação, ao mesmo tempo em que permite o uso direto dos classificadores."
      ],
      "metadata": {
        "id": "t8cD4DUEwCem"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BB2rOau4weQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ac12ab-480c-4dc9-e7d1-9ac9ba9b3eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T1kdoKHwzSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08e6ad3a-75dd-4876-c25e-42d20901082f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Distribuição das classes no treino (antes do balanceamento):\n",
            "Label\n",
            "0    16402\n",
            "1     3598\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Forma dos dados de treino após SMOTE: (32804, 5000) (32804,)\n",
            "\n",
            "Distribuição das classes no treino (depois do SMOTE):\n",
            "Label\n",
            "0    16402\n",
            "1    16402\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nDistribuição das classes no treino (antes do balanceamento):\")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "# inicializa SMOTE\n",
        "# random_state para reprodutibilidade\n",
        "# sampling_strategy='auto' reamostra todas as classes minoritárias para igualar a classe majoritária\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "# aplica SMOTE apenas nos dados de TREINO\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"\\nForma dos dados de treino após SMOTE:\", X_train_resampled.shape, y_train_resampled.shape)\n",
        "print(\"\\nDistribuição das classes no treino (depois do SMOTE):\")\n",
        "print(y_train_resampled.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdx1A1zaxRKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5ae475-842d-4acb-aeb4-06abb136f8ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando modelos tradicionais COM SMOTE...\n",
            "--------------------------------------------------\n",
            "Treinando Regressão Logística com SMOTE-balanced data...\n",
            "Avaliação da Regressão Logística (SMOTE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.79      0.85      4101\n",
            "           1       0.42      0.69      0.52       899\n",
            "\n",
            "    accuracy                           0.77      5000\n",
            "   macro avg       0.67      0.74      0.69      5000\n",
            "weighted avg       0.83      0.77      0.79      5000\n",
            "\n",
            "Acurácia:  0.7732\n",
            "Weighted F1-score: 0.7921488448488312\n",
            "AUC: 0.827631096786128\n",
            "--------------------------------------------------\n",
            "Treinando Naive Bayes Multinomial com SMOTE-balanced data...\n",
            "Avaliação do Naive Bayes Multinomial (SMOTE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.75      0.84      4101\n",
            "           1       0.41      0.78      0.54       899\n",
            "\n",
            "    accuracy                           0.76      5000\n",
            "   macro avg       0.68      0.77      0.69      5000\n",
            "weighted avg       0.84      0.76      0.78      5000\n",
            "\n",
            "Acurácia:  0.7584\n",
            "Weighted F1-score: 0.7827636662425719\n",
            "AUC: 0.8549287606945755\n",
            "--------------------------------------------------\n",
            "Treinando SVM (Kernel Linear) com SMOTE-balanced data...\n",
            "Avaliação do SVM (SMOTE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.79      0.85      4101\n",
            "           1       0.41      0.68      0.51       899\n",
            "\n",
            "    accuracy                           0.77      5000\n",
            "   macro avg       0.67      0.73      0.68      5000\n",
            "weighted avg       0.83      0.77      0.79      5000\n",
            "\n",
            "Acurácia:  0.7692\n",
            "Weighted F1-score: 0.7885649192645167\n",
            "AUC: 0.8166844734415953\n",
            "--------------------------------------------------\n",
            "Treinando Random Forest com SMOTE-balanced data...\n",
            "Avaliação do Random Forest (SMOTE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.87      0.88      4101\n",
            "           1       0.48      0.52      0.50       899\n",
            "\n",
            "    accuracy                           0.81      5000\n",
            "   macro avg       0.68      0.70      0.69      5000\n",
            "weighted avg       0.82      0.81      0.81      5000\n",
            "\n",
            "Acurácia:  0.8108\n",
            "Weighted F1-score: 0.8142599776852874\n",
            "AUC: 0.8173833181575669\n",
            "\n",
            "Resultados acumulados (incluindo SMOTE) até agora:\n",
            "{'Logistic Regression (SMOTE)': {'accuracy': 0.7732, 'precision (macro)': 0.6705950234403173, 'recall (macro)': 0.7405847186136266, 'f1-score (macro)': 0.6868116899773775, 'f1-score (weighted)': 0.7921488448488312, 'auc': np.float64(0.827631096786128)}, 'Multinomial NB (SMOTE)': {'accuracy': 0.7584, 'precision (macro)': 0.6751236959232486, 'recall (macro)': 0.7676054485205188, 'f1-score (macro)': 0.6871601621877739, 'f1-score (weighted)': 0.7827636662425719, 'auc': np.float64(0.8549287606945755)}, 'SVM (SMOTE)': {'accuracy': 0.7692, 'precision (macro)': 0.6661410845273188, 'recall (macro)': 0.7346722726137227, 'f1-score (macro)': 0.6816572098076503, 'f1-score (weighted)': 0.7885649192645167, 'auc': np.float64(0.8166844734415953)}, 'Random Forest (SMOTE)': {'accuracy': 0.8108, 'precision (macro)': 0.6847660339526362, 'recall (macro)': 0.6988024028432253, 'f1-score (macro)': 0.6911616604820646, 'f1-score (weighted)': 0.8142599776852874, 'auc': np.float64(0.8173833181575669)}}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"Treinando modelos tradicionais COM SMOTE...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 1. Regressão Logística\n",
        "print(\"Treinando Regressão Logística com SMOTE-balanced data...\")\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train_resampled, y_train_resampled)\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "lr_predictions_proba = lr_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação da Regressão Logística (SMOTE):\")\n",
        "print(classification_report(y_test, lr_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lr_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, lr_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, lr_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "lr_report = classification_report(y_test, lr_predictions, output_dict=True)\n",
        "results['Logistic Regression (SMOTE)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lr_predictions),\n",
        "    'precision (macro)': lr_report['macro avg']['precision'],\n",
        "    'recall (macro)': lr_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lr_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lr_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, lr_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 2. Naive Bayes Multinomial\n",
        "print(\"Treinando Naive Bayes Multinomial com SMOTE-balanced data...\")\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_resampled, y_train_resampled)\n",
        "nb_predictions = nb_model.predict(X_test)\n",
        "nb_predictions_proba = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do Naive Bayes Multinomial (SMOTE):\")\n",
        "print(classification_report(y_test, nb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, nb_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, nb_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, nb_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "nb_report = classification_report(y_test, nb_predictions, output_dict=True)\n",
        "results['Multinomial NB (SMOTE)'] = {\n",
        "    'accuracy': accuracy_score(y_test, nb_predictions),\n",
        "    'precision (macro)': nb_report['macro avg']['precision'],\n",
        "    'recall (macro)': nb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': nb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, nb_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, nb_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 3. Support Vector Machine (SVM)\n",
        "\n",
        "print(\"Treinando SVM (Kernel Linear) com SMOTE-balanced data...\")\n",
        "\n",
        "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
        "svm_model.fit(X_train_resampled, y_train_resampled)\n",
        "svm_predictions = svm_model.predict(X_test)\n",
        "svm_predictions_proba = svm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do SVM (SMOTE):\")\n",
        "print(classification_report(y_test, svm_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, svm_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, svm_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, svm_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "svm_report = classification_report(y_test, svm_predictions, output_dict=True)\n",
        "results['SVM (SMOTE)'] = {\n",
        "    'accuracy': accuracy_score(y_test, svm_predictions),\n",
        "    'precision (macro)': svm_report['macro avg']['precision'],\n",
        "    'recall (macro)': svm_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': svm_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, svm_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, svm_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 4. Random Forest Classifier\n",
        "print(\"Treinando Random Forest com SMOTE-balanced data...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_resampled, y_train_resampled)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "rf_predictions_proba = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do Random Forest (SMOTE):\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, rf_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, rf_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, rf_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "rf_report = classification_report(y_test, rf_predictions, output_dict=True)\n",
        "results['Random Forest (SMOTE)'] = {\n",
        "    'accuracy': accuracy_score(y_test, rf_predictions),\n",
        "    'precision (macro)': rf_report['macro avg']['precision'],\n",
        "    'recall (macro)': rf_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': rf_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, rf_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, rf_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"\\nResultados acumulados (incluindo SMOTE) até agora:\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DmJa4aryxuX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3426e7a6-c3b1-4aeb-fae2-683cbf15d508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\n",
            "\n",
            "==================================================\n",
            "Treinando LightGBM com SMOTE-balanced data...\n",
            "Número de classes em y_train_resampled para LightGBM: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 16402, number of negative: 16402\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 3.912111 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 129350\n",
            "[LightGBM] [Info] Number of data points in the train set: 32804, number of used features: 3514\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avaliação do LightGBM (SMOTE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.80      0.85      4101\n",
            "           1       0.42      0.67      0.51       899\n",
            "\n",
            "    accuracy                           0.77      5000\n",
            "   macro avg       0.67      0.73      0.68      5000\n",
            "weighted avg       0.83      0.77      0.79      5000\n",
            "\n",
            "Acurácia:  0.7732\n",
            "Weighted F1-score: 0.7913133307892427\n",
            "AUC: 0.8297703509195918\n",
            "--------------------------------------------------\n",
            "Treinando Rede Neural Densa (MLP) com SMOTE-balanced data...\n",
            "Número de classes em y_train_resampled para MLP: 2\n",
            "\n",
            "Treinando MLP (SMOTE)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.7634 - loss: 0.4726 - val_accuracy: 0.8224 - val_loss: 0.4253\n",
            "Epoch 2/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - accuracy: 0.9378 - loss: 0.1745 - val_accuracy: 0.8332 - val_loss: 0.4955\n",
            "Epoch 3/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 32ms/step - accuracy: 0.9715 - loss: 0.0913 - val_accuracy: 0.8422 - val_loss: 0.6018\n",
            "Epoch 4/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 30ms/step - accuracy: 0.9828 - loss: 0.0606 - val_accuracy: 0.8352 - val_loss: 0.6713\n",
            "Epoch 5/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 27ms/step - accuracy: 0.9891 - loss: 0.0400 - val_accuracy: 0.8360 - val_loss: 0.7464\n",
            "Epoch 6/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - accuracy: 0.9917 - loss: 0.0326 - val_accuracy: 0.8382 - val_loss: 0.8491\n",
            "Epoch 7/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - accuracy: 0.9916 - loss: 0.0286 - val_accuracy: 0.8348 - val_loss: 0.8870\n",
            "Epoch 8/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 23ms/step - accuracy: 0.9930 - loss: 0.0256 - val_accuracy: 0.8332 - val_loss: 0.9818\n",
            "Epoch 9/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 26ms/step - accuracy: 0.9931 - loss: 0.0225 - val_accuracy: 0.8354 - val_loss: 0.9833\n",
            "Epoch 10/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.9930 - loss: 0.0235 - val_accuracy: 0.8366 - val_loss: 1.0263\n",
            "Epoch 11/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.9933 - loss: 0.0207 - val_accuracy: 0.8402 - val_loss: 1.0728\n",
            "Epoch 12/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - accuracy: 0.9922 - loss: 0.0222 - val_accuracy: 0.8376 - val_loss: 1.1182\n",
            "Epoch 13/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 23ms/step - accuracy: 0.9936 - loss: 0.0191 - val_accuracy: 0.8404 - val_loss: 1.2025\n",
            "Epoch 14/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 23ms/step - accuracy: 0.9935 - loss: 0.0188 - val_accuracy: 0.8392 - val_loss: 1.2397\n",
            "Epoch 15/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.9943 - loss: 0.0173 - val_accuracy: 0.8400 - val_loss: 1.2123\n",
            "Epoch 16/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 25ms/step - accuracy: 0.9950 - loss: 0.0150 - val_accuracy: 0.8370 - val_loss: 1.2921\n",
            "Epoch 17/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.9947 - loss: 0.0159 - val_accuracy: 0.8414 - val_loss: 1.3603\n",
            "Epoch 18/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - accuracy: 0.9934 - loss: 0.0172 - val_accuracy: 0.8382 - val_loss: 1.3495\n",
            "Epoch 19/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.9943 - loss: 0.0164 - val_accuracy: 0.8394 - val_loss: 1.4143\n",
            "Epoch 20/20\n",
            "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 24ms/step - accuracy: 0.9932 - loss: 0.0173 - val_accuracy: 0.8376 - val_loss: 1.4284\n",
            "\n",
            "Resumo do Modelo MLP (SMOTE):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,280,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,939,845\u001b[0m (15.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,939,845</span> (15.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,281\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,281</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,626,564\u001b[0m (10.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,626,564</span> (10.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando MLP (SMOTE) no conjunto de teste...\n",
            "\n",
            "Acurácia da MLP (SMOTE) no conjunto de teste: 0.8376\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\n",
            "Avaliação completa da MLP (SMOTE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.93      0.90      4101\n",
            "           1       0.56      0.42      0.48       899\n",
            "\n",
            "    accuracy                           0.84      5000\n",
            "   macro avg       0.72      0.68      0.69      5000\n",
            "weighted avg       0.82      0.84      0.83      5000\n",
            "\n",
            "Weighted F1-score: 0.8283018149738962\n",
            "AUC: 0.8255558548214862\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# --- Modelos Avançados com TF-IDF ---\n",
        "\n",
        "# 5. LightGBM (Gradient Boosting)\n",
        "!pip install lightgbm\n",
        "\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Treinando LightGBM com SMOTE-balanced data...\")\n",
        "\n",
        "# o número de classes será 2\n",
        "num_classes_lgbm = len(np.unique(y_train_resampled))\n",
        "print(f\"Número de classes em y_train_resampled para LightGBM: {num_classes_lgbm}\")\n",
        "\n",
        "# converte matriz esparsa para o formato LightGBM\n",
        "lgb_model = lgb.LGBMClassifier(objective='binary', random_state=42)\n",
        "\n",
        "lgb_model.fit(X_train_resampled, y_train_resampled)\n",
        "lgb_predictions = lgb_model.predict(X_test)\n",
        "lgb_predictions_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do LightGBM (SMOTE):\")\n",
        "print(classification_report(y_test, lgb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lgb_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, lgb_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, lgb_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "lgb_report = classification_report(y_test, lgb_predictions, output_dict=True)\n",
        "results['LightGBM (SMOTE)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lgb_predictions),\n",
        "    'precision (macro)': lgb_report['macro avg']['precision'],\n",
        "    'recall (macro)': lgb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lgb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lgb_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, lgb_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 6. Rede Neural Densa (MLP) com TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "print(\"Treinando Rede Neural Densa (MLP) com SMOTE-balanced data...\")\n",
        "\n",
        "num_classes_mlp_smote = len(np.unique(y_train_resampled))\n",
        "print(f\"Número de classes em y_train_resampled para MLP: {num_classes_mlp_smote}\")\n",
        "\n",
        "# constrói o Modelo MLP\n",
        "mlp_model_smote = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train_resampled.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "mlp_model_smote.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# treino do Modelo MLP\n",
        "epochs_mlp_smote = 20\n",
        "batch_size_mlp_smote = 64\n",
        "\n",
        "\n",
        "print(\"\\nTreinando MLP (SMOTE)...\")\n",
        "history_mlp_smote = mlp_model_smote.fit(X_train_resampled, y_train_resampled,\n",
        "                          epochs=epochs_mlp_smote,\n",
        "                          batch_size=batch_size_mlp_smote,\n",
        "                          validation_data=(X_test, y_test),\n",
        "                          verbose=1)\n",
        "\n",
        "\n",
        "print(\"\\nResumo do Modelo MLP (SMOTE):\")\n",
        "mlp_model_smote.summary()\n",
        "\n",
        "# avaliação do Modelo MLP\n",
        "print(\"\\nAvaliando MLP (SMOTE) no conjunto de teste...\")\n",
        "loss_mlp_smote, accuracy_mlp_smote = mlp_model_smote.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da MLP (SMOTE) no conjunto de teste: {accuracy_mlp_smote:.4f}\")\n",
        "\n",
        "# gera previsões de probabilidade\n",
        "mlp_predictions_proba_positive_smote = mlp_model_smote.predict(X_test) # Forma (n_samples, 1)\n",
        "\n",
        "# obtém as classes preditas\n",
        "mlp_predictions_smote = (mlp_predictions_proba_positive_smote >= 0.5).astype(int).flatten() # Converter para inteiros e garantir que é 1D\n",
        "\n",
        "print(\"\\nAvaliação completa da MLP (SMOTE):\")\n",
        "print(classification_report(y_test, mlp_predictions_smote))\n",
        "\n",
        "# cálculo de F1-score ponderado para MLP (SMOTE)\n",
        "mlp_weighted_f1_smote = f1_score(y_test, mlp_predictions_smote, average='weighted')\n",
        "print(\"Weighted F1-score:\", mlp_weighted_f1_smote)\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    mlp_auc_score_smote = roc_auc_score(y_test, mlp_predictions_proba_positive_smote)\n",
        "    print(\"AUC:\", mlp_auc_score_smote)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    mlp_auc_score_smote = None\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "mlp_report_smote = classification_report(y_test, mlp_predictions_smote, output_dict=True)\n",
        "results['MLP (SMOTE)'] = {\n",
        "    'accuracy': accuracy_mlp_smote,\n",
        "    'precision (macro)': mlp_report_smote['macro avg']['precision'],\n",
        "    'recall (macro)': mlp_report_smote['macro avg']['recall'],\n",
        "    'f1-score (macro)': mlp_report_smote['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': mlp_weighted_f1_smote,\n",
        "    'auc': mlp_auc_score_smote\n",
        "}\n",
        "\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# ---pré-processamento para CNN/LSTM  ---\n",
        "\n",
        "max_words = 10000\n",
        "max_sequence_length = 100\n",
        "\n",
        "all_texts = df_subset['processed_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "all_labels = df_subset['Label']\n",
        "\n",
        "# --- tokenização ---\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(all_texts)\n",
        "\n",
        "# converte textos para sequências de inteiros\n",
        "all_sequences = tokenizer.texts_to_sequences(all_texts)\n",
        "\n",
        "# --- padding das Sequências ---\n",
        "all_padded_sequences = pad_sequences(all_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "print(\"\\nExemplo de sequências padded (df_subset completo):\")\n",
        "print(all_padded_sequences[:5])\n",
        "print(\"\\nForma das sequências padded (df_subset completo):\", all_padded_sequences.shape)\n",
        "\n",
        "# --- prepara labels para CNN/LSTM ---\n",
        "encoded_all_labels = all_labels.values\n",
        "\n",
        "print(\"\\nLabels do df_subset:\", np.unique(encoded_all_labels))\n",
        "\n",
        "# --- divide dados para CNN/LSTM (a partir do df_subset completo) ---\n",
        "X_train_val_cnn, X_test_cnn_final, y_train_val_cnn, y_test_cnn_final = train_test_split(\n",
        "    all_padded_sequences, encoded_all_labels, test_size=0.20, random_state=42, stratify=encoded_all_labels\n",
        ")\n",
        "\n",
        "X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(\n",
        "    X_train_val_cnn, y_train_val_cnn, test_size=0.25, random_state=42, stratify=y_train_val_cnn\n",
        ")\n",
        "\n",
        "print(\"\\nForma dos dados de treino para CNN (Antes do Oversampling):\", X_train_cnn.shape, y_train_cnn.shape)\n",
        "print(\"Forma dos dados de validação para CNN:\", X_val_cnn.shape, y_val_cnn.shape)\n",
        "print(\"Forma dos dados de teste FINAL para CNN:\", X_test_cnn_final.shape, y_test_cnn_final.shape)\n",
        "\n",
        "# --- aplica Oversampling (SMOTE) ---\n",
        "print(\"\\nAplicando SMOTE oversampling ao conjunto de treino...\")\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_cnn_res, y_train_cnn_res = smote.fit_resample(X_train_cnn, y_train_cnn)\n",
        "\n",
        "print(f\"Forma dos dados de treino para CNN APÓS Oversampling (SMOTE): {X_train_cnn_res.shape}, {y_train_cnn_res.shape}\")\n",
        "print(\"Contagem de classes no treino APÓS Oversampling:\", np.bincount(y_train_cnn_res))\n",
        "\n",
        "\n",
        "# --- constrói o Modelo CNN ---\n",
        "embedding_dim = 128\n",
        "filter_sizes = [3]\n",
        "num_filters = 128\n",
        "\n",
        "model_cnn_oversample = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    Conv1D(filters=num_filters, kernel_size=filter_sizes[0], activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_cnn_oversample.compile(optimizer='adam',\n",
        "                             loss='binary_crossentropy',\n",
        "                             metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nResumo do Modelo CNN (Oversample):\")\n",
        "model_cnn_oversample.summary()\n",
        "\n",
        "# --- treino do Modelo CNN (Com Oversampling) ---\n",
        "epochs_cnn_oversample = 10\n",
        "batch_size_cnn_oversample = 32\n",
        "\n",
        "print(\"\\nTreinando CNN com Oversampling...\")\n",
        "\n",
        "history_cnn_oversample = model_cnn_oversample.fit(X_train_cnn_res, y_train_cnn_res,\n",
        "                                                  epochs=epochs_cnn_oversample,\n",
        "                                                  batch_size=batch_size_cnn_oversample,\n",
        "                                                  validation_data=(X_val_cnn, y_val_cnn),\n",
        "                                                  verbose=1)\n",
        "\n",
        "# --- avaliação do modelo ---\n",
        "print(\"\\nAvaliando CNN (Oversample) no conjunto de teste FINAL...\")\n",
        "\n",
        "loss_cnn_test_os, accuracy_cnn_test_os = model_cnn_oversample.evaluate(X_test_cnn_final, y_test_cnn_final, verbose=0)\n",
        "print(f\"\\nAcurácia da CNN (Oversample) no conjunto de teste FINAL: {accuracy_cnn_test_os:.4f}\")\n",
        "\n",
        "# gera previsões de probabilidade\n",
        "cnn_predictions_probs_os = model_cnn_oversample.predict(X_test_cnn_final)\n",
        "cnn_predictions_os = (cnn_predictions_probs_os >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da CNN (Oversample) no conjunto de teste FINAL:\")\n",
        "print(classification_report(y_test_cnn_final, cnn_predictions_os))\n",
        "\n",
        "# cálculo de F1-score ponderado e AUC para a CNN (Oversample)\n",
        "cnn_f1_weighted_os = f1_score(y_test_cnn_final, cnn_predictions_os, average='weighted')\n",
        "\n",
        "try:\n",
        "    cnn_auc_os = roc_auc_score(y_test_cnn_final, cnn_predictions_probs_os)\n",
        "except ValueError as e:\n",
        "    cnn_auc_os = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "except Exception as e:\n",
        "    cnn_auc_os = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "\n",
        "\n",
        "print(\"F1-score ponderado (CNN Oversample):\", cnn_f1_weighted_os)\n",
        "if isinstance(cnn_auc_os, (float, int)):\n",
        "    print(\"AUC:\", cnn_auc_os)\n",
        "else:\n",
        "    print(\"AUC:\", cnn_auc_os)\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "try:\n",
        "    cnn_report_os = classification_report(y_test_cnn_final, cnn_predictions_os, output_dict=True)\n",
        "    results['CNN (Embedding+Seq, Oversample) Test'] = {\n",
        "        'accuracy': accuracy_cnn_test_os,\n",
        "        'precision (macro)': cnn_report_os['macro avg']['precision'],\n",
        "        'recall (macro)': cnn_report_os['macro avg']['recall'],\n",
        "        'f1-score (macro)': cnn_report_os['macro avg']['f1-score'],\n",
        "        'f1-score (weighted)': cnn_f1_weighted_os,\n",
        "        'auc': cnn_auc_os if isinstance(cnn_auc_os, (float, int)) else None\n",
        "    }\n",
        "except NameError:\n",
        "    print(\"Dicionário 'results' não encontrado. Os resultados da CNN (com oversampling) não foram armazenados.\")"
      ],
      "metadata": {
        "id": "T95vvtxLTlNB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0ed95458-86dd-4ac9-a9b5-850bde345372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Exemplo de sequências padded (df_subset completo):\n",
            "[[  57   84   90   47  156 1215   28 2589  146 3160 2199  101    1 2413\n",
            "  2589  101 2200   14  113  681  680 4581    1    6 2589  101 1262   48\n",
            "   194  348 3522    6 1263 3803    6   19  277 8319  997  942 2525   82\n",
            "    35   23 1801   62  303  198  249    8  282   60 1361    8  583 8319\n",
            "   375  363 8319 2161  198  370 1392 1008  363  252   47    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    1 7440  224  285   19    2    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [ 307   64 3058 6811 2360 1291  185  250   91   64    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [1769    1  101 6812 3804  195 8320  114    1  447  356  728 4822   13\n",
            "    97 1009 3389    1 2201 2730   32 1960  329   52    1  147 1532  728\n",
            "   418  211 8321 5121 1248 3389   15 3389   19  329   89 1802    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [1629    3 1079  164 4823  662   19 2202   13   75    1  663  129    7\n",
            "  1414 1561    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "\n",
            "Forma das sequências padded (df_subset completo): (25000, 100)\n",
            "\n",
            "Labels do df_subset: [0 1]\n",
            "\n",
            "Forma dos dados de treino para CNN (Antes do Oversampling): (15000, 100) (15000,)\n",
            "Forma dos dados de validação para CNN: (5000, 100) (5000,)\n",
            "Forma dos dados de teste FINAL para CNN: (5000, 100) (5000,)\n",
            "\n",
            "Aplicando SMOTE oversampling ao conjunto de treino...\n",
            "Forma dos dados de treino para CNN APÓS Oversampling (SMOTE): (24602, 100), (24602,)\n",
            "Contagem de classes no treino APÓS Oversampling: [12301 12301]\n",
            "\n",
            "Resumo do Modelo CNN (Oversample):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_3          │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_3          │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando CNN com Oversampling...\n",
            "Epoch 1/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - accuracy: 0.6756 - loss: 0.5991 - val_accuracy: 0.7040 - val_loss: 0.5389\n",
            "Epoch 2/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.8349 - loss: 0.3635 - val_accuracy: 0.7398 - val_loss: 0.5146\n",
            "Epoch 3/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.9412 - loss: 0.1568 - val_accuracy: 0.6792 - val_loss: 0.8654\n",
            "Epoch 4/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9776 - loss: 0.0617 - val_accuracy: 0.7370 - val_loss: 1.0194\n",
            "Epoch 5/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 37ms/step - accuracy: 0.9886 - loss: 0.0313 - val_accuracy: 0.7384 - val_loss: 1.2431\n",
            "Epoch 6/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.9922 - loss: 0.0187 - val_accuracy: 0.6860 - val_loss: 1.6011\n",
            "Epoch 7/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9929 - loss: 0.0194 - val_accuracy: 0.7034 - val_loss: 1.6680\n",
            "Epoch 8/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9936 - loss: 0.0178 - val_accuracy: 0.7318 - val_loss: 1.5550\n",
            "Epoch 9/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9944 - loss: 0.0168 - val_accuracy: 0.6490 - val_loss: 2.2847\n",
            "Epoch 10/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9942 - loss: 0.0145 - val_accuracy: 0.7334 - val_loss: 1.7112\n",
            "\n",
            "Avaliando CNN (Oversample) no conjunto de teste FINAL...\n",
            "\n",
            "Acurácia da CNN (Oversample) no conjunto de teste FINAL: 0.7432\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
            "\n",
            "Avaliação completa da CNN (Oversample) no conjunto de teste FINAL:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.78      0.83      4101\n",
            "           1       0.36      0.56      0.44       899\n",
            "\n",
            "    accuracy                           0.74      5000\n",
            "   macro avg       0.63      0.67      0.64      5000\n",
            "weighted avg       0.79      0.74      0.76      5000\n",
            "\n",
            "F1-score ponderado (CNN Oversample): 0.7624871840203278\n",
            "AUC: 0.7327832355384711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "# --- reutiliza dados preparados na célula anterior para CNN/LSTM ---\n",
        "\n",
        "# --- divide dados para CNN/LSTM (a partir do df_subset completo) ---\n",
        "\n",
        "X_train_val_lstm, X_test_lstm_final, y_train_val_lstm, y_test_lstm_final = train_test_split(\n",
        "    all_padded_sequences, encoded_all_labels, test_size=0.20, random_state=42, stratify=encoded_all_labels\n",
        ")\n",
        "\n",
        "X_train_lstm, X_val_lstm, y_train_lstm, y_val_lstm = train_test_split(\n",
        "    X_train_val_lstm, y_train_val_lstm, test_size=0.25, random_state=42, stratify=y_train_val_lstm\n",
        ")\n",
        "\n",
        "print(\"\\nForma dos dados de treino para LSTM (Antes do Oversampling):\", X_train_lstm.shape, y_train_lstm.shape)\n",
        "print(\"Forma dos dados de validação para LSTM:\", X_val_lstm.shape, y_val_lstm.shape)\n",
        "print(\"Forma dos dados de teste FINAL para LSTM:\", X_test_lstm_final.shape, y_test_lstm_final.shape)\n",
        "\n",
        "\n",
        "# --- aplica oversampling---\n",
        "print(\"\\nAplicando SMOTE oversampling ao conjunto de treino para LSTM...\")\n",
        "\n",
        "smote_lstm = SMOTE(random_state=42)\n",
        "X_train_lstm_res, y_train_lstm_res = smote_lstm.fit_resample(X_train_lstm, y_train_lstm)\n",
        "\n",
        "print(f\"Forma dos dados de treino para LSTM APÓS Oversampling (SMOTE): {X_train_lstm_res.shape}, {y_train_lstm_res.shape}\")\n",
        "print(\"Contagem de classes no treino LSTM APÓS Oversampling:\", np.bincount(y_train_lstm_res))\n",
        "\n",
        "\n",
        "# --- construção e treino do Modelo LSTM  ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Construindo e Treinando o Modelo LSTM com Oversampling...\")\n",
        "\n",
        "# reutiliza parâmetros de embedding e sequência\n",
        "embedding_dim = 128\n",
        "max_sequence_length = 100\n",
        "max_words = 10000\n",
        "\n",
        "model_lstm_oversample = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    LSTM(128),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_lstm_oversample.compile(optimizer='adam',\n",
        "                              loss='binary_crossentropy',\n",
        "                              metrics=['accuracy'])\n",
        "\n",
        "epochs_lstm_os = 15\n",
        "batch_size_lstm_os = 32\n",
        "\n",
        "print(\"\\nTreinando LSTM com dados oversampleados...\")\n",
        "\n",
        "history_lstm_oversample = model_lstm_oversample.fit(X_train_lstm_res, y_train_lstm_res,\n",
        "                                                    epochs=epochs_lstm_os,\n",
        "                                                    batch_size=batch_size_lstm_os,\n",
        "                                                    validation_data=(X_val_lstm, y_val_lstm),\n",
        "                                                    verbose=1)\n",
        "\n",
        "print(\"\\nResumo do Modelo LSTM (Oversample):\")\n",
        "model_lstm_oversample.summary()\n",
        "\n",
        "\n",
        "# --- avaliação do modelo ---\n",
        "print(\"\\nAvaliando LSTM (Oversample) no conjunto de teste FINAL...\")\n",
        "\n",
        "loss_lstm_os_test, accuracy_lstm_os_test = model_lstm_oversample.evaluate(X_test_lstm_final, y_test_lstm_final, verbose=0)\n",
        "print(f\"\\nAcurácia da LSTM (Oversample) no conjunto de teste FINAL: {accuracy_lstm_os_test:.4f}\")\n",
        "\n",
        "# gera previsões e probabilidades\n",
        "lstm_os_predictions_proba_test = model_lstm_oversample.predict(X_test_lstm_final)\n",
        "lstm_os_predictions_test = (lstm_os_predictions_proba_test >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da LSTM (Oversample) no conjunto de teste FINAL:\")\n",
        "print(classification_report(y_test_lstm_final, lstm_os_predictions_test))\n",
        "\n",
        "# cálculo de F1-score ponderado e AUC\n",
        "lstm_f1_weighted_os_test = f1_score(y_test_lstm_final, lstm_os_predictions_test, average='weighted')\n",
        "\n",
        "try:\n",
        "    lstm_auc_os_test = roc_auc_score(y_test_lstm_final, lstm_os_predictions_proba_test)\n",
        "except ValueError as e:\n",
        "    lstm_auc_os_test = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "except Exception as e:\n",
        "    lstm_auc_os_test = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "\n",
        "\n",
        "print(\"F1-score ponderado (LSTM Oversample):\", lstm_f1_weighted_os_test)\n",
        "if isinstance(lstm_auc_os_test, (float, int)):\n",
        "    print(\"AUC:\", lstm_auc_os_test)\n",
        "else:\n",
        "    print(\"AUC:\", lstm_auc_os_test)\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "try:\n",
        "    lstm_report_os_test = classification_report(y_test_lstm_final, lstm_os_predictions_test, output_dict=True)\n",
        "    results['LSTM (Embedding+Seq, Oversample) Test'] = {\n",
        "        'accuracy': accuracy_lstm_os_test,\n",
        "        'precision (macro)': lstm_report_os_test['macro avg']['precision'],\n",
        "        'recall (macro)': lstm_report_os_test['macro avg']['recall'],\n",
        "        'f1-score (macro)': lstm_report_os_test['macro avg']['f1-score'],\n",
        "        'f1-score (weighted)': lstm_f1_weighted_os_test,\n",
        "        'auc': lstm_auc_os_test if isinstance(lstm_auc_os_test, (float, int)) else None\n",
        "    }\n",
        "except NameError:\n",
        "    print(\"Dicionário 'results' não encontrado. Os resultados da LSTM (com oversampling) não foram armazenados.\")"
      ],
      "metadata": {
        "id": "dc8xPV1ewQD8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc51430c-76db-41cd-8442-e25d039e0626"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Forma dos dados de treino para LSTM (Antes do Oversampling): (15000, 100) (15000,)\n",
            "Forma dos dados de validação para LSTM: (5000, 100) (5000,)\n",
            "Forma dos dados de teste FINAL para LSTM: (5000, 100) (5000,)\n",
            "\n",
            "Aplicando SMOTE oversampling ao conjunto de treino para LSTM...\n",
            "Forma dos dados de treino para LSTM APÓS Oversampling (SMOTE): (24602, 100), (24602,)\n",
            "Contagem de classes no treino LSTM APÓS Oversampling: [12301 12301]\n",
            "\n",
            "==================================================\n",
            "Construindo e Treinando o Modelo LSTM com Oversampling...\n",
            "\n",
            "Treinando LSTM com dados oversampleados...\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 177ms/step - accuracy: 0.5072 - loss: 0.6940 - val_accuracy: 0.8202 - val_loss: 0.6733\n",
            "Epoch 2/15\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 176ms/step - accuracy: 0.5100 - loss: 0.6876 - val_accuracy: 0.5820 - val_loss: 0.5743\n",
            "Epoch 3/15\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 176ms/step - accuracy: 0.6648 - loss: 0.6349 - val_accuracy: 0.5858 - val_loss: 0.6199\n",
            "Epoch 4/15\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 177ms/step - accuracy: 0.7270 - loss: 0.5610 - val_accuracy: 0.6750 - val_loss: 0.5581\n",
            "Epoch 5/15\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 176ms/step - accuracy: 0.8216 - loss: 0.4131 - val_accuracy: 0.6506 - val_loss: 0.6942\n",
            "Epoch 6/15\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 173ms/step - accuracy: 0.8930 - loss: 0.2732 - val_accuracy: 0.6540 - val_loss: 0.7504\n",
            "Epoch 7/15\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 174ms/step - accuracy: 0.9276 - loss: 0.1942 - val_accuracy: 0.7052 - val_loss: 0.8444\n",
            "Epoch 8/15\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 177ms/step - accuracy: 0.9522 - loss: 0.1307 - val_accuracy: 0.6858 - val_loss: 1.0598\n",
            "Epoch 9/15\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 177ms/step - accuracy: 0.9632 - loss: 0.0951 - val_accuracy: 0.6842 - val_loss: 1.6268\n",
            "Epoch 10/15\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 171ms/step - accuracy: 0.9757 - loss: 0.0651 - val_accuracy: 0.7144 - val_loss: 1.3337\n",
            "Epoch 11/15\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 176ms/step - accuracy: 0.9787 - loss: 0.0577 - val_accuracy: 0.6722 - val_loss: 1.9780\n",
            "Epoch 12/15\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 176ms/step - accuracy: 0.9844 - loss: 0.0401 - val_accuracy: 0.7006 - val_loss: 1.5477\n",
            "Epoch 13/15\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 172ms/step - accuracy: 0.9868 - loss: 0.0373 - val_accuracy: 0.7140 - val_loss: 1.8055\n",
            "Epoch 14/15\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 175ms/step - accuracy: 0.9904 - loss: 0.0230 - val_accuracy: 0.7092 - val_loss: 1.9846\n",
            "Epoch 15/15\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 173ms/step - accuracy: 0.9900 - loss: 0.0287 - val_accuracy: 0.6890 - val_loss: 2.3823\n",
            "\n",
            "Resumo do Modelo LSTM (Oversample):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,259,717\u001b[0m (16.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,259,717</span> (16.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,419,905\u001b[0m (5.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,419,905</span> (5.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,839,812\u001b[0m (10.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,839,812</span> (10.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando LSTM (Oversample) no conjunto de teste FINAL...\n",
            "\n",
            "Acurácia da LSTM (Oversample) no conjunto de teste FINAL: 0.6900\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step\n",
            "\n",
            "Avaliação completa da LSTM (Oversample) no conjunto de teste FINAL:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.71      0.79      4101\n",
            "           1       0.31      0.61      0.41       899\n",
            "\n",
            "    accuracy                           0.69      5000\n",
            "   macro avg       0.60      0.66      0.60      5000\n",
            "weighted avg       0.79      0.69      0.72      5000\n",
            "\n",
            "F1-score ponderado (LSTM Oversample): 0.7217690458620953\n",
            "AUC: 0.7149666960417425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Undersampling**"
      ],
      "metadata": {
        "id": "EwRnENMusP7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como terceira estratégia de balanceamento, foi empregado o método de undersampling aleatório, utilizando a técnica **RandomUnderSampler** da biblioteca `imbalanced-learn`. Diferente do SMOTE, que gera novas amostras para as classes minoritárias, o undersampling reduz o número de instâncias da classe majoritária, removendo exemplos aleatórios até que todas as classes tenham o mesmo número de ocorrências.\n",
        "\n",
        "No experimento, o balanceamento foi aplicado apenas sobre o conjunto de treino, **igualando as classes 0 e 1 com 3.598 exemplos cada** — valor correspondente à classe originalmente menos representada. Essa abordagem visa mitigar o viés dos modelos em favor da classe dominante, porém com o custo potencial de perda de informação relevante.\n",
        "Apesar disso, ela permite a avaliação do impacto da redução de dados no desempenho de algoritmos como **Regressão Logística, Naive Bayes, SVM, Random Forest, LightGBM e redes neurais densas**, todos treinados com a base reamostrada, possibilitando uma comparação direta com os resultados obtidos nas estratégias anteriores de balanceamento."
      ],
      "metadata": {
        "id": "YlnorPx74pKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\nDistribuição das classes no treino TF-IDF (antes do undersampling):\")\n",
        "print(pd.Series(y_train).value_counts())\n",
        "\n",
        "# inicializa RandomUnderSampler\n",
        "# random_state para reprodutibilidade\n",
        "# sampling_strategy='auto' subamostra todas as classes majoritárias para igualar a classe minoritária\n",
        "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"\\nForma dos dados de treino TF-IDF após Undersampling:\", X_train_rus.shape, y_train_rus.shape)\n",
        "print(\"\\nDistribuição das classes no treino TF-IDF (depois do Undersampling):\")\n",
        "print(pd.Series(y_train_rus).value_counts())"
      ],
      "metadata": {
        "id": "88qg0xMBKb62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0865b5d1-0438-47bb-9424-41fdeca8d39b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "\n",
            "Distribuição das classes no treino TF-IDF (antes do undersampling):\n",
            "Label\n",
            "0    16402\n",
            "1     3598\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Forma dos dados de treino TF-IDF após Undersampling: (7196, 5000) (7196,)\n",
            "\n",
            "Distribuição das classes no treino TF-IDF (depois do Undersampling):\n",
            "Label\n",
            "0    3598\n",
            "1    3598\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Treinando modelos tradicionais COM UNDERSAMPLING...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 1. Regressão Logística (Undersampling)\n",
        "print(\"Treinando Regressão Logística com Undersampling-balanced data...\")\n",
        "lr_model_rus = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model_rus.fit(X_train_rus, y_train_rus)\n",
        "lr_predictions_rus = lr_model_rus.predict(X_test)\n",
        "lr_predictions_proba_rus = lr_model_rus.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação da Regressão Logística (Undersampling):\")\n",
        "print(classification_report(y_test, lr_predictions_rus))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lr_predictions_rus))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, lr_predictions_rus, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, lr_predictions_proba_rus))\n",
        "\n",
        "lr_report_rus = classification_report(y_test, lr_predictions_rus, output_dict=True)\n",
        "results['Logistic Regression (Undersample)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lr_predictions_rus),\n",
        "    'precision (macro)': lr_report_rus['macro avg']['precision'],\n",
        "    'recall (macro)': lr_report_rus['macro avg']['recall'],\n",
        "    'f1-score (macro)': lr_report_rus['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lr_predictions_rus, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, lr_predictions_proba_rus)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 2. Naive Bayes Multinomial (Undersampling)\n",
        "print(\"Treinando Naive Bayes Multinomial com Undersampling-balanced data...\")\n",
        "nb_model_rus = MultinomialNB()\n",
        "nb_model_rus.fit(X_train_rus, y_train_rus)\n",
        "nb_predictions_rus = nb_model_rus.predict(X_test)\n",
        "nb_predictions_proba_rus = nb_model_rus.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do Naive Bayes Multinomial (Undersampling):\")\n",
        "print(classification_report(y_test, nb_predictions_rus))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, nb_predictions_rus))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, nb_predictions_rus, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, nb_predictions_proba_rus))\n",
        "\n",
        "nb_report_rus = classification_report(y_test, nb_predictions_rus, output_dict=True)\n",
        "results['Multinomial NB (Undersample)'] = {\n",
        "    'accuracy': accuracy_score(y_test, nb_predictions_rus),\n",
        "    'precision (macro)': nb_report_rus['macro avg']['precision'],\n",
        "    'recall (macro)': nb_report_rus['macro avg']['recall'],\n",
        "    'f1-score (macro)': nb_report_rus['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, nb_predictions_rus, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, nb_predictions_proba_rus)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 3. Support Vector Machine (SVM) (Undersampling)\n",
        "print(\"Treinando SVM (Kernel Linear) com Undersampling-balanced data...\")\n",
        "svm_model_rus = SVC(kernel='linear', probability=True, random_state=42)\n",
        "svm_model_rus.fit(X_train_rus, y_train_rus)\n",
        "svm_predictions_rus = svm_model_rus.predict(X_test)\n",
        "svm_predictions_proba_rus = svm_model_rus.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do SVM (Undersampling):\")\n",
        "print(classification_report(y_test, svm_predictions_rus))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, svm_predictions_rus))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, svm_predictions_rus, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, svm_predictions_proba_rus))\n",
        "\n",
        "svm_report_rus = classification_report(y_test, svm_predictions_rus, output_dict=True)\n",
        "results['SVM (Undersample)'] = {\n",
        "    'accuracy': accuracy_score(y_test, svm_predictions_rus),\n",
        "    'precision (macro)': svm_report_rus['macro avg']['precision'],\n",
        "    'recall (macro)': svm_report_rus['macro avg']['recall'],\n",
        "    'f1-score (macro)': svm_report_rus['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, svm_predictions_rus, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, svm_predictions_proba_rus)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 4. Random Forest Classifier (Undersampling)\n",
        "print(\"Treinando Random Forest com Undersampling-balanced data...\")\n",
        "rf_model_rus = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model_rus.fit(X_train_rus, y_train_rus)\n",
        "rf_predictions_rus = rf_model_rus.predict(X_test)\n",
        "rf_predictions_proba_rus = rf_model_rus.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do Random Forest (Undersampling):\")\n",
        "print(classification_report(y_test, rf_predictions_rus))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, rf_predictions_rus))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, rf_predictions_rus, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, rf_predictions_proba_rus))\n",
        "\n",
        "rf_report_rus = classification_report(y_test, rf_predictions_rus, output_dict=True)\n",
        "results['Random Forest (Undersample)'] = {\n",
        "    'accuracy': accuracy_score(y_test, rf_predictions_rus),\n",
        "    'precision (macro)': rf_report_rus['macro avg']['precision'],\n",
        "    'recall (macro)': rf_report_rus['macro avg']['recall'],\n",
        "    'f1-score (macro)': rf_report_rus['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, rf_predictions_rus, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, rf_predictions_proba_rus)\n",
        "}\n",
        "\n",
        "print(\"\\nResultados acumulados (incluindo SMOTE e Undersampling) até agora:\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "6vsp4FHwKf6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b27f74a6-ad03-441b-8165-25281c83f0d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Treinando modelos tradicionais COM UNDERSAMPLING...\n",
            "--------------------------------------------------\n",
            "Treinando Regressão Logística com Undersampling-balanced data...\n",
            "Avaliação da Regressão Logística (Undersampling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.79      0.85      4101\n",
            "           1       0.44      0.75      0.55       899\n",
            "\n",
            "    accuracy                           0.78      5000\n",
            "   macro avg       0.69      0.77      0.70      5000\n",
            "weighted avg       0.85      0.78      0.80      5000\n",
            "\n",
            "Acurácia:  0.7802\n",
            "Weighted F1-score: 0.8000412990893742\n",
            "AUC: 0.8511894735785707\n",
            "--------------------------------------------------\n",
            "Treinando Naive Bayes Multinomial com Undersampling-balanced data...\n",
            "Avaliação do Naive Bayes Multinomial (Undersampling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.72      0.82      4101\n",
            "           1       0.39      0.82      0.53       899\n",
            "\n",
            "    accuracy                           0.74      5000\n",
            "   macro avg       0.67      0.77      0.67      5000\n",
            "weighted avg       0.85      0.74      0.76      5000\n",
            "\n",
            "Acurácia:  0.7356\n",
            "Weighted F1-score: 0.7644347846550633\n",
            "AUC: 0.8519427015142405\n",
            "--------------------------------------------------\n",
            "Treinando SVM (Kernel Linear) com Undersampling-balanced data...\n",
            "Avaliação do SVM (Undersampling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.78      0.85      4101\n",
            "           1       0.43      0.76      0.55       899\n",
            "\n",
            "    accuracy                           0.77      5000\n",
            "   macro avg       0.68      0.77      0.70      5000\n",
            "weighted avg       0.84      0.77      0.80      5000\n",
            "\n",
            "Acurácia:  0.7742\n",
            "Weighted F1-score: 0.7951240849221489\n",
            "AUC: 0.8384193171366272\n",
            "--------------------------------------------------\n",
            "Treinando Random Forest com Undersampling-balanced data...\n",
            "Avaliação do Random Forest (Undersampling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.74      0.83      4101\n",
            "           1       0.39      0.74      0.51       899\n",
            "\n",
            "    accuracy                           0.74      5000\n",
            "   macro avg       0.66      0.74      0.67      5000\n",
            "weighted avg       0.83      0.74      0.77      5000\n",
            "\n",
            "Acurácia:  0.7434\n",
            "Weighted F1-score: 0.7691742558286223\n",
            "AUC: 0.8281459065167371\n",
            "\n",
            "Resultados acumulados (incluindo SMOTE e Undersampling) até agora:\n",
            "{'Logistic Regression (Undersample)': {'accuracy': 0.7802, 'precision (macro)': 0.6857815387738894, 'recall (macro)': 0.7700385890307554, 'f1-score (macro)': 0.7033413908942813, 'f1-score (weighted)': 0.8000412990893742, 'auc': np.float64(0.8511894735785707)}, 'Multinomial NB (Undersample)': {'accuracy': 0.7356, 'precision (macro)': 0.6677442735768168, 'recall (macro)': 0.7676024649024804, 'f1-score (macro)': 0.6715484864000203, 'f1-score (weighted)': 0.7644347846550633, 'auc': np.float64(0.8519427015142405)}, 'SVM (Undersample)': {'accuracy': 0.7742, 'precision (macro)': 0.681551776069643, 'recall (macro)': 0.7668151965973735, 'f1-score (macro)': 0.6978832722650796, 'f1-score (weighted)': 0.7951240849221489, 'auc': np.float64(0.8384193171366272)}, 'Random Forest (Undersample)': {'accuracy': 0.7434, 'precision (macro)': 0.6577975562075118, 'recall (macro)': 0.7406570035415546, 'f1-score (macro)': 0.6671604521657686, 'f1-score (weighted)': 0.7691742558286223, 'auc': np.float64(0.8281459065167371)}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Modelos Avançados com TF-IDF (Undersampling) ---\n",
        "\n",
        "# 5. LightGBM (Gradient Boosting) (Undersampling)\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Treinando LightGBM com Undersampling-balanced data...\")\n",
        "\n",
        "# O número de classes será 2\n",
        "num_classes_lgbm_rus = len(np.unique(y_train_rus))\n",
        "print(f\"Número de classes em y_train_rus para LightGBM: {num_classes_lgbm_rus}\")\n",
        "\n",
        "lgb_model_rus = lgb.LGBMClassifier(objective='binary', random_state=42)\n",
        "lgb_model_rus.fit(X_train_rus, y_train_rus)\n",
        "lgb_predictions_rus = lgb_model_rus.predict(X_test)\n",
        "lgb_predictions_proba_rus = lgb_model_rus.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do LightGBM (Undersampling):\")\n",
        "print(classification_report(y_test, lgb_predictions_rus))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lgb_predictions_rus))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, lgb_predictions_rus, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, lgb_predictions_proba_rus))\n",
        "\n",
        "lgb_report_rus = classification_report(y_test, lgb_predictions_rus, output_dict=True)\n",
        "results['LightGBM (Undersample)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lgb_predictions_rus),\n",
        "    'precision (macro)': lgb_report_rus['macro avg']['precision'],\n",
        "    'recall (macro)': lgb_report_rus['macro avg']['recall'],\n",
        "    'f1-score (macro)': lgb_report_rus['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lgb_predictions_rus, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, lgb_predictions_proba_rus)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 6. Rede Neural Densa (MLP) com TensorFlow/Keras (Undersampling)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "print(\"Treinando Rede Neural Densa (MLP) com Undersampling-balanced data...\")\n",
        "\n",
        "# O número de classes será 2\n",
        "num_classes_mlp_rus = len(np.unique(y_train_rus))\n",
        "print(f\"Número de classes em y_train_rus para MLP: {num_classes_mlp_rus}\")\n",
        "\n",
        "mlp_model_rus = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train_rus.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "mlp_model_rus.compile(optimizer='adam',\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "epochs_mlp_rus = 20\n",
        "batch_size_mlp_rus = 64\n",
        "\n",
        "print(\"\\nTreinando MLP (Undersampling)...\")\n",
        "\n",
        "history_mlp_rus = mlp_model_rus.fit(X_train_rus, y_train_rus,\n",
        "                                  epochs=epochs_mlp_rus,\n",
        "                                  batch_size=batch_size_mlp_rus,\n",
        "                                  validation_data=(X_test, y_test),\n",
        "                                  verbose=1)\n",
        "\n",
        "print(\"\\nResumo do Modelo MLP (Undersampling):\")\n",
        "mlp_model_rus.summary()\n",
        "\n",
        "print(\"\\nAvaliando MLP (Undersampling) no conjunto de teste...\")\n",
        "loss_mlp_rus, accuracy_mlp_rus = mlp_model_rus.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da MLP (Undersampling) no conjunto de teste: {accuracy_mlp_rus:.4f}\")\n",
        "\n",
        "mlp_predictions_proba_positive_rus = mlp_model_rus.predict(X_test)\n",
        "mlp_predictions_rus = (mlp_predictions_proba_positive_rus >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da MLP (Undersampling):\")\n",
        "print(classification_report(y_test, mlp_predictions_rus))\n",
        "\n",
        "mlp_weighted_f1_rus = f1_score(y_test, mlp_predictions_rus, average='weighted')\n",
        "print(\"Weighted F1-score:\", mlp_weighted_f1_rus)\n",
        "\n",
        "try:\n",
        "    mlp_auc_score_rus = roc_auc_score(y_test, mlp_predictions_proba_positive_rus)\n",
        "    print(\"AUC:\", mlp_auc_score_rus)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    mlp_auc_score_rus = None\n",
        "\n",
        "mlp_report_rus = classification_report(y_test, mlp_predictions_rus, output_dict=True)\n",
        "results['MLP (Undersample)'] = {\n",
        "    'accuracy': accuracy_mlp_rus,\n",
        "    'precision (macro)': mlp_report_rus['macro avg']['precision'],\n",
        "    'recall (macro)': mlp_report_rus['macro avg']['recall'],\n",
        "    'f1-score (macro)': mlp_report_rus['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': mlp_weighted_f1_rus,\n",
        "    'auc': mlp_auc_score_rus\n",
        "}\n",
        "\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "LDWGIjC9Kj0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8fd5277d-75a2-4f6e-a3de-a7428661b6c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Treinando LightGBM com Undersampling-balanced data...\n",
            "Número de classes em y_train_rus para LightGBM: 2\n",
            "[LightGBM] [Info] Number of positive: 3598, number of negative: 3598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094371 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 24457\n",
            "[LightGBM] [Info] Number of data points in the train set: 7196, number of used features: 1055\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avaliação do LightGBM (Undersampling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.78      0.84      4101\n",
            "           1       0.41      0.70      0.52       899\n",
            "\n",
            "    accuracy                           0.77      5000\n",
            "   macro avg       0.67      0.74      0.68      5000\n",
            "weighted avg       0.83      0.77      0.79      5000\n",
            "\n",
            "Acurácia:  0.7652\n",
            "Weighted F1-score: 0.7860827080873041\n",
            "AUC: 0.8301776147818203\n",
            "--------------------------------------------------\n",
            "Treinando Rede Neural Densa (MLP) com Undersampling-balanced data...\n",
            "Número de classes em y_train_rus para MLP: 2\n",
            "\n",
            "Treinando MLP (Undersampling)...\n",
            "Epoch 1/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.6347 - loss: 0.6496 - val_accuracy: 0.7910 - val_loss: 0.4298\n",
            "Epoch 2/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.8428 - loss: 0.3778 - val_accuracy: 0.7312 - val_loss: 0.5448\n",
            "Epoch 3/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.8867 - loss: 0.2803 - val_accuracy: 0.7476 - val_loss: 0.5786\n",
            "Epoch 4/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9335 - loss: 0.1909 - val_accuracy: 0.7638 - val_loss: 0.6027\n",
            "Epoch 5/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - accuracy: 0.9601 - loss: 0.1224 - val_accuracy: 0.7406 - val_loss: 0.7443\n",
            "Epoch 6/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 0.9733 - loss: 0.0821 - val_accuracy: 0.7490 - val_loss: 0.8202\n",
            "Epoch 7/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.9786 - loss: 0.0599 - val_accuracy: 0.7406 - val_loss: 0.9446\n",
            "Epoch 8/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - accuracy: 0.9845 - loss: 0.0448 - val_accuracy: 0.7296 - val_loss: 1.0925\n",
            "Epoch 9/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.9891 - loss: 0.0318 - val_accuracy: 0.7312 - val_loss: 1.1329\n",
            "Epoch 10/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - accuracy: 0.9905 - loss: 0.0295 - val_accuracy: 0.7514 - val_loss: 1.1020\n",
            "Epoch 11/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9900 - loss: 0.0287 - val_accuracy: 0.7326 - val_loss: 1.2456\n",
            "Epoch 12/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9924 - loss: 0.0219 - val_accuracy: 0.7380 - val_loss: 1.2656\n",
            "Epoch 13/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9918 - loss: 0.0233 - val_accuracy: 0.7460 - val_loss: 1.2482\n",
            "Epoch 14/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9908 - loss: 0.0200 - val_accuracy: 0.7424 - val_loss: 1.3207\n",
            "Epoch 15/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9931 - loss: 0.0202 - val_accuracy: 0.7382 - val_loss: 1.4080\n",
            "Epoch 16/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9938 - loss: 0.0175 - val_accuracy: 0.7564 - val_loss: 1.3419\n",
            "Epoch 17/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.9942 - loss: 0.0135 - val_accuracy: 0.7458 - val_loss: 1.4204\n",
            "Epoch 18/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.9935 - loss: 0.0117 - val_accuracy: 0.7376 - val_loss: 1.5378\n",
            "Epoch 19/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9940 - loss: 0.0173 - val_accuracy: 0.7498 - val_loss: 1.4275\n",
            "Epoch 20/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - accuracy: 0.9940 - loss: 0.0156 - val_accuracy: 0.7456 - val_loss: 1.4699\n",
            "\n",
            "Resumo do Modelo MLP (Undersampling):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,280,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,939,845\u001b[0m (15.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,939,845</span> (15.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,281\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,281</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,626,564\u001b[0m (10.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,626,564</span> (10.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando MLP (Undersampling) no conjunto de teste...\n",
            "\n",
            "Acurácia da MLP (Undersampling) no conjunto de teste: 0.7456\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "\n",
            "Avaliação completa da MLP (Undersampling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.75      0.83      4101\n",
            "           1       0.39      0.73      0.51       899\n",
            "\n",
            "    accuracy                           0.75      5000\n",
            "   macro avg       0.66      0.74      0.67      5000\n",
            "weighted avg       0.83      0.75      0.77      5000\n",
            "\n",
            "Weighted F1-score: 0.7708405891656657\n",
            "AUC: 0.8147236939144228\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "\n",
        "# --- pré-processamento para CNN/LSTM ---\n",
        "\n",
        "max_words = 10000\n",
        "max_sequence_length = 100\n",
        "\n",
        "all_texts = df_subset['processed_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "all_labels = df_subset['Label']\n",
        "\n",
        "# --- tokenização ---\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(all_texts)\n",
        "\n",
        "# converte textos para sequências de inteiros\n",
        "all_sequences = tokenizer.texts_to_sequences(all_texts)\n",
        "\n",
        "# --- padding das Sequências ---\n",
        "all_padded_sequences = pad_sequences(all_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "print(\"\\nExemplo de sequências padded (df_subset completo):\")\n",
        "print(all_padded_sequences[:5])\n",
        "print(\"\\nForma das sequências padded (df_subset completo):\", all_padded_sequences.shape)\n",
        "\n",
        "# --- prepara labels para CNN/LSTM ---\n",
        "encoded_all_labels = all_labels.values\n",
        "print(\"\\nLabels do df_subset:\", np.unique(encoded_all_labels))\n",
        "\n",
        "# --- divide dados para CNN/LSTM ---\n",
        "\n",
        "X_train_val_cnn, X_test_cnn_final, y_train_val_cnn, y_test_cnn_final = train_test_split(\n",
        "    all_padded_sequences, encoded_all_labels, test_size=0.20, random_state=42, stratify=encoded_all_labels\n",
        ")\n",
        "\n",
        "X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(\n",
        "    X_train_val_cnn, y_train_val_cnn, test_size=0.25, random_state=42, stratify=y_train_val_cnn\n",
        ")\n",
        "\n",
        "print(\"\\nForma dos dados de treino para CNN (Antes do Undersampling):\", X_train_cnn.shape, y_train_cnn.shape)\n",
        "print(\"Forma dos dados de validação para CNN:\", X_val_cnn.shape, y_val_cnn.shape)\n",
        "print(\"Forma dos dados de teste FINAL para CNN:\", X_test_cnn_final.shape, y_test_cnn_final.shape)\n",
        "\n",
        "# --- aplicar undersampling  ---\n",
        "print(\"\\nAplicando RandomUnderSampler undersampling ao conjunto de treino...\")\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_train_cnn_us, y_train_cnn_us = rus.fit_resample(X_train_cnn, y_train_cnn)\n",
        "\n",
        "print(f\"Forma dos dados de treino para CNN APÓS Undersampling: {X_train_cnn_us.shape}, {y_train_cnn_us.shape}\")\n",
        "print(\"Contagem de classes no treino APÓS Undersampling:\", np.bincount(y_train_cnn_us))\n",
        "\n",
        "# --- constrói o Modelo CNN ---\n",
        "embedding_dim = 128\n",
        "filter_sizes = [3]\n",
        "num_filters = 128\n",
        "\n",
        "model_cnn_undersample = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    Conv1D(filters=num_filters, kernel_size=filter_sizes[0], activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_cnn_undersample.compile(optimizer='adam',\n",
        "                              loss='binary_crossentropy',\n",
        "                              metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nResumo do Modelo CNN (Undersample):\")\n",
        "model_cnn_undersample.summary()\n",
        "\n",
        "# --- treino do Modelo CNN (Com Undersampling) ---\n",
        "epochs_cnn_us = 10\n",
        "batch_size_cnn_us = 32\n",
        "\n",
        "print(\"\\nTreinando CNN com Undersampling...\")\n",
        "history_cnn_undersample = model_cnn_undersample.fit(X_train_cnn_us, y_train_cnn_us,\n",
        "                                                    epochs=epochs_cnn_us,\n",
        "                                                    batch_size=batch_size_cnn_us,\n",
        "                                                    validation_data=(X_val_cnn, y_val_cnn),\n",
        "                                                    verbose=1)\n",
        "\n",
        "# --- avaliação do modelo ---\n",
        "print(\"\\nAvaliando CNN (Undersample) no conjunto de teste FINAL...\")\n",
        "loss_cnn_test_us, accuracy_cnn_test_us = model_cnn_undersample.evaluate(X_test_cnn_final, y_test_cnn_final, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da CNN (Undersample) no conjunto de teste FINAL: {accuracy_cnn_test_us:.4f}\")\n",
        "\n",
        "# gera previsões de probabilidade e classes\n",
        "cnn_predictions_probs_us = model_cnn_undersample.predict(X_test_cnn_final)\n",
        "cnn_predictions_us = (cnn_predictions_probs_us >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da CNN (Undersample) no conjunto de teste FINAL:\")\n",
        "print(classification_report(y_test_cnn_final, cnn_predictions_us))\n",
        "\n",
        "# cálculo de F1-score ponderado e AUC para a CNN (Undersample)\n",
        "cnn_f1_weighted_us = f1_score(y_test_cnn_final, cnn_predictions_us, average='weighted')\n",
        "\n",
        "try:\n",
        "    cnn_auc_us = roc_auc_score(y_test_cnn_final, cnn_predictions_probs_us)\n",
        "except ValueError as e:\n",
        "    cnn_auc_us = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "except Exception as e:\n",
        "    cnn_auc_us = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "\n",
        "\n",
        "print(\"F1-score ponderado (CNN Undersample):\", cnn_f1_weighted_us)\n",
        "if isinstance(cnn_auc_us, (float, int)):\n",
        "    print(\"AUC:\", cnn_auc_us)\n",
        "else:\n",
        "    print(\"AUC:\", cnn_auc_us)\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "try:\n",
        "    cnn_report_us = classification_report(y_test_cnn_final, cnn_predictions_us, output_dict=True)\n",
        "    results['CNN (Embedding+Seq, Undersample) Test'] = {\n",
        "        'accuracy': accuracy_cnn_test_us,\n",
        "        'precision (macro)': cnn_report_us['macro avg']['precision'],\n",
        "        'recall (macro)': cnn_report_us['macro avg']['recall'],\n",
        "        'f1-score (macro)': cnn_report_us['macro avg']['f1-score'],\n",
        "        'f1-score (weighted)': cnn_f1_weighted_us,\n",
        "        'auc': cnn_auc_us if isinstance(cnn_auc_us, (float, int)) else None\n",
        "    }\n",
        "except NameError:\n",
        "    print(\"Dicionário 'results' não encontrado. Os resultados da CNN (com undersampling) não foram armazenados.\")"
      ],
      "metadata": {
        "id": "_-Lx__HvKrGN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b7d1968-ea81-4f32-85c6-2da0160ab114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Exemplo de sequências padded (df_subset completo):\n",
            "[[  57   84   90   47  156 1215   28 2589  146 3160 2199  101    1 2413\n",
            "  2589  101 2200   14  113  681  680 4581    1    6 2589  101 1262   48\n",
            "   194  348 3522    6 1263 3803    6   19  277 8319  997  942 2525   82\n",
            "    35   23 1801   62  303  198  249    8  282   60 1361    8  583 8319\n",
            "   375  363 8319 2161  198  370 1392 1008  363  252   47    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    1 7440  224  285   19    2    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [ 307   64 3058 6811 2360 1291  185  250   91   64    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [1769    1  101 6812 3804  195 8320  114    1  447  356  728 4822   13\n",
            "    97 1009 3389    1 2201 2730   32 1960  329   52    1  147 1532  728\n",
            "   418  211 8321 5121 1248 3389   15 3389   19  329   89 1802    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [1629    3 1079  164 4823  662   19 2202   13   75    1  663  129    7\n",
            "  1414 1561    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "\n",
            "Forma das sequências padded (df_subset completo): (25000, 100)\n",
            "\n",
            "Labels do df_subset: [0 1]\n",
            "\n",
            "Forma dos dados de treino para CNN (Antes do Undersampling): (15000, 100) (15000,)\n",
            "Forma dos dados de validação para CNN: (5000, 100) (5000,)\n",
            "Forma dos dados de teste FINAL para CNN: (5000, 100) (5000,)\n",
            "\n",
            "Aplicando RandomUnderSampler undersampling ao conjunto de treino...\n",
            "Forma dos dados de treino para CNN APÓS Undersampling: (5398, 100), (5398,)\n",
            "Contagem de classes no treino APÓS Undersampling: [2699 2699]\n",
            "\n",
            "Resumo do Modelo CNN (Undersample):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_7 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_4          │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_4          │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando CNN com Undersampling...\n",
            "Epoch 1/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 56ms/step - accuracy: 0.5814 - loss: 0.6612 - val_accuracy: 0.7830 - val_loss: 0.4910\n",
            "Epoch 2/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 41ms/step - accuracy: 0.8628 - loss: 0.3523 - val_accuracy: 0.7824 - val_loss: 0.4444\n",
            "Epoch 3/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.9495 - loss: 0.1714 - val_accuracy: 0.7948 - val_loss: 0.5057\n",
            "Epoch 4/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 39ms/step - accuracy: 0.9760 - loss: 0.0824 - val_accuracy: 0.7470 - val_loss: 0.7423\n",
            "Epoch 5/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.9872 - loss: 0.0478 - val_accuracy: 0.7544 - val_loss: 0.8880\n",
            "Epoch 6/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.9945 - loss: 0.0204 - val_accuracy: 0.7430 - val_loss: 0.9910\n",
            "Epoch 7/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 39ms/step - accuracy: 0.9964 - loss: 0.0121 - val_accuracy: 0.7462 - val_loss: 1.0601\n",
            "Epoch 8/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.9959 - loss: 0.0099 - val_accuracy: 0.7246 - val_loss: 1.2945\n",
            "Epoch 9/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 49ms/step - accuracy: 0.9979 - loss: 0.0075 - val_accuracy: 0.7488 - val_loss: 1.1556\n",
            "Epoch 10/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.9986 - loss: 0.0071 - val_accuracy: 0.7480 - val_loss: 1.1933\n",
            "\n",
            "Avaliando CNN (Undersample) no conjunto de teste FINAL...\n",
            "\n",
            "Acurácia da CNN (Undersample) no conjunto de teste FINAL: 0.7430\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
            "\n",
            "Avaliação completa da CNN (Undersample) no conjunto de teste FINAL:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.74      0.83      4101\n",
            "           1       0.39      0.75      0.51       899\n",
            "\n",
            "    accuracy                           0.74      5000\n",
            "   macro avg       0.66      0.74      0.67      5000\n",
            "weighted avg       0.83      0.74      0.77      5000\n",
            "\n",
            "F1-score ponderado (CNN Undersample): 0.7691024387533998\n",
            "AUC: 0.8119789009381849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- construção e treino do Modelo LSTM ---\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Construindo e Treinando o Modelo LSTM com Undersampling...\")\n",
        "\n",
        "# reutiliza parâmetros de embedding e LSTM da célula anterior\n",
        "\n",
        "model_lstm_undersample = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    LSTM(128),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_lstm_undersample.compile(optimizer='adam',\n",
        "                               loss='binary_crossentropy',\n",
        "                               metrics=['accuracy'])\n",
        "\n",
        "epochs_lstm_rus = 15\n",
        "batch_size_lstm_rus = 32\n",
        "\n",
        "print(\"\\nTreinando LSTM com dados subamostrados...\")\n",
        "\n",
        "history_lstm_undersample = model_lstm_undersample.fit(X_train_cnn_us, y_train_cnn_us,\n",
        "                                                   epochs=epochs_lstm_rus,\n",
        "                                                   batch_size=batch_size_lstm_rus,\n",
        "                                                   validation_data=(X_val_cnn, y_val_cnn),\n",
        "                                                   verbose=1)\n",
        "\n",
        "print(\"\\nResumo do Modelo LSTM (Undersampling):\")\n",
        "model_lstm_undersample.summary()\n",
        "\n",
        "# --- avaliação do modelo ---\n",
        "print(\"\\nAvaliando LSTM (Undersampling) no conjunto de validação...\")\n",
        "loss_lstm_us_val, accuracy_lstm_us_val = model_lstm_undersample.evaluate(X_val_cnn, y_val_cnn, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da LSTM (Undersampling) no conjunto de validação: {accuracy_lstm_us_val:.4f}\")\n",
        "\n",
        "lstm_us_predictions_proba_val = model_lstm_undersample.predict(X_val_cnn)\n",
        "lstm_us_predictions_val = (lstm_us_predictions_proba_val >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da LSTM (Undersampling) no conjunto de validação:\")\n",
        "print(classification_report(y_val_cnn, lstm_us_predictions_val))\n",
        "\n",
        "lstm_us_weighted_f1_val = f1_score(y_val_cnn, lstm_us_predictions_val, average='weighted')\n",
        "print(\"Weighted F1-score:\", lstm_us_weighted_f1_val)\n",
        "\n",
        "try:\n",
        "    lstm_us_auc_score_val = roc_auc_score(y_val_cnn, lstm_us_predictions_proba_val)\n",
        "    print(\"AUC:\", lstm_us_auc_score_val)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    lstm_us_auc_score_val = None\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "# dicionário de resultados\n",
        "lstm_us_report_val = classification_report(y_val_cnn, lstm_us_predictions_val, output_dict=True)\n",
        "results['LSTM (Embedding+Seq) Undersample Validation'] = {\n",
        "    'accuracy': accuracy_lstm_us_val,\n",
        "    'precision (macro)': lstm_us_report_val['macro avg']['precision'],\n",
        "    'recall (macro)': lstm_us_report_val['macro avg']['recall'],\n",
        "    'f1-score (macro)': lstm_us_report_val['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': lstm_us_weighted_f1_val,\n",
        "    'auc': lstm_us_auc_score_val\n",
        "}\n",
        "\n",
        "print(\"\\nResultados Finais Acumulados:\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "OzAy8SmkKtoG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e11fc419-7565-43dc-ca2b-bdb33eaac0da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Construindo e Treinando o Modelo LSTM com Undersampling...\n",
            "\n",
            "Treinando LSTM com dados subamostrados...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 229ms/step - accuracy: 0.4909 - loss: 0.6956 - val_accuracy: 0.8202 - val_loss: 0.6830\n",
            "Epoch 2/15\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 226ms/step - accuracy: 0.5125 - loss: 0.6943 - val_accuracy: 0.8202 - val_loss: 0.6786\n",
            "Epoch 3/15\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 230ms/step - accuracy: 0.4981 - loss: 0.6940 - val_accuracy: 0.2158 - val_loss: 0.7042\n",
            "Epoch 4/15\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 215ms/step - accuracy: 0.5297 - loss: 0.6907 - val_accuracy: 0.2688 - val_loss: 0.6912\n",
            "Epoch 5/15\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 226ms/step - accuracy: 0.5682 - loss: 0.7065 - val_accuracy: 0.8206 - val_loss: 0.6748\n",
            "Epoch 6/15\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 229ms/step - accuracy: 0.5455 - loss: 0.6800 - val_accuracy: 0.6430 - val_loss: 0.6559\n",
            "Epoch 7/15\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 206ms/step - accuracy: 0.6334 - loss: 0.6614 - val_accuracy: 0.6274 - val_loss: 0.6220\n",
            "Epoch 8/15\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 208ms/step - accuracy: 0.6575 - loss: 0.6415 - val_accuracy: 0.3810 - val_loss: 0.6672\n",
            "Epoch 9/15\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 227ms/step - accuracy: 0.6542 - loss: 0.6161 - val_accuracy: 0.4914 - val_loss: 0.6123\n",
            "Epoch 10/15\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 219ms/step - accuracy: 0.6846 - loss: 0.5897 - val_accuracy: 0.4832 - val_loss: 0.5389\n",
            "Epoch 11/15\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 219ms/step - accuracy: 0.7071 - loss: 0.5878 - val_accuracy: 0.4482 - val_loss: 0.6112\n",
            "Epoch 12/15\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 227ms/step - accuracy: 0.6321 - loss: 0.6327 - val_accuracy: 0.2046 - val_loss: 0.6956\n",
            "Epoch 13/15\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 225ms/step - accuracy: 0.5414 - loss: 0.6805 - val_accuracy: 0.5660 - val_loss: 0.5841\n",
            "Epoch 14/15\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 288ms/step - accuracy: 0.6960 - loss: 0.5767 - val_accuracy: 0.5636 - val_loss: 0.5687\n",
            "Epoch 15/15\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 226ms/step - accuracy: 0.7596 - loss: 0.5238 - val_accuracy: 0.5650 - val_loss: 0.6107\n",
            "\n",
            "Resumo do Modelo LSTM (Undersampling):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_9 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,259,717\u001b[0m (16.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,259,717</span> (16.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,419,905\u001b[0m (5.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,419,905</span> (5.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,839,812\u001b[0m (10.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,839,812</span> (10.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando LSTM (Undersampling) no conjunto de validação...\n",
            "\n",
            "Acurácia da LSTM (Undersampling) no conjunto de validação: 0.5650\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 54ms/step\n",
            "\n",
            "Avaliação completa da LSTM (Undersampling) no conjunto de validação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.49      0.65      4101\n",
            "           1       0.28      0.92      0.43       899\n",
            "\n",
            "    accuracy                           0.56      5000\n",
            "   macro avg       0.62      0.71      0.54      5000\n",
            "weighted avg       0.84      0.56      0.61      5000\n",
            "\n",
            "Weighted F1-score: 0.6086216642306727\n",
            "AUC: 0.7746843535543978\n",
            "==================================================\n",
            "\n",
            "Resultados Finais Acumulados:\n",
            "{'Logistic Regression (Undersample)': {'accuracy': 0.7802, 'precision (macro)': 0.6857815387738894, 'recall (macro)': 0.7700385890307554, 'f1-score (macro)': 0.7033413908942813, 'f1-score (weighted)': 0.8000412990893742, 'auc': np.float64(0.8511894735785707)}, 'Multinomial NB (Undersample)': {'accuracy': 0.7356, 'precision (macro)': 0.6677442735768168, 'recall (macro)': 0.7676024649024804, 'f1-score (macro)': 0.6715484864000203, 'f1-score (weighted)': 0.7644347846550633, 'auc': np.float64(0.8519427015142405)}, 'SVM (Undersample)': {'accuracy': 0.7742, 'precision (macro)': 0.681551776069643, 'recall (macro)': 0.7668151965973735, 'f1-score (macro)': 0.6978832722650796, 'f1-score (weighted)': 0.7951240849221489, 'auc': np.float64(0.8384193171366272)}, 'Random Forest (Undersample)': {'accuracy': 0.7434, 'precision (macro)': 0.6577975562075118, 'recall (macro)': 0.7406570035415546, 'f1-score (macro)': 0.6671604521657686, 'f1-score (weighted)': 0.7691742558286223, 'auc': np.float64(0.8281459065167371)}, 'LightGBM (Undersample)': {'accuracy': 0.7652, 'precision (macro)': 0.6667554682934074, 'recall (macro)': 0.7409188838339167, 'f1-score (macro)': 0.6816039091880108, 'f1-score (weighted)': 0.7860827080873041, 'auc': np.float64(0.8301776147818203)}, 'MLP (Undersample)': {'accuracy': 0.7455999851226807, 'precision (macro)': 0.6582015737611762, 'recall (macro)': 0.7398268796318974, 'f1-score (macro)': 0.6682766722767548, 'f1-score (weighted)': 0.7708405891656657, 'auc': np.float64(0.8147236939144228)}, 'CNN (Embedding+Seq, Undersample) Test': {'accuracy': 0.7429999709129333, 'precision (macro)': 0.6593140167979715, 'recall (macro)': 0.7443214289685984, 'f1-score (macro)': 0.6682821626516073, 'f1-score (weighted)': 0.7691024387533998, 'auc': np.float64(0.8119789009381849)}, 'LSTM (Embedding+Seq) Undersample Validation': {'accuracy': 0.5649999976158142, 'precision (macro)': 0.6249338907746845, 'recall (macro)': 0.705291636457534, 'f1-score (macro)': 0.54011917827353, 'f1-score (weighted)': 0.6086216642306727, 'auc': np.float64(0.7746843535543978)}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIzkmiNXCC1L"
      },
      "source": [
        "### **Resultados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVK1y6MJCHoq"
      },
      "source": [
        "#### **Dataset Original**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Modelo                        | Acurácia | F1-score Ponderado | AUC (ponderado, OVR) |\n",
        "| ----------------------------- | -------- | ------------------ | -------------------- |\n",
        "| Regressão Logística           | 0.8506   | 0.8281             | 0.8609               |\n",
        "| Naive Bayes Multinomial       | 0.8446   | 0.8175             | 0.8494               |\n",
        "| SVM (Kernel Linear)           | 0.8522   | 0.8372             | 0.8339               |\n",
        "| Random Forest                 | 0.8440   | 0.8283             | 0.8387               |\n",
        "| LightGBM                      | 0.8456   | 0.8269             | 0.8414               |\n",
        "| MLP (Perceptron Multicamadas) | 0.8400   | 0.8348             | 0.8199               |\n",
        "| CNN (Convolutional NN)        | 0.8300   | 0.8206             | 0.8076               |\n",
        "| LSTM                          | 0.8200   | 0.8203             | 0.8144               |\n",
        "\n"
      ],
      "metadata": {
        "id": "RUXwatekfG13"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP75jgq3CM-h"
      },
      "source": [
        "#### **Dataset Balanceado**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Quadro Resumo dos Modelos com `class_weight`**"
      ],
      "metadata": {
        "id": "-0zOgBw1sqAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Modelo                            | Acurácia | F1-score Ponderado | AUC (ponderado, OVR) |\n",
        "| --------------------------------- | -------- | ------------------ | -------------------- |\n",
        "| Regressão Logística           | 0.8054   | 0.8195             | 0.8600               |\n",
        "| Naive Bayes Multinomial           | 0.8446   | 0.8175             | 0.8494 *(sem CW)*    |\n",
        "| SVM (Kernel Linear)           | 0.7954   | 0.8109             | 0.8462               |\n",
        "| Random Forest                | 0.8260   | 0.8184             | 0.8357               |\n",
        "| LightGBM                      | 0.7928   | 0.8076             | 0.8360               |\n",
        "| MLP (Perceptron Multicamadas) | 0.8200   | 0.8195             | 0.8155               |\n",
        "| CNN                           | 0.8100   | 0.8200             | 0.8236               |\n",
        "| LSTM                          | 0.8000   | 0.8151             | 0.8320               |\n",
        "\n"
      ],
      "metadata": {
        "id": "Mb-Vx95FmC9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Quadro Resumo dos Modelos com Oversampling**\n"
      ],
      "metadata": {
        "id": "VSk7ikv1stuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Modelo                          | Acurácia | F1-score Ponderado | AUC (ponderado, OVR) |\n",
        "| ------------------------------- | -------- | ------------------ | -------------------- |\n",
        "| Regressão Logística      | 0.7732   | 0.7921             | 0.8276               |\n",
        "| Naive Bayes Multinomial  | 0.7584   | 0.7828             | 0.8549               |\n",
        "| SVM (Kernel Linear)      | 0.7692   | 0.7886             | 0.8167               |\n",
        "| Random Forest            | 0.8108   | 0.8143             | 0.8174               |\n",
        "| LightGBM                 | 0.7732   | 0.7913             | 0.8298               |\n",
        "| MLP                     | 0.8400   | 0.8283             | 0.8256               |\n",
        "| CNN                 | 0.7400   | 0.7625             | 0.7328               |\n",
        "| LSTM                | 0.6900   | 0.7218             | 0.7150               |\n",
        "\n"
      ],
      "metadata": {
        "id": "E_gDs8Ywjb18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Quadro Resumo dos Modelos com Undersampling**"
      ],
      "metadata": {
        "id": "9EnkDYwCs0X0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Modelo                               | Acurácia | F1-score Ponderado | AUC (ponderado, OVR) |\n",
        "| ------------------------------------ | -------- | ------------------ | -------------------- |\n",
        "| Regressão Logística           | 0.7802   | 0.8000             | 0.8512               |\n",
        "| Naive Bayes Multinomial       | 0.7356   | 0.7644             | 0.8519               |\n",
        "| SVM (Kernel Linear)           | 0.7742   | 0.7951             | 0.8384               |\n",
        "| Random Forest                 | 0.7434   | 0.7692             | 0.8281               |\n",
        "| LightGBM                      | 0.7652   | 0.7861             | 0.8302               |\n",
        "| MLP  | 0.7500   | 0.7708             | 0.8147               |\n",
        "| CNN                    | 0.7400   | 0.7691             | 0.8120               |\n",
        "| LSTM                   | 0.5600   | 0.6086             | 0.7747               |\n"
      ],
      "metadata": {
        "id": "GbfU59iqkBs3"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1j3TuSWj9Z8_6Dm52-pxetg5a7MkrNcOL",
      "authorship_tag": "ABX9TyPSy1Xkwrxby7CMzKivlZ+X",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}