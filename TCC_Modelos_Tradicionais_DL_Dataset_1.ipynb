{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOFUhOO35b2Rx6lif1AQYdQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajuliasousa/TCC-2025-1/blob/main/TCC_Modelos_Tradicionais_DL_Dataset_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Panorama Geral do Trabalho**\n",
        "\n",
        "**Título:**\n",
        "\n",
        "Análise Comparativa de Modelos Clássicos e\n",
        "Deep Learning na Detecção de Discurso de Ódio\n",
        "\n",
        "**Objetivo Geral:**\n",
        "\n",
        "Avaliar e comparar o desempenho de diferentes algoritmos de classificação, incluindo modelos tradicionais de Machine Learning (ML) e arquiteturas de Deep Learning (DL), na tarefa de detecção de discurso de ódio em dados textuais, analisando o impacto de distintas abordagens de representação vetorial e técnicas de balanceamento de classes.\n",
        "\n",
        "**Objetivos Específicos:**\n",
        "\n",
        "1.\tRealizar o pré-processamento e a limpeza de um corpus textual rotulado para discurso de ódio.\n",
        "2.\tRepresentar os dados textuais por meio de TF-IDF e embeddings (pré-treinados ou ajustados).\n",
        "3.\tTreinar e avaliar modelos de ML com TF-IDF: Naive Bayes, Regressão Logística, SVM, Random Forest, LightGBM, MLP.\n",
        "4.\tPreparar dados com tokenização, vocabulário e padding para modelos DL com embeddings.\n",
        "5.\tTreinar redes profundas: CNN e LSTM com embeddings.\n",
        "6.\tAplicar diferentes estratégias de balanceamento de dados: oversampling, undersampling e uso de class_weight.\n",
        "7.\tAvaliar os modelos usando métricas: F1-score ponderado, AUC e acurácia.\n",
        "8.\tComparar sistematicamente o desempenho dos modelos em diferentes cenários e técnicas de representação.\n",
        "9.\tDiscutir os impactos do tipo de vetorização e do balanceamento nos resultados obtidos.\n"
      ],
      "metadata": {
        "id": "WN-GPL6RJ9D6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset 1: Hate Speech and Offensive Language Dataset**\n",
        "\n",
        "**Link:** https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset"
      ],
      "metadata": {
        "id": "SUUTFj-_KATf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este dataset reúne textos coletados do Twitter com o objetivo de auxiliar pesquisas em detecção automática de discurso de ódio e linguagem ofensiva. Ele foi desenvolvido por pesquisadores da Universidade de Cornell e da Universidade da Califórnia e publicado originalmente no artigo \"Automated Hate Speech Detection and the Problem of Offensive Language\" (Davidson et al., 2017).\n",
        "\n",
        "Cada tweet da base foi anotado manualmente e classificado em uma das seguintes categorias:\n",
        "\n",
        "    Hate Speech (Discurso de ódio): conteúdo que expressa ódio ou incita violência contra um grupo com base em atributos como raça, etnia, nacionalidade, gênero, orientação sexual, religião, entre outros.\n",
        "\n",
        "    Offensive Language (Linguagem ofensiva): conteúdo que pode conter palavrões, xingamentos ou linguagem agressiva, mas que não necessariamente se configura como discurso de ódio.\n",
        "\n",
        "    Neither (Nenhum dos dois): textos que não se enquadram em nenhuma das categorias acima, sendo neutros ou não ofensivos.\n",
        "\n",
        "O dataset é composto por 24.783 tweets, sendo uma ferramenta valiosa para tarefas de processamento de linguagem natural (PLN), como classificação de texto, análise de sentimentos e desenvolvimento de sistemas de moderação automatizada."
      ],
      "metadata": {
        "id": "sH4eIpBON2v3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bibliotecas e Visão do Dataset**"
      ],
      "metadata": {
        "id": "T6Uv2Rw1KNah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bibliotecas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/TCC/Datasets/Hate Speech and Offensive Language Dataset/labeled_data.csv'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4hyC2lUKVdU",
        "outputId": "bf12cce8-5498-4b10-a6a8-715c262af519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# informações básicas\n",
        "print(\"Primeiras linhas do dataset:\")\n",
        "print(df.head())\n",
        "print(\"\\nInformações do dataset:\")\n",
        "df.info()\n",
        "print(\"\\nDescrição estatística das colunas numéricas:\")\n",
        "print(df.describe())\n",
        "print(\"\\nDistribuição da coluna 'class':\")\n",
        "print(df['class'].value_counts(normalize=True) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knalaEcsKXso",
        "outputId": "9768c15d-47a6-42ee-e4f6-34bfba489384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeiras linhas do dataset:\n",
            "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
            "0           0      3            0                   0        3      2   \n",
            "1           1      3            0                   3        0      1   \n",
            "2           2      3            0                   3        0      1   \n",
            "3           3      3            0                   2        1      1   \n",
            "4           4      6            0                   6        0      1   \n",
            "\n",
            "                                               tweet  \n",
            "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
            "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
            "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
            "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
            "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
            "\n",
            "Informações do dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 24783 entries, 0 to 24782\n",
            "Data columns (total 7 columns):\n",
            " #   Column              Non-Null Count  Dtype \n",
            "---  ------              --------------  ----- \n",
            " 0   Unnamed: 0          24783 non-null  int64 \n",
            " 1   count               24783 non-null  int64 \n",
            " 2   hate_speech         24783 non-null  int64 \n",
            " 3   offensive_language  24783 non-null  int64 \n",
            " 4   neither             24783 non-null  int64 \n",
            " 5   class               24783 non-null  int64 \n",
            " 6   tweet               24783 non-null  object\n",
            "dtypes: int64(6), object(1)\n",
            "memory usage: 1.3+ MB\n",
            "\n",
            "Descrição estatística das colunas numéricas:\n",
            "         Unnamed: 0         count   hate_speech  offensive_language  \\\n",
            "count  24783.000000  24783.000000  24783.000000        24783.000000   \n",
            "mean   12681.192027      3.243473      0.280515            2.413711   \n",
            "std     7299.553863      0.883060      0.631851            1.399459   \n",
            "min        0.000000      3.000000      0.000000            0.000000   \n",
            "25%     6372.500000      3.000000      0.000000            2.000000   \n",
            "50%    12703.000000      3.000000      0.000000            3.000000   \n",
            "75%    18995.500000      3.000000      0.000000            3.000000   \n",
            "max    25296.000000      9.000000      7.000000            9.000000   \n",
            "\n",
            "            neither         class  \n",
            "count  24783.000000  24783.000000  \n",
            "mean       0.549247      1.110277  \n",
            "std        1.113299      0.462089  \n",
            "min        0.000000      0.000000  \n",
            "25%        0.000000      1.000000  \n",
            "50%        0.000000      1.000000  \n",
            "75%        0.000000      1.000000  \n",
            "max        9.000000      2.000000  \n",
            "\n",
            "Distribuição da coluna 'class':\n",
            "class\n",
            "1    77.432111\n",
            "2    16.797805\n",
            "0     5.770084\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A distribuição das classes mostra que a maior parte dos tweets foi rotulada como linguagem ofensiva (77,4%), seguida por neutros (16,8%), e apenas uma pequena parte corresponde a discurso de ódio (5,8%).\n",
        "\n",
        "Além da coluna de texto (tweet), o dataset traz colunas com contagens indicando o número de votos recebidos para cada categoria durante o processo de anotação manual. Isso permite avaliar o nível de concordância entre os anotadores.\n",
        "Estrutura dos dados:\n",
        "\n",
        "    7 colunas no total, sendo 6 numéricas e 1 de texto.\n",
        "\n",
        "    Todas as linhas estão completas (sem valores ausentes).\n",
        "\n",
        "    O tamanho total da base é de aproximadamente 1,3 MB.\n",
        "\n",
        "Algumas estatísticas:\n",
        "\n",
        "    A maioria dos tweets recebeu 3 votos durante a anotação.\n",
        "\n",
        "    A média de votos para \"linguagem ofensiva\" é significativamente maior do que para \"discurso de ódio\" e \"neutro\".\n",
        "\n",
        "    A classificação final (class) é derivada da categoria com mais votos em cada tweet.\n",
        "\n",
        "Esse panorama inicial ajuda a entender o viés de distribuição e reforça a importância de cuidados ao treinar modelos com essa base, já que o número reduzido de exemplos de discurso de ódio pode influenciar o desempenho do classificador."
      ],
      "metadata": {
        "id": "PolBr4WvOXXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pré-processamento Textual**"
      ],
      "metadata": {
        "id": "yLZryRJdNwoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Limpeza com clean_text()**\n",
        "\n",
        "Função que aplica regras para \"limpar\" os textos brutos:\n",
        "\n",
        "    Remove URLs: tira links da internet (ex: http://...).\n",
        "\n",
        "    Remove menções: elimina nomes de usuários (@usuario).\n",
        "\n",
        "    Remove hashtags: exclui palavras precedidas de # (ou poderia apenas remover o símbolo).\n",
        "\n",
        "    Remove caracteres não alfabéticos: exclui números, pontuações e símbolos, mantendo letras e acentuação.\n",
        "\n",
        "    Converte para minúsculas: uniformiza o texto.\n",
        "\n",
        "    Remove espaços extras: com strip().\n",
        "\n",
        "Resultado: uma versão mais \"limpa\" do tweet.\n",
        "\n",
        "**2. Tokenização e Processamento Avançado**\n",
        "\n",
        "Com a função preprocess_text_advanced():\n",
        "\n",
        "    Tokenização: divide o texto em palavras (tokens) com word_tokenize.\n",
        "\n",
        "    Remoção de stopwords: elimina palavras muito comuns em inglês (ex: the, and, is) que pouco contribuem para a análise.\n",
        "\n",
        "    Lematização: reduz as palavras à sua forma base (ex: running vira run, cars vira car), usando o WordNetLemmatizer."
      ],
      "metadata": {
        "id": "C_pYOIx3OnhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# regras de limpeza\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text) # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text) # Remove menções\n",
        "    text = re.sub(r'#\\w+', '', text) # Remove hashtags (ou pode mantê-las sem o #)\n",
        "    text = re.sub(r'[^a-zA-Z\\sÀ-ÿ]', '', text, re.I|re.A) # Remove caracteres não alfabéticos, mantém acentos\n",
        "    text = text.lower() # Converte para minúsculas\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Aplicar limpeza\n",
        "df['cleaned_tweet'] = df['tweet'].apply(clean_text)\n",
        "\n",
        "# Baixar recursos necessários para stop words e lematização\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Inicializar lematizador e stop words para inglês\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english')) # <--- Aqui a mudança para inglês\n",
        "\n",
        "def preprocess_text_advanced(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remover stop words e lematizar\n",
        "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return processed_tokens\n",
        "\n",
        "# Aplicar pré-processamento avançado\n",
        "df['processed_tokens'] = df['cleaned_tweet'].apply(preprocess_text_advanced)\n",
        "\n",
        "print(\"\\nExemplo de tokens processados:\")\n",
        "print(df[['tweet', 'processed_tokens']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PGZ5tU4KnOQ",
        "outputId": "68be308c-1400-4d59-ad73-021e61181f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Exemplo de tokens processados:\n",
            "                                               tweet  \\\n",
            "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
            "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
            "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
            "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
            "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
            "\n",
            "                                    processed_tokens  \n",
            "0  [rt, woman, shouldnt, complain, cleaning, hous...  \n",
            "1  [rt, boy, dat, coldtyga, dwn, bad, cuffin, dat...  \n",
            "2  [rt, dawg, rt, ever, fuck, bitch, start, cry, ...  \n",
            "3                           [rt, look, like, tranny]  \n",
            "4  [rt, shit, hear, might, true, might, faker, bi...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Vetorização com  TF-IDF**"
      ],
      "metadata": {
        "id": "sGL9oMu7bH88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O texto processado dos tweets foi transformado em uma matriz numérica utilizando a técnica TF-IDF (Term Frequency–Inverse Document Frequency), que atribui pesos às palavras com base na sua frequência e relevância nos textos.\n",
        "\n",
        "Para isso, os tokens foram reunidos novamente em strings e, em seguida, vetorizados com o TfidfVectorizer, limitando a 5.000 as palavras mais representativas. O resultado é uma matriz com 24.783 linhas (um para cada tweet) e 5.000 colunas (palavras-chave), que serve como base para treinar modelos de machine learning."
      ],
      "metadata": {
        "id": "iOovadTaQq_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 'processed_tokens' é a coluna com as listas de tokens\n",
        "# TfidfVectorizer espera strings como entrada, então junta-se os tokens de volta\n",
        "df['processed_text'] = df['processed_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "# inicializa o TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# aplica o vetorizador aos textos processados\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_text'])\n",
        "\n",
        "print(\"\\nForma da matriz TF-IDF:\")\n",
        "print(tfidf_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktc9o8kwLjHC",
        "outputId": "614bfd1b-71ba-4473-931a-9f3a22ef0319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forma da matriz TF-IDF:\n",
            "(24783, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Aplicação em dataset desbalanceado**"
      ],
      "metadata": {
        "id": "zSh1LyyuJzkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Divisão em treino/ teste**"
      ],
      "metadata": {
        "id": "H6G441n_bOqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os dados vetorizados foram divididos em dois conjuntos: 80% para treino e 20% para teste, garantindo reprodutibilidade com random_state=42. Essa divisão permite treinar o modelo em uma parte dos dados e avaliar seu desempenho em dados não vistos."
      ],
      "metadata": {
        "id": "ncdcvI88ROFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# define as features (X) e o target (y)\n",
        "X = tfidf_matrix\n",
        "y = df['class']\n",
        "\n",
        "# divide os dados em conjuntos de treino e teste\n",
        "# test_size=0.20 significa 20% dos dados para teste\n",
        "# random_state para reprodutibilidade\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "print(\"Forma dos dados de treino (X_train, y_train):\", X_train.shape, y_train.shape)\n",
        "print(\"Forma dos dados de teste (X_test, y_test):\", X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzsuXIjDMZpT",
        "outputId": "0b75156b-0549-4080-8a0d-699f0b9bc685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma dos dados de treino (X_train, y_train): (19826, 5000) (19826,)\n",
            "Forma dos dados de teste (X_test, y_test): (4957, 5000) (4957,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Modelos Tradicionais**"
      ],
      "metadata": {
        "id": "Y-oIDIWZbtPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quatro modelos de classificação supervisionada foram treinados para prever categorias de tweets (discurso de ódio, linguagem ofensiva ou nenhum dos dois), utilizando a matriz TF-IDF como entrada. Os modelos testados foram: **Regressão Logística, Naive Bayes Multinomial, Support Vector Machine (SVM com kernel linear) e Random Forest**. Cada modelo foi ajustado com os dados de treino e avaliado com os dados de teste usando métricas como **acurácia, F1-score (ponderado) e AUC (curva ROC, multiclasse, ponderada)**. Os resultados foram armazenados em um dicionário para facilitar a comparação de desempenho entre os classificadores.\n",
        "\n",
        "**Regressão Logística**\n",
        "\n",
        "A Regressão Logística foi utilizada como um modelo linear de base para classificação multiclasse. Ela estima a probabilidade de um tweet pertencer a cada uma das três classes com base nas palavras mais relevantes (extraídas via TF-IDF). O modelo foi treinado com um número maior de iterações (max_iter=1000) para garantir a convergência, dado o tamanho da matriz. Por oferecer suporte ao método predict_proba, foi possível calcular a métrica AUC ponderada (one-vs-rest), o que fornece uma medida robusta da capacidade do modelo em distinguir entre as classes.\n",
        "\n",
        "**Naive Bayes Multinomial**\n",
        "\n",
        "O modelo Naive Bayes Multinomial é especialmente adequado para tarefas de classificação de texto, por assumir que as características (neste caso, palavras) ocorrem de forma independente. Ele é simples, eficiente e frequentemente usado como forte baseline em PLN. No experimento, ele também permitiu a geração de probabilidades de classe (predict_proba), o que possibilitou o cálculo da AUC multiclasse. Apesar de suas suposições simplificadas, o Naive Bayes costuma ter desempenho competitivo quando os dados estão bem vetorizados.\n",
        "\n",
        "**Support Vector Machine (SVM)**\n",
        "\n",
        "O SVM foi utilizado com kernel linear, uma configuração comum e eficaz para dados textuais de alta dimensionalidade, como é o caso da matriz TF-IDF. Foi ativada a opção probability=True para possibilitar o cálculo da AUC, embora isso torne o treinamento mais custoso computacionalmente. O SVM busca encontrar hiperplanos que melhor separam as classes, sendo especialmente útil quando há margens claras entre categorias. Apesar de não ser naturalmente probabilístico, sua robustez o torna uma escolha frequente em tarefas de classificação com múltiplas classes.\n",
        "\n",
        "**Random Forest**\n",
        "\n",
        "O modelo Random Forest foi treinado com 100 árvores de decisão, combinando os resultados de várias árvores para aumentar a estabilidade e a precisão da predição. Como um modelo de ensemble, ele lida bem com dados complexos e é menos sensível a overfitting do que uma única árvore. Também oferece suporte a predict_proba, permitindo calcular a AUC ponderada. Sua capacidade de capturar interações não lineares entre os termos dos textos pode ser vantajosa em relação a modelos lineares, especialmente quando o texto contém padrões mais sutis."
      ],
      "metadata": {
        "id": "SY6ERpmPRd_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# Inicializar um dicionário para armazenar os resultados de todos os modelos\n",
        "results = {}\n",
        "\n",
        "# 1. Regressão Logística\n",
        "print(\"Treinando Regressão Logística...\")\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "\n",
        "# Obter probabilidades para AUC\n",
        "# O SVM por padrão não tem predict_proba, a menos que probability=True seja definido,\n",
        "# o que pode ser computacionalmente caro. Para SVM, AUC é menos comum ou requer adaptações.\n",
        "# Para outros modelos como Logistic Regression, Naive Bayes e Random Forest, predict_proba funciona.\n",
        "\n",
        "try:\n",
        "    lr_probabilities = lr_model.predict_proba(X_test)\n",
        "    # Calcule AUC para cada classe e depois a média (macro, weighted, etc.)\n",
        "    # Para classificação multiclasse, AUC é tipicamente calculado 'ovr' (one-vs-rest)\n",
        "    # Usaremos 'ovr' com average='weighted' para considerar o desbalanceamento\n",
        "    lr_auc = roc_auc_score(y_test, lr_probabilities, multi_class='ovr', average='weighted')\n",
        "except AttributeError:\n",
        "    lr_auc = \"N/A (predict_proba não disponível ou com problema)\"\n",
        "\n",
        "\n",
        "print(\"Avaliação da Regressão Logística:\")\n",
        "print(classification_report(y_test, lr_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lr_predictions))\n",
        "print(\"F1-score ponderado:\", f1_score(y_test, lr_predictions, average='weighted'))\n",
        "print(\"AUC (ponderado, one-vs-rest):\", lr_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "lr_report = classification_report(y_test, lr_predictions, output_dict=True)\n",
        "results['Logistic Regression'] = {\n",
        "    'accuracy': accuracy_score(y_test, lr_predictions),\n",
        "    'precision (macro)': lr_report['macro avg']['precision'],\n",
        "    'recall (macro)': lr_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lr_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lr_predictions, average='weighted'),\n",
        "    'auc (weighted_ovr)': lr_auc\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 2. Naive Bayes Multinomial\n",
        "print(\"Treinando Naive Bayes Multinomial...\")\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_predictions = nb_model.predict(X_test)\n",
        "\n",
        "try:\n",
        "    nb_probabilities = nb_model.predict_proba(X_test)\n",
        "    nb_auc = roc_auc_score(y_test, nb_probabilities, multi_class='ovr', average='weighted')\n",
        "except AttributeError:\n",
        "    nb_auc = \"N/A (predict_proba não disponível ou com problema)\"\n",
        "\n",
        "print(\"Avaliação do Naive Bayes Multinomial:\")\n",
        "print(classification_report(y_test, nb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, nb_predictions))\n",
        "print(\"F1-score ponderado:\", f1_score(y_test, nb_predictions, average='weighted'))\n",
        "print(\"AUC (ponderado, one-vs-rest):\", nb_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "nb_report = classification_report(y_test, nb_predictions, output_dict=True)\n",
        "results['Multinomial NB'] = {\n",
        "    'accuracy': accuracy_score(y_test, nb_predictions),\n",
        "    'precision (macro)': nb_report['macro avg']['precision'],\n",
        "    'recall (macro)': nb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': nb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, nb_predictions, average='weighted'),\n",
        "    'auc (weighted_ovr)': nb_auc\n",
        "}\n",
        "\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 3. Support Vector Machine (SVM)\n",
        "# Para AUC com SVM, adicionar probability=True\n",
        "svm_model = SVC(kernel='linear', probability=True)\n",
        "print(\"Treinando SVM (Kernel Linear)...\")\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_predictions = svm_model.predict(X_test)\n",
        "\n",
        "try:\n",
        "    # verificação\n",
        "    if hasattr(svm_model, 'predict_proba'):\n",
        "        svm_probabilities = svm_model.predict_proba(X_test)\n",
        "        svm_auc = roc_auc_score(y_test, svm_probabilities, multi_class='ovr', average='weighted')\n",
        "    else:\n",
        "         svm_auc = \"N/A (probability=False)\"\n",
        "except AttributeError:\n",
        "    svm_auc = \"N/A (problema ao obter predict_proba)\"\n",
        "\n",
        "\n",
        "print(\"Avaliação do SVM:\")\n",
        "print(classification_report(y_test, svm_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, svm_predictions))\n",
        "print(\"F1-score ponderado:\", f1_score(y_test, svm_predictions, average='weighted'))\n",
        "print(\"AUC (ponderado, one-vs-rest):\", svm_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "svm_report = classification_report(y_test, svm_predictions, output_dict=True)\n",
        "results['SVM'] = {\n",
        "    'accuracy': accuracy_score(y_test, svm_predictions),\n",
        "    'precision (macro)': svm_report['macro avg']['precision'],\n",
        "    'recall (macro)': svm_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': svm_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, svm_predictions, average='weighted'),\n",
        "    'auc (weighted_ovr)': svm_auc\n",
        "}\n",
        "\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 4. Random Forest Classifier\n",
        "print(\"Treinando Random Forest...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42) # seta o número de árvores\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "\n",
        "try:\n",
        "    rf_probabilities = rf_model.predict_proba(X_test)\n",
        "    rf_auc = roc_auc_score(y_test, rf_probabilities, multi_class='ovr', average='weighted')\n",
        "except AttributeError:\n",
        "    rf_auc = \"N/A (predict_proba não disponível ou com problema)\"\n",
        "\n",
        "\n",
        "print(\"Avaliação do Random Forest:\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, rf_predictions))\n",
        "print(\"F1-score ponderado:\", f1_score(y_test, rf_predictions, average='weighted'))\n",
        "print(\"AUC (ponderado, one-vs-rest):\", rf_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "rf_report = classification_report(y_test, rf_predictions, output_dict=True)\n",
        "results['Random Forest'] = {\n",
        "    'accuracy': accuracy_score(y_test, rf_predictions),\n",
        "    'precision (macro)': rf_report['macro avg']['precision'],\n",
        "    'recall (macro)': rf_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': rf_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, rf_predictions, average='weighted'),\n",
        "    'auc (weighted_ovr)': rf_auc\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAGE1eTyMTTM",
        "outputId": "cf1600a6-faa1-4fae-b21b-8b41f6a91224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando Regressão Logística...\n",
            "Avaliação da Regressão Logística:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.17      0.25       290\n",
            "           1       0.91      0.96      0.94      3832\n",
            "           2       0.83      0.83      0.83       835\n",
            "\n",
            "    accuracy                           0.89      4957\n",
            "   macro avg       0.76      0.65      0.67      4957\n",
            "weighted avg       0.88      0.89      0.88      4957\n",
            "\n",
            "Acurácia:  0.8922735525519467\n",
            "F1-score ponderado: 0.8784331676581796\n",
            "AUC (ponderado, one-vs-rest): 0.9386849954267578\n",
            "--------------------------------------------------\n",
            "Treinando Naive Bayes Multinomial...\n",
            "Avaliação do Naive Bayes Multinomial:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.00      0.01       290\n",
            "           1       0.83      0.99      0.90      3832\n",
            "           2       0.90      0.38      0.53       835\n",
            "\n",
            "    accuracy                           0.83      4957\n",
            "   macro avg       0.74      0.46      0.48      4957\n",
            "weighted avg       0.82      0.83      0.79      4957\n",
            "\n",
            "Acurácia:  0.8321565462981642\n",
            "F1-score ponderado: 0.7881920612643274\n",
            "AUC (ponderado, one-vs-rest): 0.8859837248469582\n",
            "--------------------------------------------------\n",
            "Treinando SVM (Kernel Linear)...\n",
            "Avaliação do SVM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.12      0.19       290\n",
            "           1       0.92      0.96      0.94      3832\n",
            "           2       0.82      0.89      0.86       835\n",
            "\n",
            "    accuracy                           0.90      4957\n",
            "   macro avg       0.75      0.66      0.66      4957\n",
            "weighted avg       0.88      0.90      0.88      4957\n",
            "\n",
            "Acurácia:  0.898325600161388\n",
            "F1-score ponderado: 0.882415777727608\n",
            "AUC (ponderado, one-vs-rest): 0.9361058675226303\n",
            "--------------------------------------------------\n",
            "Treinando Random Forest...\n",
            "Avaliação do Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.14      0.22       290\n",
            "           1       0.92      0.95      0.94      3832\n",
            "           2       0.80      0.86      0.83       835\n",
            "\n",
            "    accuracy                           0.89      4957\n",
            "   macro avg       0.74      0.65      0.66      4957\n",
            "weighted avg       0.87      0.89      0.88      4957\n",
            "\n",
            "Acurácia:  0.8910631430300585\n",
            "F1-score ponderado: 0.8768605385753226\n",
            "AUC (ponderado, one-vs-rest): 0.9391895523013912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Além dos modelos tradicionais, também foram implementados dois modelos avançados para comparação de desempenho: **LightGBM e uma Rede Neural densa (MLP).**\n",
        "\n",
        "O **LightGBM**, um algoritmo de **gradient boosting** eficiente e otimizado para velocidade, foi treinado diretamente sobre a matriz TF-IDF esparsa, com configuração para classificação multiclasse. Por suportar `predict_proba`, permitiu o cálculo da métrica AUC multiclasse ponderada, além de métricas tradicionais como acurácia e F1-score.\n",
        "\n",
        "Já a **Rede Neural MLP**, construída com o **Keras/TensorFlow**, possui camadas densas com funções de ativação ReLU, camadas de dropout para reduzir overfitting e uma saída com softmax para previsão de múltiplas classes. O modelo foi treinado por 20 épocas, com validação interna durante o processo, e sua performance foi avaliada também com base em métricas clássicas, incluindo a AUC gerada a partir das probabilidades preditas.\n",
        "\n",
        "Ambas as abordagens complementam os modelos anteriores, oferecendo perspectivas mais robustas sobre o comportamento dos dados e o desempenho de classificadores baseados em vetores TF-IDF."
      ],
      "metadata": {
        "id": "1MGqWzGgTfof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instalações\n",
        "!pip install tensorflow\n",
        "\n",
        "# bibliotecas para CNN\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oirSDRZibcG",
        "outputId": "bf0a6e61-b2c6-4593-ee2c-6dd71573d041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Modelos Avançados com TF-IDF ---\n",
        "\n",
        "# 5. LightGBM (Gradient Boosting)\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score # Importar métricas\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Treinando LightGBM...\")\n",
        "\n",
        "# converte matriz esparsa para o formato LightGBM\n",
        "# LightGBM funciona diretamente com matrizes esparsas do SciPy\n",
        "lgb_model = lgb.LGBMClassifier(objective='multiclass', num_class=len(df['class'].unique()), random_state=42)\n",
        "\n",
        "lgb_model.fit(X_train, y_train) # uso do X_train e y_train do split anterior (baseado em TF-IDF)\n",
        "\n",
        "lgb_predictions = lgb_model.predict(X_test)\n",
        "\n",
        "# obtém probabilidades para AUC\n",
        "try:\n",
        "    lgb_probabilities = lgb_model.predict_proba(X_test)\n",
        "    lgb_auc = roc_auc_score(y_test, lgb_probabilities, multi_class='ovr', average='weighted')\n",
        "except AttributeError:\n",
        "    lgb_auc = \"N/A (predict_proba não disponível ou com problema)\"\n",
        "\n",
        "\n",
        "print(\"Avaliação do LightGBM:\")\n",
        "print(classification_report(y_test, lgb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lgb_predictions))\n",
        "print(\"F1-score ponderado:\", f1_score(y_test, lgb_predictions, average='weighted'))\n",
        "print(\"AUC (ponderado, one-vs-rest):\", lgb_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "lgb_report = classification_report(y_test, lgb_predictions, output_dict=True)\n",
        "results['LightGBM'] = {\n",
        "    'accuracy': accuracy_score(y_test, lgb_predictions),\n",
        "    'precision (macro)': lgb_report['macro avg']['precision'],\n",
        "    'recall (macro)': lgb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lgb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lgb_predictions, average='weighted'),\n",
        "    'auc (weighted_ovr)': lgb_auc\n",
        "}\n",
        "\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 6. Rede Neural Densa (MLP) com TensorFlow/Keras\n",
        "\n",
        "# reutiliza TensorFlow importado anteriormente para a CNN\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "print(\"Treinando Rede Neural Densa (MLP)...\")\n",
        "\n",
        "# A matriz TF-IDF já está pronta (X_train, X_test, y_train, y_test)\n",
        "\n",
        "# constrói o Modelo MLP\n",
        "mlp_model = Sequential([\n",
        "    # flatten é necessário se a entrada não for 1D, mas a matriz TF-IDF já é 2D (samples x features)\n",
        "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)), # camada densa com neurônios\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    # camada de saída (igual à CNN para classificação multiclasse)\n",
        "    Dense(len(df['class'].unique()), activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "mlp_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy', # loss para caso de labels inteiras\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "mlp_model.summary()\n",
        "\n",
        "# treina o Modelo MLP\n",
        "epochs_mlp = 20\n",
        "batch_size_mlp = 64\n",
        "\n",
        "print(\"\\nTreinando MLP...\")\n",
        "history_mlp = mlp_model.fit(X_train, y_train,\n",
        "                          epochs=epochs_mlp,\n",
        "                          batch_size=batch_size_mlp,\n",
        "                          validation_split=0.1, # parte dos dados de treino para validação\n",
        "                          verbose=1)\n",
        "\n",
        "# avalia o Modelo MLP\n",
        "print(\"\\nAvaliando MLP no conjunto de teste...\")\n",
        "loss_mlp, accuracy_mlp = mlp_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da MLP no conjunto de teste: {accuracy_mlp:.4f}\")\n",
        "\n",
        "# classification report\n",
        "mlp_predictions_probs = mlp_model.predict(X_test)\n",
        "mlp_predictions = tf.argmax(mlp_predictions_probs, axis=1).numpy()\n",
        "\n",
        "# obtém probabilidades para AUC (TensorFlow/Keras model.predict retorna probabilidades com softmax)\n",
        "try:\n",
        "    # model.predict() em Keras com ativação 'softmax' na última camada já retorna probabilidades\n",
        "    mlp_auc = roc_auc_score(y_test, mlp_predictions_probs, multi_class='ovr', average='weighted')\n",
        "except Exception as e: # tratamento de exceção\n",
        "    mlp_auc = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "\n",
        "\n",
        "print(\"\\nAvaliação completa da MLP:\")\n",
        "print(classification_report(y_test, mlp_predictions))\n",
        "print(\"F1-score ponderado:\", f1_score(y_test, mlp_predictions, average='weighted'))\n",
        "print(\"AUC (ponderado, one-vs-rest):\", mlp_auc)\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "mlp_report = classification_report(y_test, mlp_predictions, output_dict=True)\n",
        "results['MLP'] = {\n",
        "    'accuracy': accuracy_score(y_test, mlp_predictions),\n",
        "    'precision (macro)': mlp_report['macro avg']['precision'],\n",
        "    'recall (macro)': mlp_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': mlp_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, mlp_predictions, average='weighted'),\n",
        "    'auc (weighted_ovr)': mlp_auc\n",
        "}\n",
        "\n",
        "\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s0C8wz0nQSuc",
        "outputId": "ab3841e1-3cd2-4228-8717-85632b339828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Treinando LightGBM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164951 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 30288\n",
            "[LightGBM] [Info] Number of data points in the train set: 19826, number of used features: 979\n",
            "[LightGBM] [Info] Start training from score -2.855966\n",
            "[LightGBM] [Info] Start training from score -0.255358\n",
            "[LightGBM] [Info] Start training from score -1.784623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avaliação do LightGBM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.20      0.28       290\n",
            "           1       0.93      0.94      0.94      3832\n",
            "           2       0.80      0.92      0.86       835\n",
            "\n",
            "    accuracy                           0.90      4957\n",
            "   macro avg       0.73      0.69      0.69      4957\n",
            "weighted avg       0.88      0.90      0.89      4957\n",
            "\n",
            "Acurácia:  0.8963082509582408\n",
            "F1-score ponderado: 0.886614172266831\n",
            "AUC (ponderado, one-vs-rest): 0.9453254006688412\n",
            "--------------------------------------------------\n",
            "Treinando Rede Neural Densa (MLP)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,280,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,313,539\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,539</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,539\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,539</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando MLP...\n",
            "Epoch 1/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 30ms/step - accuracy: 0.7889 - loss: 0.6339 - val_accuracy: 0.8926 - val_loss: 0.2952\n",
            "Epoch 2/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.9041 - loss: 0.2659 - val_accuracy: 0.8986 - val_loss: 0.2846\n",
            "Epoch 3/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - accuracy: 0.9315 - loss: 0.1901 - val_accuracy: 0.8921 - val_loss: 0.3081\n",
            "Epoch 4/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.9460 - loss: 0.1489 - val_accuracy: 0.8981 - val_loss: 0.3556\n",
            "Epoch 5/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 30ms/step - accuracy: 0.9592 - loss: 0.1216 - val_accuracy: 0.8886 - val_loss: 0.3699\n",
            "Epoch 6/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - accuracy: 0.9712 - loss: 0.0923 - val_accuracy: 0.8775 - val_loss: 0.4229\n",
            "Epoch 7/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - accuracy: 0.9772 - loss: 0.0733 - val_accuracy: 0.8845 - val_loss: 0.4753\n",
            "Epoch 8/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.9782 - loss: 0.0630 - val_accuracy: 0.8729 - val_loss: 0.5123\n",
            "Epoch 9/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - accuracy: 0.9816 - loss: 0.0522 - val_accuracy: 0.8749 - val_loss: 0.5323\n",
            "Epoch 10/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - accuracy: 0.9881 - loss: 0.0429 - val_accuracy: 0.8780 - val_loss: 0.5755\n",
            "Epoch 11/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.9882 - loss: 0.0373 - val_accuracy: 0.8754 - val_loss: 0.5907\n",
            "Epoch 12/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - accuracy: 0.9881 - loss: 0.0348 - val_accuracy: 0.8709 - val_loss: 0.6160\n",
            "Epoch 13/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - accuracy: 0.9899 - loss: 0.0312 - val_accuracy: 0.8724 - val_loss: 0.6461\n",
            "Epoch 14/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - accuracy: 0.9911 - loss: 0.0279 - val_accuracy: 0.8759 - val_loss: 0.6885\n",
            "Epoch 15/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - accuracy: 0.9915 - loss: 0.0242 - val_accuracy: 0.8795 - val_loss: 0.7249\n",
            "Epoch 16/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 39ms/step - accuracy: 0.9919 - loss: 0.0236 - val_accuracy: 0.8795 - val_loss: 0.7552\n",
            "Epoch 17/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 34ms/step - accuracy: 0.9925 - loss: 0.0230 - val_accuracy: 0.8724 - val_loss: 0.7549\n",
            "Epoch 18/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - accuracy: 0.9925 - loss: 0.0220 - val_accuracy: 0.8764 - val_loss: 0.7976\n",
            "Epoch 19/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.9930 - loss: 0.0195 - val_accuracy: 0.8800 - val_loss: 0.7717\n",
            "Epoch 20/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - accuracy: 0.9926 - loss: 0.0190 - val_accuracy: 0.8709 - val_loss: 0.7644\n",
            "\n",
            "Avaliando MLP no conjunto de teste...\n",
            "\n",
            "Acurácia da MLP no conjunto de teste: 0.8513\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "\n",
            "Avaliação completa da MLP:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.24      0.27       290\n",
            "           1       0.90      0.93      0.91      3832\n",
            "           2       0.79      0.71      0.75       835\n",
            "\n",
            "    accuracy                           0.85      4957\n",
            "   macro avg       0.66      0.63      0.64      4957\n",
            "weighted avg       0.84      0.85      0.85      4957\n",
            "\n",
            "F1-score ponderado: 0.8463880791262417\n",
            "AUC (ponderado, one-vs-rest): 0.8944160513380391\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Modelos de DL**"
      ],
      "metadata": {
        "id": "BaGFQTW_dhin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para explorar abordagens mais profundas de aprendizado, foi implementada **uma rede neural convolucional (CNN)** voltada para a classificação de textos. Como esse tipo de modelo trabalha melhor com sequências de palavras em vez de vetores TF-IDF, os tweets foram tokenizados e convertidos em **sequências inteiras**, com padding para garantir um comprimento fixo.\n",
        "\n",
        "O modelo foi construído com uma camada de embedding (para mapear palavras em vetores densos), seguida por uma **camada convolucional 1D** que captura padrões locais no texto e uma **camada de pooling** que extrai as informações mais relevantes. Camadas densas e dropout foram adicionadas para refinar o aprendizado e reduzir overfitting. O modelo foi treinado por 10 épocas e avaliado com base em métricas como acurácia, F1-score e AUC multiclasse. Essa arquitetura permite capturar melhor a estrutura local e semântica dos textos, sendo especialmente útil para dados curtos e ruidosos como tweets."
      ],
      "metadata": {
        "id": "poME_r3-V-Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reutiliza importações de bibliotecas\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# --- Pré-processamento para CNN ---\n",
        "\n",
        "# O TfidfVectorizer não é ideal para CNNs, precisamos de uma representação baseada em sequências/embeddings.\n",
        "\n",
        "# 1. Tokenização para CNN (Sequências de Inteiros)\n",
        "max_words = 10000\n",
        "max_sequence_length = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df['processed_text'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df['processed_text'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "print(\"\\nExemplo de sequências padded:\")\n",
        "print(padded_sequences[:5])\n",
        "print(\"\\nForma das sequências padded:\", padded_sequences.shape)\n",
        "\n",
        "# --- prepara labels para CNN ---\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_y = label_encoder.fit_transform(df['class'])\n",
        "\n",
        "print(\"\\nLabels originais:\", df['class'].unique())\n",
        "print(\"Labels codificadas:\", encoded_y)\n",
        "\n",
        "# --- divide dados para CNN (usando as sequências e labels codificadas) ---\n",
        "X_cnn = padded_sequences\n",
        "y_cnn = encoded_y\n",
        "\n",
        "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(X_cnn, y_cnn, test_size=0.20, random_state=42)\n",
        "\n",
        "print(\"\\nForma dos dados de treino para CNN:\", X_train_cnn.shape, y_train_cnn.shape)\n",
        "print(\"Forma dos dados de teste para CNN:\", X_test_cnn.shape, y_test_cnn.shape)\n",
        "\n",
        "# --- constrói o Modelo CNN ---\n",
        "embedding_dim = 128\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_filters = 128\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    Conv1D(filters=num_filters, kernel_size=filter_sizes[0], activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# --- treina o Modelo CNN---\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "print(\"\\nTreinando CNN sem class_weight...\")\n",
        "history = model.fit(X_train_cnn, y_train_cnn,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_split=0.1,\n",
        "                    verbose=1)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# --- avaliação do modelo\n",
        "print(\"\\nAvaliando CNN no conjunto de teste...\")\n",
        "loss, accuracy = model.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da CNN no conjunto de teste: {accuracy:.4f}\")\n",
        "\n",
        "# classification report para CNN\n",
        "cnn_predictions_probs = model.predict(X_test_cnn)\n",
        "cnn_predictions = tf.argmax(cnn_predictions_probs, axis=1).numpy()\n",
        "\n",
        "print(\"\\nAvaliação completa da CNN:\")\n",
        "print(classification_report(y_test_cnn, cnn_predictions))\n",
        "\n",
        "# cálculo F1-score ponderado e AUC para a CNN\n",
        "cnn_f1_weighted = f1_score(y_test_cnn, cnn_predictions, average='weighted')\n",
        "\n",
        "try:\n",
        "    cnn_auc = roc_auc_score(y_test_cnn, cnn_predictions_probs, multi_class='ovr', average='weighted')\n",
        "except Exception as e:\n",
        "    cnn_auc = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "\n",
        "print(\"F1-score ponderado (CNN):\", cnn_f1_weighted)\n",
        "print(\"AUC (ponderado, one-vs-rest) (CNN):\", cnn_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "try:\n",
        "    cnn_report = classification_report(y_test_cnn, cnn_predictions, output_dict=True)\n",
        "    results['CNN (Embedding+Seq, No Class_Weight)'] = {\n",
        "        'accuracy': accuracy_score(y_test_cnn, cnn_predictions),\n",
        "        'precision (macro)': cnn_report['macro avg']['precision'],\n",
        "        'recall (macro)': cnn_report['macro avg']['recall'],\n",
        "        'f1-score (macro)': cnn_report['macro avg']['f1-score'],\n",
        "        'f1-score (weighted)': cnn_f1_weighted,\n",
        "        'auc (weighted_ovr)': cnn_auc\n",
        "    }\n",
        "except NameError:\n",
        "    print(\"Dicionário 'results' não encontrado. Os resultados da CNN (sem class_weight) não foram armazenados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5Rswna5mV5bO",
        "outputId": "aacc7155-3cc1-4e61-ed3f-4308f4d5cabf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Exemplo de sequências padded:\n",
            "[[   3  100  838  920 2932  204   20   38   84   71   17    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   3   96   78 8148 5799   37 2045   78    4  457  381    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   3  635    3   97   14    2  175  256  921   15    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   3   33    5  568    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   3   15  433  235  434  235 4598    2  185   46    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "\n",
            "Forma das sequências padded: (24783, 100)\n",
            "\n",
            "Labels originais: [2 1 0]\n",
            "Labels codificadas: [2 1 1 ... 1 1 2]\n",
            "\n",
            "Forma dos dados de treino para CNN: (19826, 100) (19826,)\n",
            "Forma dos dados de teste para CNN: (4957, 100) (4957,)\n",
            "\n",
            "Treinando CNN sem class_weight...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 51ms/step - accuracy: 0.8300 - loss: 0.4777 - val_accuracy: 0.9097 - val_loss: 0.2735\n",
            "Epoch 2/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 47ms/step - accuracy: 0.9209 - loss: 0.2217 - val_accuracy: 0.9037 - val_loss: 0.2652\n",
            "Epoch 3/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 46ms/step - accuracy: 0.9549 - loss: 0.1342 - val_accuracy: 0.9057 - val_loss: 0.3110\n",
            "Epoch 4/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 46ms/step - accuracy: 0.9787 - loss: 0.0648 - val_accuracy: 0.9052 - val_loss: 0.3900\n",
            "Epoch 5/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 45ms/step - accuracy: 0.9877 - loss: 0.0405 - val_accuracy: 0.8865 - val_loss: 0.4609\n",
            "Epoch 6/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 42ms/step - accuracy: 0.9888 - loss: 0.0322 - val_accuracy: 0.8916 - val_loss: 0.5410\n",
            "Epoch 7/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 44ms/step - accuracy: 0.9924 - loss: 0.0237 - val_accuracy: 0.8906 - val_loss: 0.5615\n",
            "Epoch 8/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.9936 - loss: 0.0183 - val_accuracy: 0.8850 - val_loss: 0.6342\n",
            "Epoch 9/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.9958 - loss: 0.0151 - val_accuracy: 0.8785 - val_loss: 0.7190\n",
            "Epoch 10/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 63ms/step - accuracy: 0.9941 - loss: 0.0155 - val_accuracy: 0.8941 - val_loss: 0.7649\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m49,280\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_4          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_4          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,038,539\u001b[0m (15.41 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,038,539</span> (15.41 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,346,179\u001b[0m (5.14 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,346,179</span> (5.14 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,692,360\u001b[0m (10.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,692,360</span> (10.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando CNN no conjunto de teste...\n",
            "\n",
            "Acurácia da CNN no conjunto de teste: 0.8745\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
            "\n",
            "Avaliação completa da CNN:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.23      0.29       290\n",
            "           1       0.91      0.94      0.93      3832\n",
            "           2       0.79      0.80      0.80       835\n",
            "\n",
            "    accuracy                           0.87      4957\n",
            "   macro avg       0.70      0.66      0.67      4957\n",
            "weighted avg       0.86      0.87      0.87      4957\n",
            "\n",
            "F1-score ponderado (CNN): 0.8673805816983572\n",
            "AUC (ponderado, one-vs-rest) (CNN): 0.9169551714873231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O modelo baseado em **LSTM (Long Short-Term Memory)** foi desenvolvido como uma abordagem voltada ao reconhecimento de padrões sequenciais nos tweets. Utilizando a mesma base de dados preparada para a CNN, os textos foram previamente tokenizados e convertidos em sequências numéricas com padding, garantindo comprimento uniforme.\n",
        "\n",
        "A arquitetura do modelo inclui uma **camada de embedding**, que transforma cada palavra em um vetor denso, seguida por uma camada **LSTM com 128 unidades**, capaz de capturar dependências de longo prazo no texto. Foram adicionadas camadas densas e de **dropout** para auxiliar na generalização do modelo. A saída utiliza a função softmax para classificação multiclasse.\n",
        "\n",
        "O modelo foi treinado por 15 épocas e avaliado por meio de métricas como acurácia, F1-score ponderado e AUC, demonstrando seu potencial para lidar com a natureza sequencial e contextual das mensagens de texto. Essa abordagem é especialmente adequada para capturar a ordem das palavras, o que pode ser relevante na detecção de nuances ofensivas em linguagem natural."
      ],
      "metadata": {
        "id": "aNAtlPLSYC5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "# from tensorflow.keras.layers import Embedding, Dense, Dropout, GlobalMaxPooling1D, Conv1D\n",
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score # Já importado\n",
        "\n",
        "# --- Reutiliza dados preparados para a CNN ---\n",
        "# X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn\n",
        "# max_words, max_sequence_length, embedding_dim\n",
        "# label_encoder (fitado)\n",
        "\n",
        "# --- Constrói o Modelo LSTM ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Construindo o Modelo LSTM...\")\n",
        "\n",
        "model_lstm = Sequential([\n",
        "    # camada de embedding: Reutiliza os mesmos parâmetros da CNN para consistência na base de embedding\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "\n",
        "    # camada LSTM\n",
        "    # return_sequences=False significa que a LSTM retorna a saída da última etapa de tempo (para classificação de sequência)\n",
        "    # return_sequences=True seria para tarefas de predição de sequência ou para empilhar LSTMs\n",
        "    LSTM(128),\n",
        "\n",
        "    # camada de dropout para evitar overfitting\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # camada densa (Fully Connected)\n",
        "    Dense(64, activation='relu'), # camada densa adicional\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # camada de saída (igual à CNN)\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model_lstm.compile(optimizer='adam',\n",
        "                   loss='sparse_categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "model_lstm.summary()\n",
        "\n",
        "# --- treina o Modelo LSTM  ---\n",
        "epochs_lstm = 15\n",
        "batch_size_lstm = 32\n",
        "\n",
        "print(\"\\nTreinando LSTM sem class_weight...\")\n",
        "history_lstm = model_lstm.fit(X_train_cnn, y_train_cnn, # usa os mesmos dados preparados para CNN\n",
        "                              epochs=epochs_lstm,\n",
        "                              batch_size=batch_size_lstm,\n",
        "                              validation_split=0.1,\n",
        "                              verbose=1)\n",
        "\n",
        "\n",
        "# --- avaliação do modelo\n",
        "print(\"\\nAvaliando LSTM no conjunto de teste...\")\n",
        "loss_lstm, accuracy_lstm = model_lstm.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da LSTM no conjunto de teste: {accuracy_lstm:.4f}\")\n",
        "\n",
        "# classification report para LSTM\n",
        "lstm_predictions_probs = model_lstm.predict(X_test_cnn)\n",
        "lstm_predictions = tf.argmax(lstm_predictions_probs, axis=1).numpy()\n",
        "\n",
        "print(\"\\nAvaliação completa da LSTM:\")\n",
        "# lembrete que y_test_cnn são labels codificadas (0, 1, 2)\n",
        "class_names = [str(cls) for cls in label_encoder.classes_]\n",
        "print(classification_report(y_test_cnn, lstm_predictions, target_names=class_names)) #\n",
        "\n",
        "# cálculo de F1-score ponderado e AUC para a LSTM\n",
        "lstm_f1_weighted = f1_score(y_test_cnn, lstm_predictions, average='weighted')\n",
        "\n",
        "try:\n",
        "    lstm_auc = roc_auc_score(y_test_cnn, lstm_predictions_probs, multi_class='ovr', average='weighted')\n",
        "except Exception as e:\n",
        "    lstm_auc = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "\n",
        "print(\"F1-score ponderado (LSTM):\", lstm_f1_weighted)\n",
        "print(\"AUC (ponderado, one-vs-rest) (LSTM):\", lstm_auc)\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "# dicionário de resultados\n",
        "try:\n",
        "    lstm_report = classification_report(y_test_cnn, lstm_predictions, output_dict=True, target_names=class_names)\n",
        "    results['LSTM (Embedding+Seq, No Class_Weight)'] = {\n",
        "        'accuracy': accuracy_score(y_test_cnn, lstm_predictions),\n",
        "        'precision (macro)': lstm_report['macro avg']['precision'],\n",
        "        'recall (macro)': lstm_report['macro avg']['recall'],\n",
        "        'f1-score (macro)': lstm_report['macro avg']['f1-score'],\n",
        "        'f1-score (weighted)': lstm_f1_weighted,\n",
        "        'auc (weighted_ovr)': lstm_auc\n",
        "    }\n",
        "except NameError:\n",
        "    print(\"Dicionário 'results' não encontrado. Os resultados da LSTM (sem class_weight) não foram armazenados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-TfkmaT5RGku",
        "outputId": "91c21444-bb97-4d9e-8b22-21440fd3d010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Construindo o Modelo LSTM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando LSTM sem class_weight...\n",
            "Epoch 1/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 208ms/step - accuracy: 0.7655 - loss: 0.7198 - val_accuracy: 0.7796 - val_loss: 0.6582\n",
            "Epoch 2/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 258ms/step - accuracy: 0.7748 - loss: 0.6770 - val_accuracy: 0.7796 - val_loss: 0.6490\n",
            "Epoch 3/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 197ms/step - accuracy: 0.7752 - loss: 0.6738 - val_accuracy: 0.7796 - val_loss: 0.6474\n",
            "Epoch 4/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 215ms/step - accuracy: 0.7752 - loss: 0.6675 - val_accuracy: 0.7796 - val_loss: 0.6472\n",
            "Epoch 5/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 196ms/step - accuracy: 0.7682 - loss: 0.6822 - val_accuracy: 0.7796 - val_loss: 0.6480\n",
            "Epoch 6/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 197ms/step - accuracy: 0.7762 - loss: 0.6638 - val_accuracy: 0.7796 - val_loss: 0.6483\n",
            "Epoch 7/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 203ms/step - accuracy: 0.7729 - loss: 0.6678 - val_accuracy: 0.7796 - val_loss: 0.6479\n",
            "Epoch 8/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 203ms/step - accuracy: 0.7727 - loss: 0.6683 - val_accuracy: 0.7796 - val_loss: 0.6482\n",
            "Epoch 9/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 195ms/step - accuracy: 0.7762 - loss: 0.6630 - val_accuracy: 0.7796 - val_loss: 0.6478\n",
            "Epoch 10/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 199ms/step - accuracy: 0.7762 - loss: 0.6651 - val_accuracy: 0.7796 - val_loss: 0.6473\n",
            "Epoch 11/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 199ms/step - accuracy: 0.7801 - loss: 0.6585 - val_accuracy: 0.7796 - val_loss: 0.6472\n",
            "Epoch 12/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 198ms/step - accuracy: 0.7711 - loss: 0.6704 - val_accuracy: 0.7796 - val_loss: 0.6482\n",
            "Epoch 13/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 201ms/step - accuracy: 0.7781 - loss: 0.6583 - val_accuracy: 0.7796 - val_loss: 0.6486\n",
            "Epoch 14/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 198ms/step - accuracy: 0.7707 - loss: 0.6721 - val_accuracy: 0.7796 - val_loss: 0.6474\n",
            "Epoch 15/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 202ms/step - accuracy: 0.7790 - loss: 0.6553 - val_accuracy: 0.7796 - val_loss: 0.6489\n",
            "\n",
            "Avaliando LSTM no conjunto de teste...\n",
            "\n",
            "Acurácia da LSTM no conjunto de teste: 0.7730\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step\n",
            "\n",
            "Avaliação completa da LSTM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       290\n",
            "           1       0.77      1.00      0.87      3832\n",
            "           2       0.00      0.00      0.00       835\n",
            "\n",
            "    accuracy                           0.77      4957\n",
            "   macro avg       0.26      0.33      0.29      4957\n",
            "weighted avg       0.60      0.77      0.67      4957\n",
            "\n",
            "F1-score ponderado (LSTM): 0.6740973395206054\n",
            "AUC (ponderado, one-vs-rest) (LSTM): 0.5010165791764108\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Balanceamento do dataset e retreino dos modelos**"
      ],
      "metadata": {
        "id": "XGQsnDqrwSlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Class_weight**"
      ],
      "metadata": {
        "id": "LzDZw4uE4ZL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta etapa do experimento, foi aplicado o **balanceamento de classes por meio do parâmetro class_weight**, com o objetivo de mitigar o impacto do desbalanceamento dos dados na performance dos modelos.\n",
        "\n",
        "Utilizando a função `compute_class_weight` da biblioteca `sklearn`, foram calculados pesos proporcionais à frequência das classes no conjunto de treinamento. Esses pesos foram incorporados diretamente ao processo de treinamento em modelos que suportam esse recurso, como **Regressão Logística, SVM, Random Forest, LightGBM e Redes Neurais com Keras**.\n",
        "\n",
        "A inclusão dos pesos penaliza erros cometidos em classes minoritárias, forçando os algoritmos a considerarem com maior atenção esses exemplos menos frequentes. Essa estratégia é especialmente eficaz em cenários de classificação desbalanceada, como o de detecção de discursos ofensivos, contribuindo para **melhorias em métricas como recall e F1-score das classes minoritárias, sem necessariamente comprometer a acurácia geral**.\n",
        "\n",
        "Modelos como o Naive Bayes Multinomial, que não oferecem suporte direto a class_weight, foram mantidos como referência sem ajuste neste momento."
      ],
      "metadata": {
        "id": "iDaObzs1ZtpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import lightgbm as lgb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# X e y já estão definidos e representam a matriz TF-IDF e os rótulos originais.\n",
        "\n",
        "print(\"Forma dos dados de treino (X_train, y_train):\", X_train.shape, y_train.shape)\n",
        "print(\"Forma dos dados de teste (X_test, y_test):\", X_test.shape, y_test.shape)\n",
        "\n",
        "# --- cálculo de Class Weights ---\n",
        "# uso do y_train para calcular os pesos, pois o balanceamento deve ser baseado nos dados de treino.\n",
        "classes = np.unique(y_train)\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
        "class_weight_dict = dict(zip(classes, class_weights))\n",
        "\n",
        "print(\"\\nPesos das classes calculados (baseado em y_train):\")\n",
        "print(class_weight_dict)\n",
        "\n",
        "# 1. Regressão Logística\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Treinando Regressão Logística com class_weight...\")\n",
        "lr_model = LogisticRegression(max_iter=1000, class_weight=class_weight_dict)\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "\n",
        "# probabilidades para AUC\n",
        "try:\n",
        "    lr_probabilities = lr_model.predict_proba(X_test)\n",
        "    lr_auc = roc_auc_score(y_test, lr_probabilities, multi_class='ovr', average='weighted')\n",
        "except AttributeError:\n",
        "    lr_auc = \"N/A (predict_proba não disponível ou com problema)\"\n",
        "\n",
        "\n",
        "print(\"Avaliação da Regressão Logística:\")\n",
        "print(classification_report(y_test, lr_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lr_predictions))\n",
        "print(\"F1-score ponderado:\", f1_score(y_test, lr_predictions, average='weighted'))\n",
        "print(\"AUC (ponderado, one-vs-rest):\", lr_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "lr_report = classification_report(y_test, lr_predictions, output_dict=True)\n",
        "results['Logistic Regression (Class_Weight)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lr_predictions),\n",
        "    'precision (macro)': lr_report['macro avg']['precision'],\n",
        "    'recall (macro)': lr_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lr_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lr_predictions, average='weighted'),\n",
        "    'auc (weighted_ovr)': lr_auc\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 2. Naive Bayes Multinomial\n",
        "# Naive Bayes Multinomial no scikit-learn não tem o argumento class_weight diretamente.\n",
        "# O treino será sem class_weight, como é o padrão para este modelo.\n",
        "\n",
        "print(\"Treinando Naive Bayes Multinomial (sem class_weight)...\")\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_predictions = nb_model.predict(X_test)\n",
        "\n",
        "try:\n",
        "    nb_probabilities = nb_model.predict_proba(X_test)\n",
        "    nb_auc = roc_auc_score(y_test, nb_probabilities, multi_class='ovr', average='weighted')\n",
        "except AttributeError:\n",
        "    nb_auc = \"N/A (predict_proba não disponível ou com problema)\"\n",
        "\n",
        "\n",
        "print(\"Avaliação do Naive Bayes Multinomial:\")\n",
        "print(classification_report(y_test, nb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, nb_predictions))\n",
        "print(\"F1-score ponderado:\", f1_score(y_test, nb_predictions, average='weighted'))\n",
        "print(\"AUC (ponderado, one-vs-rest):\", nb_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "nb_report = classification_report(y_test, nb_predictions, output_dict=True)\n",
        "results['Multinomial NB (No Class_Weight)'] = {\n",
        "    'accuracy': accuracy_score(y_test, nb_predictions),\n",
        "    'precision (macro)': nb_report['macro avg']['precision'],\n",
        "    'recall (macro)': nb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': nb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, nb_predictions, average='weighted'),\n",
        "    'auc (weighted_ovr)': nb_auc\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 3. Support Vector Machine (SVM)\n",
        "\n",
        "print(\"Treinando SVM (Kernel Linear) com class_weight...\")\n",
        "\n",
        "svm_model = SVC(kernel='linear', class_weight=class_weight_dict, probability=True)\n",
        "\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_predictions = svm_model.predict(X_test)\n",
        "\n",
        "# cálculo de AUC para SVM com probability=True\n",
        "try:\n",
        "    if hasattr(svm_model, 'predict_proba'):\n",
        "        svm_probabilities = svm_model.predict_proba(X_test)\n",
        "        svm_auc = roc_auc_score(y_test, svm_probabilities, multi_class='ovr', average='weighted')\n",
        "    else:\n",
        "         svm_auc = \"N/A (probability=False)\"\n",
        "except AttributeError:\n",
        "    svm_auc = \"N/A (problema ao obter predict_proba)\"\n",
        "\n",
        "\n",
        "print(\"Avaliação do SVM:\")\n",
        "print(classification_report(y_test, svm_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, svm_predictions))\n",
        "print(\"F1-score ponderado:\", f1_score(y_test, svm_predictions, average='weighted'))\n",
        "print(\"AUC (ponderado, one-vs-rest):\", svm_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "svm_report = classification_report(y_test, svm_predictions, output_dict=True)\n",
        "results['SVM (Class_Weight)'] = {\n",
        "    'accuracy': accuracy_score(y_test, svm_predictions),\n",
        "    'precision (macro)': svm_report['macro avg']['precision'],\n",
        "    'recall (macro)': svm_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': svm_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, svm_predictions, average='weighted'),\n",
        "    'auc (weighted_ovr)': svm_auc\n",
        "}\n",
        "\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 4. Random Forest Classifier\n",
        "print(\"Treinando Random Forest com class_weight...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=class_weight_dict)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "\n",
        "try:\n",
        "    rf_probabilities = rf_model.predict_proba(X_test)\n",
        "    rf_auc = roc_auc_score(y_test, rf_probabilities, multi_class='ovr', average='weighted')\n",
        "except AttributeError:\n",
        "    rf_auc = \"N/A (predict_proba não disponível ou com problema)\"\n",
        "\n",
        "\n",
        "print(\"Avaliação do Random Forest:\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, rf_predictions))\n",
        "print(\"F1-score ponderado:\", f1_score(y_test, rf_predictions, average='weighted'))\n",
        "print(\"AUC (ponderado, one-vs-rest):\", rf_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "rf_report = classification_report(y_test, rf_predictions, output_dict=True)\n",
        "results['Random Forest (Class_Weight)'] = {\n",
        "    'accuracy': accuracy_score(y_test, rf_predictions),\n",
        "    'precision (macro)': rf_report['macro avg']['precision'],\n",
        "    'recall (macro)': rf_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': rf_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, rf_predictions, average='weighted'),\n",
        "    'auc (weighted_ovr)': rf_auc\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 5. LightGBM (Gradient Boosting)\n",
        "print(\"Treinando LightGBM com class_weight...\")\n",
        "lgb_model = lgb.LGBMClassifier(objective='multiclass', num_class=len(df['class'].unique()), random_state=42, class_weight=class_weight_dict)\n",
        "\n",
        "lgb_model.fit(X_train, y_train)\n",
        "lgb_predictions = lgb_model.predict(X_test)\n",
        "\n",
        "try:\n",
        "    lgb_probabilities = lgb_model.predict_proba(X_test)\n",
        "    lgb_auc = roc_auc_score(y_test, lgb_probabilities, multi_class='ovr', average='weighted')\n",
        "except AttributeError:\n",
        "    lgb_auc = \"N/A (predict_proba não disponível ou com problema)\"\n",
        "\n",
        "\n",
        "print(\"Avaliação do LightGBM:\")\n",
        "print(classification_report(y_test, lgb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lgb_predictions))\n",
        "print(\"F1-score ponderado:\", f1_score(y_test, lgb_predictions, average='weighted'))\n",
        "print(\"AUC (ponderado, one-vs-rest):\", lgb_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "lgb_report = classification_report(y_test, lgb_predictions, output_dict=True)\n",
        "results['LightGBM (Class_Weight)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lgb_predictions),\n",
        "    'precision (macro)': lgb_report['macro avg']['precision'],\n",
        "    'recall (macro)': lgb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lgb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lgb_predictions, average='weighted'),\n",
        "    'auc (weighted_ovr)': lgb_auc\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 6. Rede Neural Densa (MLP) com TensorFlow/Keras\n",
        "print(\"Treinando Rede Neural Densa (MLP) com class_weight...\")\n",
        "\n",
        "# constrói do Modelo MLP (estrutura igual)\n",
        "mlp_model = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(df['class'].unique()), activation='softmax')\n",
        "])\n",
        "\n",
        "mlp_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "mlp_model.summary()\n",
        "\n",
        "# treina o Modelo MLP\n",
        "epochs_mlp = 20\n",
        "batch_size_mlp = 64\n",
        "\n",
        "print(\"\\nTreinando MLP com class_weight...\")\n",
        "history_mlp = mlp_model.fit(X_train, y_train,\n",
        "                          epochs=epochs_mlp,\n",
        "                          batch_size=batch_size_mlp,\n",
        "                          validation_split=0.1,\n",
        "                          verbose=1,\n",
        "                          class_weight=class_weight_dict)\n",
        "mlp_model.summary()\n",
        "\n",
        "# avaliação do modelo\n",
        "print(\"\\nAvaliando MLP no conjunto de teste original...\")\n",
        "loss_mlp, accuracy_mlp = mlp_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da MLP no conjunto de teste original: {accuracy_mlp:.4f}\")\n",
        "\n",
        "# classification report para MLP\n",
        "mlp_predictions_probs = mlp_model.predict(X_test)\n",
        "mlp_predictions = tf.argmax(mlp_predictions_probs, axis=1).numpy()\n",
        "\n",
        "try:\n",
        "    mlp_auc = roc_auc_score(y_test, mlp_predictions_probs, multi_class='ovr', average='weighted')\n",
        "except Exception as e:\n",
        "    mlp_auc = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "\n",
        "print(\"\\nAvaliação completa da MLP:\")\n",
        "print(classification_report(y_test, mlp_predictions))\n",
        "print(\"F1-score ponderado:\", f1_score(y_test, mlp_predictions, average='weighted'))\n",
        "print(\"AUC (ponderado, one-vs-rest):\", mlp_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "mlp_report = classification_report(y_test, mlp_predictions, output_dict=True)\n",
        "results['MLP (Class_Weight)'] = {\n",
        "    'accuracy': accuracy_score(y_test, mlp_predictions),\n",
        "    'precision (macro)': mlp_report['macro avg']['precision'],\n",
        "    'recall (macro)': mlp_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': mlp_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, mlp_predictions, average='weighted'),\n",
        "    'auc (weighted_ovr)': mlp_auc\n",
        "}\n",
        "\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SIj0kM-iRwXY",
        "outputId": "6d0adead-d8cb-41cf-eb53-6639d8d71d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma dos dados de treino (X_train, y_train): (19826, 5000) (19826,)\n",
            "Forma dos dados de teste (X_test, y_test): (4957, 5000) (4957,)\n",
            "\n",
            "Pesos das classes calculados (baseado em y_train):\n",
            "{np.int64(0): np.float64(5.797076023391813), np.int64(1): np.float64(0.4303077657681122), np.int64(2): np.float64(1.9857772435897436)}\n",
            "\n",
            "==================================================\n",
            "Treinando Regressão Logística com class_weight...\n",
            "Avaliação da Regressão Logística:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.57      0.38       290\n",
            "           1       0.97      0.84      0.90      3832\n",
            "           2       0.72      0.93      0.81       835\n",
            "\n",
            "    accuracy                           0.84      4957\n",
            "   macro avg       0.66      0.78      0.70      4957\n",
            "weighted avg       0.89      0.84      0.85      4957\n",
            "\n",
            "Acurácia:  0.8376033891466613\n",
            "F1-score ponderado: 0.8538630525491657\n",
            "AUC (ponderado, one-vs-rest): 0.9313824779471941\n",
            "--------------------------------------------------\n",
            "Treinando Naive Bayes Multinomial (sem class_weight)...\n",
            "Avaliação do Naive Bayes Multinomial:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.00      0.01       290\n",
            "           1       0.83      0.99      0.90      3832\n",
            "           2       0.90      0.38      0.53       835\n",
            "\n",
            "    accuracy                           0.83      4957\n",
            "   macro avg       0.74      0.46      0.48      4957\n",
            "weighted avg       0.82      0.83      0.79      4957\n",
            "\n",
            "Acurácia:  0.8321565462981642\n",
            "F1-score ponderado: 0.7881920612643274\n",
            "AUC (ponderado, one-vs-rest): 0.8859837248469582\n",
            "--------------------------------------------------\n",
            "Treinando SVM (Kernel Linear) com class_weight...\n",
            "Avaliação do SVM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.58      0.36       290\n",
            "           1       0.97      0.84      0.90      3832\n",
            "           2       0.76      0.92      0.83       835\n",
            "\n",
            "    accuracy                           0.84      4957\n",
            "   macro avg       0.67      0.78      0.70      4957\n",
            "weighted avg       0.89      0.84      0.86      4957\n",
            "\n",
            "Acurácia:  0.8374016542263466\n",
            "F1-score ponderado: 0.8572085879625051\n",
            "AUC (ponderado, one-vs-rest): 0.9409039984777016\n",
            "--------------------------------------------------\n",
            "Treinando Random Forest com class_weight...\n",
            "Avaliação do Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.19      0.27       290\n",
            "           1       0.92      0.95      0.94      3832\n",
            "           2       0.80      0.87      0.84       835\n",
            "\n",
            "    accuracy                           0.89      4957\n",
            "   macro avg       0.73      0.67      0.68      4957\n",
            "weighted avg       0.88      0.89      0.88      4957\n",
            "\n",
            "Acurácia:  0.8906596731894291\n",
            "F1-score ponderado: 0.880224938688829\n",
            "AUC (ponderado, one-vs-rest): 0.9394144502777446\n",
            "--------------------------------------------------\n",
            "Treinando LightGBM com class_weight...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.145318 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 30288\n",
            "[LightGBM] [Info] Number of data points in the train set: 19826, number of used features: 979\n",
            "[LightGBM] [Info] Start training from score -1.098612\n",
            "[LightGBM] [Info] Start training from score -1.098612\n",
            "[LightGBM] [Info] Start training from score -1.098612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avaliação do LightGBM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.61      0.40       290\n",
            "           1       0.97      0.85      0.90      3832\n",
            "           2       0.76      0.94      0.84       835\n",
            "\n",
            "    accuracy                           0.85      4957\n",
            "   macro avg       0.68      0.80      0.72      4957\n",
            "weighted avg       0.90      0.85      0.86      4957\n",
            "\n",
            "Acurácia:  0.8484970748436554\n",
            "F1-score ponderado: 0.8646944512588363\n",
            "AUC (ponderado, one-vs-rest): 0.9386147953065481\n",
            "--------------------------------------------------\n",
            "Treinando Rede Neural Densa (MLP) com class_weight...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_20\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_20\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,280,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_32 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_33 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_50 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,313,539\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,539</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,539\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,539</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando MLP com class_weight...\n",
            "Epoch 1/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.6862 - loss: 0.9423 - val_accuracy: 0.8033 - val_loss: 0.5543\n",
            "Epoch 2/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 28ms/step - accuracy: 0.8409 - loss: 0.4212 - val_accuracy: 0.7932 - val_loss: 0.5386\n",
            "Epoch 3/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.8758 - loss: 0.3008 - val_accuracy: 0.8346 - val_loss: 0.4631\n",
            "Epoch 4/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 28ms/step - accuracy: 0.9031 - loss: 0.2183 - val_accuracy: 0.8089 - val_loss: 0.5343\n",
            "Epoch 5/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.9193 - loss: 0.1678 - val_accuracy: 0.8422 - val_loss: 0.4857\n",
            "Epoch 6/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.9405 - loss: 0.1256 - val_accuracy: 0.8452 - val_loss: 0.5108\n",
            "Epoch 7/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.9474 - loss: 0.1082 - val_accuracy: 0.8477 - val_loss: 0.5262\n",
            "Epoch 8/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.9607 - loss: 0.0801 - val_accuracy: 0.8492 - val_loss: 0.5560\n",
            "Epoch 9/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.9683 - loss: 0.0717 - val_accuracy: 0.8563 - val_loss: 0.5758\n",
            "Epoch 10/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 28ms/step - accuracy: 0.9699 - loss: 0.0629 - val_accuracy: 0.8492 - val_loss: 0.5847\n",
            "Epoch 11/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.9748 - loss: 0.0574 - val_accuracy: 0.8497 - val_loss: 0.6070\n",
            "Epoch 12/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - accuracy: 0.9782 - loss: 0.0488 - val_accuracy: 0.8593 - val_loss: 0.6116\n",
            "Epoch 13/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.9816 - loss: 0.0429 - val_accuracy: 0.8613 - val_loss: 0.6391\n",
            "Epoch 14/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.9827 - loss: 0.0411 - val_accuracy: 0.8598 - val_loss: 0.6698\n",
            "Epoch 15/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.9841 - loss: 0.0354 - val_accuracy: 0.8563 - val_loss: 0.6793\n",
            "Epoch 16/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - accuracy: 0.9846 - loss: 0.0421 - val_accuracy: 0.8603 - val_loss: 0.7087\n",
            "Epoch 17/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.9873 - loss: 0.0310 - val_accuracy: 0.8578 - val_loss: 0.6956\n",
            "Epoch 18/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - accuracy: 0.9880 - loss: 0.0298 - val_accuracy: 0.8522 - val_loss: 0.7641\n",
            "Epoch 19/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.9873 - loss: 0.0314 - val_accuracy: 0.8623 - val_loss: 0.7535\n",
            "Epoch 20/20\n",
            "\u001b[1m279/279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.9885 - loss: 0.0270 - val_accuracy: 0.8492 - val_loss: 0.8218\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_20\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_20\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,280,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_32 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_33 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_50 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,940,619\u001b[0m (15.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,940,619</span> (15.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,539\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,539</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,627,080\u001b[0m (10.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,627,080</span> (10.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando MLP no conjunto de teste original...\n",
            "\n",
            "Acurácia da MLP no conjunto de teste original: 0.8318\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
            "\n",
            "Avaliação completa da MLP:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.41      0.33       290\n",
            "           1       0.93      0.87      0.90      3832\n",
            "           2       0.73      0.80      0.76       835\n",
            "\n",
            "    accuracy                           0.83      4957\n",
            "   macro avg       0.64      0.69      0.66      4957\n",
            "weighted avg       0.86      0.83      0.84      4957\n",
            "\n",
            "F1-score ponderado: 0.8420082737857929\n",
            "AUC (ponderado, one-vs-rest): 0.8977889468548846\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np # para uso de unique e zip\n",
        "\n",
        "# --- Pré-processamento para CNN ---\n",
        "\n",
        "# O TfidfVectorizer não é ideal para CNNs, então opta-se por uma representação baseada em sequências/embeddings.\n",
        "\n",
        "# 1. Tokenização para CNN (Sequências de Inteiros)\n",
        "max_words = 10000\n",
        "max_sequence_length = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df['processed_text'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df['processed_text'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "print(\"\\nExemplo de sequências padded:\")\n",
        "print(padded_sequences[:5])\n",
        "print(\"\\nForma das sequências padded:\", padded_sequences.shape)\n",
        "\n",
        "# --- prepara labels para CNN ---\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_y = label_encoder.fit_transform(df['class'])\n",
        "\n",
        "print(\"\\nLabels originais:\", df['class'].unique())\n",
        "print(\"Labels codificadas:\", encoded_y)\n",
        "\n",
        "# --- divisão dos dados para CNN (usando as sequências e labels codificadas) ---\n",
        "X_cnn = padded_sequences\n",
        "y_cnn = encoded_y\n",
        "\n",
        "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(X_cnn, y_cnn, test_size=0.20, random_state=42)\n",
        "\n",
        "print(\"\\nForma dos dados de treino para CNN:\", X_train_cnn.shape, y_train_cnn.shape)\n",
        "print(\"Forma dos dados de teste para CNN:\", X_test_cnn.shape, y_test_cnn.shape)\n",
        "\n",
        "\n",
        "# --- cálculo de Class Weights para a CNN (baseado em y_train_cnn) ---\n",
        "classes_cnn = np.unique(y_train_cnn)\n",
        "class_weights_cnn = compute_class_weight(class_weight='balanced', classes=classes_cnn, y=y_train_cnn)\n",
        "class_weight_dict_cnn = dict(zip(classes_cnn, class_weights_cnn))\n",
        "\n",
        "print(\"\\nPesos das classes calculados para CNN (baseado em y_train_cnn):\")\n",
        "print(class_weight_dict_cnn)\n",
        "\n",
        "\n",
        "# --- constrói o Modelo CNN ---\n",
        "embedding_dim = 128\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_filters = 128\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    Conv1D(filters=num_filters, kernel_size=filter_sizes[0], activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# --- treinar o Modelo CNN (Com class_weight) ---\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "print(\"\\nTreinando CNN com class_weight...\")\n",
        "history = model.fit(X_train_cnn, y_train_cnn,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_split=0.1,\n",
        "                    verbose=1,\n",
        "                    class_weight=class_weight_dict_cnn)\n",
        "\n",
        "# --- avaliação do Modelo CNN ---\n",
        "print(\"\\nAvaliando CNN no conjunto de teste...\")\n",
        "loss, accuracy = model.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da CNN no conjunto de teste: {accuracy:.4f}\")\n",
        "\n",
        "# classification report para CNN\n",
        "cnn_predictions_probs = model.predict(X_test_cnn)\n",
        "cnn_predictions = tf.argmax(cnn_predictions_probs, axis=1).numpy()\n",
        "\n",
        "print(\"\\nAvaliação completa da CNN:\")\n",
        "print(classification_report(y_test_cnn, cnn_predictions))\n",
        "\n",
        "# cálculo de F1-score ponderado e AUC para a CNN\n",
        "cnn_f1_weighted = f1_score(y_test_cnn, cnn_predictions, average='weighted')\n",
        "\n",
        "try:\n",
        "    cnn_auc = roc_auc_score(y_test_cnn, cnn_predictions_probs, multi_class='ovr', average='weighted')\n",
        "except Exception as e:\n",
        "    cnn_auc = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "\n",
        "print(\"F1-score ponderado (CNN):\", cnn_f1_weighted)\n",
        "print(\"AUC (ponderado, one-vs-rest) (CNN):\", cnn_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "try:\n",
        "    cnn_report = classification_report(y_test_cnn, cnn_predictions, output_dict=True)\n",
        "    results['CNN (Embedding+Seq, Class_Weight)'] = { # Alterar a chave para refletir class_weight\n",
        "        'accuracy': accuracy_score(y_test_cnn, cnn_predictions),\n",
        "        'precision (macro)': cnn_report['macro avg']['precision'],\n",
        "        'recall (macro)': cnn_report['macro avg']['recall'],\n",
        "        'f1-score (macro)': cnn_report['macro avg']['f1-score'],\n",
        "        'f1-score (weighted)': cnn_f1_weighted,\n",
        "        'auc (weighted_ovr)': cnn_auc\n",
        "    }\n",
        "except NameError:\n",
        "    print(\"Dicionário 'results' não encontrado. Os resultados da CNN (com class_weight) não foram armazenados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7Ke_1re_SY9L",
        "outputId": "33e55af3-3904-4f5f-fab2-f9c33892b4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Exemplo de sequências padded:\n",
            "[[   3  100  838  920 2932  204   20   38   84   71   17    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   3   96   78 8148 5799   37 2045   78    4  457  381    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   3  635    3   97   14    2  175  256  921   15    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   3   33    5  568    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   3   15  433  235  434  235 4598    2  185   46    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "\n",
            "Forma das sequências padded: (24783, 100)\n",
            "\n",
            "Labels originais: [2 1 0]\n",
            "Labels codificadas: [2 1 1 ... 1 1 2]\n",
            "\n",
            "Forma dos dados de treino para CNN: (19826, 100) (19826,)\n",
            "Forma dos dados de teste para CNN: (4957, 100) (4957,)\n",
            "\n",
            "Pesos das classes calculados para CNN (baseado em y_train_cnn):\n",
            "{np.int64(0): np.float64(5.797076023391813), np.int64(1): np.float64(0.4303077657681122), np.int64(2): np.float64(1.9857772435897436)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_5          │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_5          │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando CNN com class_weight...\n",
            "Epoch 1/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 50ms/step - accuracy: 0.7128 - loss: 0.7976 - val_accuracy: 0.8048 - val_loss: 0.5176\n",
            "Epoch 2/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - accuracy: 0.8367 - loss: 0.3801 - val_accuracy: 0.8522 - val_loss: 0.3964\n",
            "Epoch 3/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 50ms/step - accuracy: 0.9171 - loss: 0.1943 - val_accuracy: 0.8341 - val_loss: 0.4672\n",
            "Epoch 4/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 42ms/step - accuracy: 0.9540 - loss: 0.0988 - val_accuracy: 0.8608 - val_loss: 0.5239\n",
            "Epoch 5/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 44ms/step - accuracy: 0.9729 - loss: 0.0600 - val_accuracy: 0.8724 - val_loss: 0.5337\n",
            "Epoch 6/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.9810 - loss: 0.0460 - val_accuracy: 0.8754 - val_loss: 0.5449\n",
            "Epoch 7/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 49ms/step - accuracy: 0.9817 - loss: 0.0397 - val_accuracy: 0.8659 - val_loss: 0.5881\n",
            "Epoch 8/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 45ms/step - accuracy: 0.9861 - loss: 0.0328 - val_accuracy: 0.8588 - val_loss: 0.6383\n",
            "Epoch 9/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 49ms/step - accuracy: 0.9880 - loss: 0.0248 - val_accuracy: 0.8633 - val_loss: 0.6943\n",
            "Epoch 10/10\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 45ms/step - accuracy: 0.9884 - loss: 0.0285 - val_accuracy: 0.8437 - val_loss: 0.7430\n",
            "\n",
            "Avaliando CNN no conjunto de teste...\n",
            "\n",
            "Acurácia da CNN no conjunto de teste: 0.8299\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
            "\n",
            "Avaliação completa da CNN:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.47      0.32       290\n",
            "           1       0.94      0.86      0.90      3832\n",
            "           2       0.77      0.83      0.80       835\n",
            "\n",
            "    accuracy                           0.83      4957\n",
            "   macro avg       0.65      0.72      0.67      4957\n",
            "weighted avg       0.87      0.83      0.85      4957\n",
            "\n",
            "F1-score ponderado (CNN): 0.8455334730320679\n",
            "AUC (ponderado, one-vs-rest) (CNN): 0.9078035851417375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# --- reutiliza dados preparados para a CNN --\n",
        "# --- cálculo de Class Weights para a LSTM (baseado em y_train_cnn) ---\n",
        "classes_lstm = np.unique(y_train_cnn)\n",
        "class_weights_lstm = compute_class_weight(class_weight='balanced', classes=classes_lstm, y=y_train_cnn)\n",
        "class_weight_dict_lstm = dict(zip(classes_lstm, class_weights_lstm))\n",
        "\n",
        "print(\"\\nPesos das classes calculados para LSTM (baseado em y_train_cnn):\")\n",
        "print(class_weight_dict_lstm)\n",
        "\n",
        "# --- constrói o Modelo LSTM ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Construindo o Modelo LSTM...\")\n",
        "\n",
        "model_lstm = Sequential([\n",
        "    # camada de Embedding: reutiliza os mesmos parâmetros da CNN para consistência na base de embedding\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "\n",
        "    # camada LSTM\n",
        "    LSTM(128), #\n",
        "\n",
        "    # Camada de Dropout\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Camada Densa (Fully Connected)\n",
        "    Dense(64, activation='relu'), # Camada densa adicional\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Camada de saída (igual à CNN)\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model_lstm.compile(optimizer='adam',\n",
        "                   loss='sparse_categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "model_lstm.summary()\n",
        "\n",
        "# --- treino o Modelo LSTM (Com class_weight) ---\n",
        "epochs_lstm = 15\n",
        "batch_size_lstm = 32\n",
        "\n",
        "print(\"\\nTreinando LSTM com class_weight...\")\n",
        "history_lstm = model_lstm.fit(X_train_cnn, y_train_cnn,\n",
        "                              epochs=epochs_lstm,\n",
        "                              batch_size=batch_size_lstm,\n",
        "                              validation_split=0.1,\n",
        "                              verbose=1,\n",
        "                              class_weight=class_weight_dict_lstm)\n",
        "\n",
        "\n",
        "# --- avaliação do Modelo LSTM ---\n",
        "print(\"\\nAvaliando LSTM no conjunto de teste...\")\n",
        "loss_lstm, accuracy_lstm = model_lstm.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da LSTM no conjunto de teste: {accuracy_lstm:.4f}\")\n",
        "\n",
        "# classification report para LSTM\n",
        "lstm_predictions_probs = model_lstm.predict(X_test_cnn)\n",
        "lstm_predictions = tf.argmax(lstm_predictions_probs, axis=1).numpy()\n",
        "\n",
        "print(\"\\nAvaliação completa da LSTM:\")\n",
        "\n",
        "class_names = [str(cls) for cls in label_encoder.classes_]\n",
        "print(classification_report(y_test_cnn, lstm_predictions, target_names=class_names))\n",
        "\n",
        "# cálculo de F1-score ponderado e AUC para a LSTM\n",
        "lstm_f1_weighted = f1_score(y_test_cnn, lstm_predictions, average='weighted')\n",
        "\n",
        "try:\n",
        "    lstm_auc = roc_auc_score(y_test_cnn, lstm_predictions_probs, multi_class='ovr', average='weighted')\n",
        "except Exception as e:\n",
        "    lstm_auc = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "\n",
        "print(\"F1-score ponderado (LSTM):\", lstm_f1_weighted)\n",
        "print(\"AUC (ponderado, one-vs-rest) (LSTM):\", lstm_auc)\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "# dicionário de resultados\n",
        "try:\n",
        "    lstm_report = classification_report(y_test_cnn, lstm_predictions, output_dict=True, target_names=class_names)\n",
        "    results['LSTM (Embedding+Seq, Class_Weight)'] = {\n",
        "        'accuracy': accuracy_score(y_test_cnn, lstm_predictions),\n",
        "        'precision (macro)': lstm_report['macro avg']['precision'],\n",
        "        'recall (macro)': lstm_report['macro avg']['recall'],\n",
        "        'f1-score (macro)': lstm_report['macro avg']['f1-score'],\n",
        "        'f1-score (weighted)': lstm_f1_weighted,\n",
        "        'auc (weighted_ovr)': lstm_auc\n",
        "    }\n",
        "except NameError:\n",
        "    print(\"Dicionário 'results' não encontrado. Os resultados da LSTM (com class_weight) não foram armazenados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P9rjcg1_TMN2",
        "outputId": "f7a253d8-dcff-47f2-a40d-70ade39f97f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pesos das classes calculados para LSTM (baseado em y_train_cnn):\n",
            "{np.int64(0): np.float64(5.797076023391813), np.int64(1): np.float64(0.4303077657681122), np.int64(2): np.float64(1.9857772435897436)}\n",
            "\n",
            "==================================================\n",
            "Construindo o Modelo LSTM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_7 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando LSTM com class_weight...\n",
            "Epoch 1/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 227ms/step - accuracy: 0.3139 - loss: 1.1076 - val_accuracy: 0.0514 - val_loss: 1.1129\n",
            "Epoch 2/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 218ms/step - accuracy: 0.2049 - loss: 1.1206 - val_accuracy: 0.7796 - val_loss: 1.0928\n",
            "Epoch 3/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 208ms/step - accuracy: 0.2231 - loss: 1.1090 - val_accuracy: 0.7796 - val_loss: 1.0893\n",
            "Epoch 4/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 259ms/step - accuracy: 0.2071 - loss: 1.1169 - val_accuracy: 0.7796 - val_loss: 1.0520\n",
            "Epoch 5/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 206ms/step - accuracy: 0.3215 - loss: 1.1044 - val_accuracy: 0.1689 - val_loss: 1.0845\n",
            "Epoch 6/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 201ms/step - accuracy: 0.2629 - loss: 1.1149 - val_accuracy: 0.0514 - val_loss: 1.0991\n",
            "Epoch 7/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 204ms/step - accuracy: 0.0917 - loss: 1.1275 - val_accuracy: 0.1689 - val_loss: 1.0676\n",
            "Epoch 8/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 203ms/step - accuracy: 0.2368 - loss: 1.1034 - val_accuracy: 0.0514 - val_loss: 1.1055\n",
            "Epoch 9/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 203ms/step - accuracy: 0.2004 - loss: 1.1052 - val_accuracy: 0.7796 - val_loss: 1.0823\n",
            "Epoch 10/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 200ms/step - accuracy: 0.3372 - loss: 1.0964 - val_accuracy: 0.0514 - val_loss: 1.1069\n",
            "Epoch 11/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 200ms/step - accuracy: 0.1919 - loss: 1.1018 - val_accuracy: 0.0514 - val_loss: 1.1012\n",
            "Epoch 12/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 204ms/step - accuracy: 0.1748 - loss: 1.1059 - val_accuracy: 0.0514 - val_loss: 1.1278\n",
            "Epoch 13/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 207ms/step - accuracy: 0.1628 - loss: 1.0934 - val_accuracy: 0.0514 - val_loss: 1.1017\n",
            "Epoch 14/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 205ms/step - accuracy: 0.2217 - loss: 1.0959 - val_accuracy: 0.0514 - val_loss: 1.1007\n",
            "Epoch 15/15\n",
            "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 198ms/step - accuracy: 0.1294 - loss: 1.1025 - val_accuracy: 0.0514 - val_loss: 1.1074\n",
            "\n",
            "Avaliando LSTM no conjunto de teste...\n",
            "\n",
            "Acurácia da LSTM no conjunto de teste: 0.0585\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step\n",
            "\n",
            "Avaliação completa da LSTM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.06      1.00      0.11       290\n",
            "           1       0.00      0.00      0.00      3832\n",
            "           2       0.00      0.00      0.00       835\n",
            "\n",
            "    accuracy                           0.06      4957\n",
            "   macro avg       0.02      0.33      0.04      4957\n",
            "weighted avg       0.00      0.06      0.01      4957\n",
            "\n",
            "F1-score ponderado (LSTM): 0.006466897960155066\n",
            "AUC (ponderado, one-vs-rest) (LSTM): 0.5994398783875952\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Oversampling**"
      ],
      "metadata": {
        "id": "Upuq-C1b4Kxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como segunda abordagem para tratar o desbalanceamento entre as classes, foi utilizado o método de oversampling sintético **SMOTE (Synthetic Minority Over-sampling Technique)**. Essa técnica atua gerando novos exemplos sintéticos para as classes minoritárias com base na interpolação entre amostras reais vizinhas no espaço de atributos, equilibrando a distribuição sem simplesmente duplicar instâncias existentes.\n",
        "\n",
        "O SMOTE foi aplicado exclusivamente ao conjunto de treino, preservando a distribuição real nos dados de teste. Com isso, as três classes (0, 1 e 2) passaram a ter exatamente o mesmo número de instâncias (15.358), conforme evidenciado após o reamostramento.\n",
        "\n",
        "Esse balanceamento favorece o aprendizado de modelos como Regressão Logística, SVM, Random Forest, LightGBM e MLP, aumentando sua capacidade de generalização para exemplos das classes originalmente sub-representadas. A técnica é especialmente vantajosa por evitar overfitting comum em métodos de duplicação, ao mesmo tempo em que permite o uso direto dos classificadores."
      ],
      "metadata": {
        "id": "VKE9w2iOiTkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB2rOau4weQw",
        "outputId": "62ea4135-4849-4e68-af72-005ffb674a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nDistribuição das classes no treino (antes do balanceamento):\")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "# inicialiaz SMOTE\n",
        "# random_state para reprodutibilidade\n",
        "# sampling_strategy='auto' reamostra todas as classes minoritárias para igualar a classe majoritária\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "# aplica SMOTE apenas nos dados de TREINO\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"\\nForma dos dados de treino após SMOTE:\", X_train_resampled.shape, y_train_resampled.shape)\n",
        "print(\"\\nDistribuição das classes no treino (depois do SMOTE):\")\n",
        "print(y_train_resampled.value_counts())\n",
        "\n",
        "# uso do X_train_resampled e y_train_resampled para treinar modelos TF-IDF\n",
        "# (Logistic Regression, Naive Bayes, SVM, Random Forest, LightGBM, MLP)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T1kdoKHwzSq",
        "outputId": "1598e9e6-7cd2-479c-d265-284f3ca4ae17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Distribuição das classes no treino (antes do balanceamento):\n",
            "class\n",
            "1    15358\n",
            "2     3328\n",
            "0     1140\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Forma dos dados de treino após SMOTE: (46074, 5000) (46074,)\n",
            "\n",
            "Distribuição das classes no treino (depois do SMOTE):\n",
            "class\n",
            "0    15358\n",
            "2    15358\n",
            "1    15358\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import pandas as pd\n",
        "\n",
        "results = {}\n",
        "# considera X_train_resampled, y_train_resampled, X_test, y_test já estão definidos\n",
        "\n",
        "# 1. Regressão Logística\n",
        "print(\"Treinando Regressão Logística com SMOTE-balanced data...\")\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train_resampled, y_train_resampled)\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "\n",
        "print(\"Avaliação da Regressão Logística:\")\n",
        "print(classification_report(y_test, lr_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lr_predictions))\n",
        "\n",
        "# cálculo de F1-score ponderado\n",
        "lr_f1_weighted = f1_score(y_test, lr_predictions, average='weighted')\n",
        "print(\"F1-score ponderado:\", lr_f1_weighted)\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    lr_probabilities = lr_model.predict_proba(X_test)\n",
        "    lr_auc = roc_auc_score(y_test, lr_probabilities, multi_class='ovr', average='weighted')\n",
        "except AttributeError:\n",
        "    lr_auc = \"N/A (predict_proba não disponível ou com problema)\"\n",
        "print(\"AUC (ponderado, one-vs-rest):\", lr_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "lr_report = classification_report(y_test, lr_predictions, output_dict=True)\n",
        "results['Logistic Regression (SMOTE)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lr_predictions),\n",
        "    'precision (macro)': lr_report['macro avg']['precision'],\n",
        "    'recall (macro)': lr_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lr_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': lr_f1_weighted,\n",
        "    'auc (weighted_ovr)': lr_auc\n",
        "}\n",
        "\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 2. Naive Bayes Multinomial (geralmente bom para dados textuais esparsos como TF-IDF)\n",
        "# Naive Bayes pode ser sensível ao oversampling sintético como SMOTE,\n",
        "# mas o treino será feito com os dados reamostrados para comparação.\n",
        "\n",
        "print(\"Treinando Naive Bayes Multinomial com SMOTE-balanced data...\")\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_resampled, y_train_resampled)\n",
        "nb_predictions = nb_model.predict(X_test)\n",
        "\n",
        "print(\"Avaliação do Naive Bayes Multinomial:\")\n",
        "print(classification_report(y_test, nb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, nb_predictions))\n",
        "\n",
        "# cálculo de F1-score ponderado\n",
        "nb_f1_weighted = f1_score(y_test, nb_predictions, average='weighted')\n",
        "print(\"F1-score ponderado:\", nb_f1_weighted)\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    nb_probabilities = nb_model.predict_proba(X_test)\n",
        "    nb_auc = roc_auc_score(y_test, nb_probabilities, multi_class='ovr', average='weighted')\n",
        "except AttributeError:\n",
        "    nb_auc = \"N/A (predict_proba não disponível ou com problema)\"\n",
        "print(\"AUC (ponderado, one-vs-rest):\", nb_auc)\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "nb_report = classification_report(y_test, nb_predictions, output_dict=True)\n",
        "results['Multinomial NB (SMOTE)'] = {\n",
        "    'accuracy': accuracy_score(y_test, nb_predictions),\n",
        "    'precision (macro)': nb_report['macro avg']['precision'],\n",
        "    'recall (macro)': nb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': nb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': nb_f1_weighted,\n",
        "    'auc (weighted_ovr)': nb_auc\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 3. Support Vector Machine (SVM)\n",
        "print(\"Treinando SVM (Kernel Linear) com SMOTE-balanced data...\")\n",
        "svm_model = SVC(kernel='linear', probability=True)\n",
        "svm_model.fit(X_train_resampled, y_train_resampled)\n",
        "svm_predictions = svm_model.predict(X_test)\n",
        "\n",
        "print(\"Avaliação do SVM:\")\n",
        "print(classification_report(y_test, svm_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, svm_predictions))\n",
        "\n",
        "# cálculo de F1-score ponderado\n",
        "svm_f1_weighted = f1_score(y_test, svm_predictions, average='weighted')\n",
        "print(\"F1-score ponderado:\", svm_f1_weighted)\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    if hasattr(svm_model, 'predict_proba'):\n",
        "        svm_probabilities = svm_model.predict_proba(X_test)\n",
        "        svm_auc = roc_auc_score(y_test, svm_probabilities, multi_class='ovr', average='weighted')\n",
        "    else:\n",
        "        svm_auc = \"N/A (probability=False)\"\n",
        "except AttributeError:\n",
        "    svm_auc = \"N/A (problema ao obter predict_proba)\"\n",
        "print(\"AUC (ponderado, one-vs-rest):\", svm_auc)\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "svm_report = classification_report(y_test, svm_predictions, output_dict=True)\n",
        "results['SVM (SMOTE)'] = {\n",
        "    'accuracy': accuracy_score(y_test, svm_predictions),\n",
        "    'precision (macro)': svm_report['macro avg']['precision'],\n",
        "    'recall (macro)': svm_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': svm_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': svm_f1_weighted,\n",
        "    'auc (weighted_ovr)': svm_auc\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 4. Random Forest Classifier\n",
        "print(\"Treinando Random Forest com SMOTE-balanced data...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42) # número de árvores\n",
        "rf_model.fit(X_train_resampled, y_train_resampled)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "\n",
        "print(\"Avaliação do Random Forest:\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, rf_predictions))\n",
        "\n",
        "# cálculo de F1-score ponderado\n",
        "rf_f1_weighted = f1_score(y_test, rf_predictions, average='weighted')\n",
        "print(\"F1-score ponderado:\", rf_f1_weighted)\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    rf_probabilities = rf_model.predict_proba(X_test)\n",
        "    rf_auc = roc_auc_score(y_test, rf_probabilities, multi_class='ovr', average='weighted')\n",
        "except AttributeError:\n",
        "    rf_auc = \"N/A (predict_proba não disponível ou com problema)\"\n",
        "print(\"AUC (ponderado, one-vs-rest):\", rf_auc)\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "rf_report = classification_report(y_test, rf_predictions, output_dict=True)\n",
        "results['Random Forest (SMOTE)'] = {\n",
        "    'accuracy': accuracy_score(y_test, rf_predictions),\n",
        "    'precision (macro)': rf_report['macro avg']['precision'],\n",
        "    'recall (macro)': rf_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': rf_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': rf_f1_weighted,\n",
        "    'auc (weighted_ovr)': rf_auc\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdx1A1zaxRKA",
        "outputId": "7ead7c2d-34cc-422f-a4fc-34675dfff6b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando Regressão Logística com SMOTE-balanced data...\n",
            "Avaliação da Regressão Logística:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.56      0.36       290\n",
            "           1       0.96      0.85      0.90      3832\n",
            "           2       0.77      0.89      0.83       835\n",
            "\n",
            "    accuracy                           0.84      4957\n",
            "   macro avg       0.67      0.77      0.70      4957\n",
            "weighted avg       0.89      0.84      0.86      4957\n",
            "\n",
            "Acurácia:  0.8396207383498083\n",
            "F1-score ponderado: 0.8572308795413649\n",
            "AUC (ponderado, one-vs-rest): 0.9265650506968165\n",
            "--------------------------------------------------\n",
            "Treinando Naive Bayes Multinomial com SMOTE-balanced data...\n",
            "Avaliação do Naive Bayes Multinomial:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.57      0.32       290\n",
            "           1       0.94      0.81      0.87      3832\n",
            "           2       0.71      0.77      0.74       835\n",
            "\n",
            "    accuracy                           0.79      4957\n",
            "   macro avg       0.62      0.72      0.64      4957\n",
            "weighted avg       0.86      0.79      0.82      4957\n",
            "\n",
            "Acurácia:  0.7905991527133347\n",
            "F1-score ponderado: 0.8159579080553736\n",
            "AUC (ponderado, one-vs-rest): 0.8966891053827708\n",
            "--------------------------------------------------\n",
            "Treinando SVM (Kernel Linear) com SMOTE-balanced data...\n",
            "Avaliação do SVM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.56      0.34       290\n",
            "           1       0.96      0.84      0.90      3832\n",
            "           2       0.79      0.90      0.84       835\n",
            "\n",
            "    accuracy                           0.84      4957\n",
            "   macro avg       0.67      0.77      0.70      4957\n",
            "weighted avg       0.89      0.84      0.86      4957\n",
            "\n",
            "Acurácia:  0.8363929796247731\n",
            "F1-score ponderado: 0.8576016002046708\n",
            "AUC (ponderado, one-vs-rest): 0.9238648900443465\n",
            "--------------------------------------------------\n",
            "Treinando Random Forest com SMOTE-balanced data...\n",
            "Avaliação do Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.42      0.39       290\n",
            "           1       0.94      0.91      0.93      3832\n",
            "           2       0.80      0.86      0.83       835\n",
            "\n",
            "    accuracy                           0.88      4957\n",
            "   macro avg       0.70      0.73      0.71      4957\n",
            "weighted avg       0.88      0.88      0.88      4957\n",
            "\n",
            "Acurácia:  0.8757312890861408\n",
            "F1-score ponderado: 0.8787583077820414\n",
            "AUC (ponderado, one-vs-rest): 0.9340663778453312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Modelos Avançados com TF-IDF ---\n",
        "\n",
        "# 5. LightGBM (Gradient Boosting)\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Treinando LightGBM com SMOTE-balanced data...\")\n",
        "\n",
        "# converte matriz esparsa para o formato LightGBM\n",
        "# LightGBM funciona diretamente com matrizes esparsas do SciPy\n",
        "lgb_model = lgb.LGBMClassifier(objective='multiclass', num_class=len(df['class'].unique()), random_state=42)\n",
        "lgb_model.fit(X_train_resampled, y_train_resampled)\n",
        "lgb_predictions = lgb_model.predict(X_test)\n",
        "\n",
        "print(\"Avaliação do LightGBM:\")\n",
        "print(classification_report(y_test, lgb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lgb_predictions))\n",
        "\n",
        "# cálculo de F1-score ponderado\n",
        "lgb_f1_weighted = f1_score(y_test, lgb_predictions, average='weighted')\n",
        "print(\"F1-score ponderado:\", lgb_f1_weighted)\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    lgb_probabilities = lgb_model.predict_proba(X_test)\n",
        "    lgb_auc = roc_auc_score(y_test, lgb_probabilities, multi_class='ovr', average='weighted')\n",
        "except AttributeError:\n",
        "    lgb_auc = \"N/A (predict_proba não disponível ou com problema)\"\n",
        "print(\"AUC (ponderado, one-vs-rest):\", lgb_auc)\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "lgb_report = classification_report(y_test, lgb_predictions, output_dict=True)\n",
        "results['LightGBM (SMOTE)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lgb_predictions),\n",
        "    'precision (macro)': lgb_report['macro avg']['precision'],\n",
        "    'recall (macro)': lgb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lgb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': lgb_f1_weighted,\n",
        "    'auc (weighted_ovr)': lgb_auc\n",
        "}\n",
        "\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 6. Rede Neural Densa (MLP) com TensorFlow/Keras\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "\n",
        "print(\"Treinando Rede Neural Densa (MLP) com SMOTE-balanced data...\")\n",
        "\n",
        "# A matriz TF-IDF balanceada já está pronta (X_train_resampled, y_train_resampled)\n",
        "# O X_test e y_test originais são usados para avaliação\n",
        "\n",
        "# constrói o Modelo MLP\n",
        "mlp_model = Sequential([\n",
        "    # Flatten é necessário se a entrada não for 1D, mas a matriz TF-IDF já é 2D (samples x features)\n",
        "    Dense(256, activation='relu', input_shape=(X_train_resampled.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    # camada de saída (igual à CNN para classificação multiclasse)\n",
        "    Dense(len(df['class'].unique()), activation='softmax')\n",
        "])\n",
        "\n",
        "mlp_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "mlp_model.summary()\n",
        "\n",
        "\n",
        "epochs_mlp = 20\n",
        "batch_size_mlp = 64\n",
        "\n",
        "print(\"\\nTreinando MLP com dados SMOTE...\")\n",
        "history_mlp = mlp_model.fit(X_train_resampled, y_train_resampled,\n",
        "                          epochs=epochs_mlp,\n",
        "                          batch_size=batch_size_mlp,\n",
        "                          validation_split=0.1, # validar no conjunto de treino reamostrado\n",
        "                          verbose=1)\n",
        "\n",
        "\n",
        "# avaliação do Modelo MLP\n",
        "print(\"\\nAvaliando MLP (SMOTE) no conjunto de teste original...\")\n",
        "loss_mlp, accuracy_mlp = mlp_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da MLP (SMOTE) no conjunto de teste original: {accuracy_mlp:.4f}\")\n",
        "\n",
        "# classification report para MLP (SMOTE)\n",
        "mlp_predictions_probs = mlp_model.predict(X_test) # prediz no teste original\n",
        "mlp_predictions = tf.argmax(mlp_predictions_probs, axis=1).numpy()\n",
        "\n",
        "print(\"\\nAvaliação completa da MLP (SMOTE):\")\n",
        "print(classification_report(y_test, mlp_predictions))\n",
        "\n",
        "# cálculo de F1-score ponderado\n",
        "mlp_f1_weighted = f1_score(y_test, mlp_predictions, average='weighted')\n",
        "print(\"F1-score ponderado:\", mlp_f1_weighted)\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    mlp_auc = roc_auc_score(y_test, mlp_predictions_probs, multi_class='ovr', average='weighted')\n",
        "except Exception as e:\n",
        "    mlp_auc = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "print(\"AUC (ponderado, one-vs-rest):\", mlp_auc)\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "try:\n",
        "    mlp_report = classification_report(y_test, mlp_predictions, output_dict=True)\n",
        "    results['MLP (SMOTE)'] = {\n",
        "        'accuracy': accuracy_score(y_test, mlp_predictions),\n",
        "        'precision (macro)': mlp_report['macro avg']['precision'],\n",
        "        'recall (macro)': mlp_report['macro avg']['recall'],\n",
        "        'f1-score (macro)': mlp_report['macro avg']['f1-score'],\n",
        "        'f1-score (weighted)': mlp_f1_weighted,\n",
        "        'auc (weighted_ovr)': mlp_auc\n",
        "    }\n",
        "except NameError:\n",
        "    print(\"Dicionário 'results' não encontrado. Os resultados da MLP (SMOTE) não foram armazenados.\")\n",
        "\n",
        "\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UJr1S-jvm98y",
        "outputId": "769cab56-8e68-4335-affb-6e3c3eabdd85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Treinando LightGBM com SMOTE-balanced data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.389420 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 81147\n",
            "[LightGBM] [Info] Number of data points in the train set: 46074, number of used features: 2338\n",
            "[LightGBM] [Info] Start training from score -1.098612\n",
            "[LightGBM] [Info] Start training from score -1.098612\n",
            "[LightGBM] [Info] Start training from score -1.098612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avaliação do LightGBM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.53      0.42       290\n",
            "           1       0.96      0.88      0.92      3832\n",
            "           2       0.78      0.94      0.85       835\n",
            "\n",
            "    accuracy                           0.87      4957\n",
            "   macro avg       0.70      0.79      0.73      4957\n",
            "weighted avg       0.90      0.87      0.88      4957\n",
            "\n",
            "Acurácia:  0.8723017954407908\n",
            "F1-score ponderado: 0.8811002395679967\n",
            "AUC (ponderado, one-vs-rest): 0.9388846494445826\n",
            "--------------------------------------------------\n",
            "Treinando Rede Neural Densa (MLP) com SMOTE-balanced data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_14\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,280,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_35 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_23 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,313,539\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,539</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,539\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,539</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando MLP com dados SMOTE...\n",
            "Epoch 1/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.7330 - loss: 0.6325 - val_accuracy: 0.9811 - val_loss: 0.1209\n",
            "Epoch 2/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.9538 - loss: 0.1480 - val_accuracy: 0.9848 - val_loss: 0.0877\n",
            "Epoch 3/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.9767 - loss: 0.0725 - val_accuracy: 0.9939 - val_loss: 0.0342\n",
            "Epoch 4/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.9847 - loss: 0.0503 - val_accuracy: 0.9974 - val_loss: 0.0293\n",
            "Epoch 5/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 32ms/step - accuracy: 0.9871 - loss: 0.0400 - val_accuracy: 0.9948 - val_loss: 0.0242\n",
            "Epoch 6/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.9898 - loss: 0.0318 - val_accuracy: 0.9976 - val_loss: 0.0211\n",
            "Epoch 7/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.9910 - loss: 0.0289 - val_accuracy: 0.9813 - val_loss: 0.0373\n",
            "Epoch 8/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 30ms/step - accuracy: 0.9908 - loss: 0.0273 - val_accuracy: 0.9976 - val_loss: 0.0170\n",
            "Epoch 9/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.9915 - loss: 0.0250 - val_accuracy: 0.9800 - val_loss: 0.0343\n",
            "Epoch 10/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.9930 - loss: 0.0225 - val_accuracy: 0.9987 - val_loss: 0.0191\n",
            "Epoch 11/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.9925 - loss: 0.0218 - val_accuracy: 0.9601 - val_loss: 0.0748\n",
            "Epoch 12/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.9946 - loss: 0.0172 - val_accuracy: 0.9985 - val_loss: 0.0116\n",
            "Epoch 13/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9934 - loss: 0.0208 - val_accuracy: 0.9729 - val_loss: 0.0478\n",
            "Epoch 14/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.9936 - loss: 0.0185 - val_accuracy: 0.9970 - val_loss: 0.0085\n",
            "Epoch 15/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9944 - loss: 0.0184 - val_accuracy: 0.9807 - val_loss: 0.0336\n",
            "Epoch 16/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.9942 - loss: 0.0172 - val_accuracy: 0.9640 - val_loss: 0.0694\n",
            "Epoch 17/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.9942 - loss: 0.0171 - val_accuracy: 0.9974 - val_loss: 0.0153\n",
            "Epoch 18/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.9944 - loss: 0.0180 - val_accuracy: 0.9807 - val_loss: 0.0356\n",
            "Epoch 19/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.9952 - loss: 0.0151 - val_accuracy: 0.9983 - val_loss: 0.0152\n",
            "Epoch 20/20\n",
            "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9943 - loss: 0.0172 - val_accuracy: 0.9931 - val_loss: 0.0266\n",
            "\n",
            "Avaliando MLP (SMOTE) no conjunto de teste original...\n",
            "\n",
            "Acurácia da MLP (SMOTE) no conjunto de teste original: 0.8570\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "\n",
            "Avaliação completa da MLP (SMOTE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.21      0.27       290\n",
            "           1       0.89      0.95      0.92      3832\n",
            "           2       0.80      0.66      0.72       835\n",
            "\n",
            "    accuracy                           0.86      4957\n",
            "   macro avg       0.69      0.61      0.64      4957\n",
            "weighted avg       0.84      0.86      0.85      4957\n",
            "\n",
            "F1-score ponderado: 0.8456948200275326\n",
            "AUC (ponderado, one-vs-rest): 0.8814731568142896\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# --- Pré-processamento para CNN ---\n",
        "\n",
        "# O TfidfVectorizer não é ideal para CNNs, precisamos de uma representação baseada em sequências/embeddings.\n",
        "\n",
        "# 1. Tokenização para CNN (Sequências de Inteiros)\n",
        "max_words = 10000\n",
        "max_sequence_length = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df['processed_text'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df['processed_text'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "print(\"\\nExemplo de sequências padded:\")\n",
        "print(padded_sequences[:5])\n",
        "print(\"\\nForma das sequências padded:\", padded_sequences.shape)\n",
        "\n",
        "# --- prepara labels para CNN ---\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_y = label_encoder.fit_transform(df['class'])\n",
        "\n",
        "print(\"\\nLabels originais:\", df['class'].unique())\n",
        "print(\"Labels codificadas:\", encoded_y)\n",
        "\n",
        "# --- divide dados para CNN (usando as sequências e labels codificadas) ---\n",
        "X_cnn = padded_sequences\n",
        "y_cnn = encoded_y\n",
        "\n",
        "# divisão original em treino e teste para a CNN\n",
        "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(X_cnn, y_cnn, test_size=0.20, random_state=42)\n",
        "\n",
        "print(\"\\nForma dos dados de treino para CNN (antes do SMOTE):\", X_train_cnn.shape, y_train_cnn.shape)\n",
        "print(\"Forma dos dados de teste para CNN:\", X_test_cnn.shape, y_test_cnn.shape)\n",
        "print(\"\\nDistribuição das classes no treino da CNN (antes do balanceamento):\")\n",
        "print(pd.Series(y_train_cnn).value_counts())\n",
        "\n",
        "\n",
        "# --- aplica SMOTE nos dados de TREINO da CNN ---\n",
        "try:\n",
        "    smote_cnn = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "    X_train_cnn_resampled_smote, y_train_cnn_resampled_smote = smote_cnn.fit_resample(X_train_cnn, y_train_cnn)\n",
        "\n",
        "    print(\"\\nForma dos dados de treino da CNN após SMOTE:\", X_train_cnn_resampled_smote.shape, y_train_cnn_resampled_smote.shape)\n",
        "    print(\"\\nDistribuição das classes no treino da CNN (depois do SMOTE):\")\n",
        "    print(pd.Series(y_train_cnn_resampled_smote).value_counts())\n",
        "\n",
        "except NameError:\n",
        "    print(\"Erro: imblearn.over_sampling.SMOTE não encontrado. Certifique-se de que imbalanced-learn está instalado e SMOTE foi importado.\")\n",
        "    raise # relança erro pra interromper execução sem SMOTE\n",
        "\n",
        "\n",
        "# --- contrói o Modelo CNN ---\n",
        "embedding_dim = 128\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_filters = 128\n",
        "\n",
        "model_cnn_smote = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    Conv1D(filters=num_filters, kernel_size=filter_sizes[0], activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model_cnn_smote.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_cnn_smote.summary()\n",
        "\n",
        "# --- treino do Modelo CNN (com dados Oversampled SMOTE) ---\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "print(\"\\nTreinando CNN com dados Oversampled (SMOTE)...\")\n",
        "history_cnn_smote = model_cnn_smote.fit(X_train_cnn_resampled_smote, y_train_cnn_resampled_smote,\n",
        "                                        epochs=epochs,\n",
        "                                        batch_size=batch_size,\n",
        "                                        validation_split=0.1,\n",
        "                                        verbose=1)\n",
        "\n",
        "\n",
        "# --- avaliação do modelo\n",
        "print(\"\\nAvaliando CNN (SMOTE) no conjunto de teste original...\")\n",
        "loss_cnn_smote, accuracy_cnn_smote = model_cnn_smote.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da CNN (SMOTE) no conjunto de teste original: {accuracy_cnn_smote:.4f}\")\n",
        "\n",
        "# classification report para CNN (SMOTE)\n",
        "cnn_smote_predictions_probs = model_cnn_smote.predict(X_test_cnn)\n",
        "cnn_smote_predictions = tf.argmax(cnn_smote_predictions_probs, axis=1).numpy()\n",
        "\n",
        "print(\"\\nAvaliação completa da CNN (SMOTE):\")\n",
        "print(classification_report(y_test_cnn, cnn_smote_predictions))\n",
        "\n",
        "# cálculo do F1-score ponderado e AUC para a CNN (SMOTE)\n",
        "cnn_smote_f1_weighted = f1_score(y_test_cnn, cnn_smote_predictions, average='weighted')\n",
        "\n",
        "try:\n",
        "    cnn_smote_auc = roc_auc_score(y_test_cnn, cnn_smote_predictions_probs, multi_class='ovr', average='weighted')\n",
        "except Exception as e:\n",
        "    cnn_smote_auc = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "\n",
        "print(\"F1-score ponderado (CNN SMOTE):\", cnn_smote_f1_weighted)\n",
        "print(\"AUC (ponderado, one-vs-rest) (CNN SMOTE):\", cnn_smote_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "try:\n",
        "    cnn_smote_report = classification_report(y_test_cnn, cnn_smote_predictions, output_dict=True)\n",
        "    results['CNN (Embedding+Seq, SMOTE)'] = {\n",
        "        'accuracy': accuracy_score(y_test_cnn, cnn_smote_predictions),\n",
        "        'precision (macro)': cnn_smote_report['macro avg']['precision'],\n",
        "        'recall (macro)': cnn_smote_report['macro avg']['recall'],\n",
        "        'f1-score (macro)': cnn_smote_report['macro avg']['f1-score'],\n",
        "        'f1-score (weighted)': cnn_smote_f1_weighted,\n",
        "        'auc (weighted_ovr)': cnn_smote_auc\n",
        "    }\n",
        "except NameError:\n",
        "    print(\"Dicionário 'results' não encontrado. Os resultados da CNN (SMOTE) não foram armazenados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UJU8f-Wj5M_C",
        "outputId": "10b38cb9-7688-4512-bb08-5d83b849677c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Exemplo de sequências padded:\n",
            "[[   3  100  838  920 2932  204   20   38   84   71   17    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   3   96   78 8148 5799   37 2045   78    4  457  381    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   3  635    3   97   14    2  175  256  921   15    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   3   33    5  568    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   3   15  433  235  434  235 4598    2  185   46    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "\n",
            "Forma das sequências padded: (24783, 100)\n",
            "\n",
            "Labels originais: [2 1 0]\n",
            "Labels codificadas: [2 1 1 ... 1 1 2]\n",
            "\n",
            "Forma dos dados de treino para CNN (antes do SMOTE): (19826, 100) (19826,)\n",
            "Forma dos dados de teste para CNN: (4957, 100) (4957,)\n",
            "\n",
            "Distribuição das classes no treino da CNN (antes do balanceamento):\n",
            "1    15358\n",
            "2     3328\n",
            "0     1140\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Forma dos dados de treino da CNN após SMOTE: (46074, 100) (46074,)\n",
            "\n",
            "Distribuição das classes no treino da CNN (depois do SMOTE):\n",
            "0    15358\n",
            "2    15358\n",
            "1    15358\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_15\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_15\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_6          │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_6          │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando CNN com dados Oversampled (SMOTE)...\n",
            "Epoch 1/10\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 43ms/step - accuracy: 0.6130 - loss: 0.8303 - val_accuracy: 0.3444 - val_loss: 1.1217\n",
            "Epoch 2/10\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 43ms/step - accuracy: 0.7785 - loss: 0.5448 - val_accuracy: 0.3333 - val_loss: 1.2339\n",
            "Epoch 3/10\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 43ms/step - accuracy: 0.8668 - loss: 0.3463 - val_accuracy: 0.3576 - val_loss: 1.3700\n",
            "Epoch 4/10\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 43ms/step - accuracy: 0.9287 - loss: 0.1985 - val_accuracy: 0.3885 - val_loss: 1.8538\n",
            "Epoch 5/10\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 43ms/step - accuracy: 0.9598 - loss: 0.1212 - val_accuracy: 0.4377 - val_loss: 1.9127\n",
            "Epoch 6/10\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 44ms/step - accuracy: 0.9713 - loss: 0.0820 - val_accuracy: 0.4015 - val_loss: 2.9573\n",
            "Epoch 7/10\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 43ms/step - accuracy: 0.9790 - loss: 0.0613 - val_accuracy: 0.4974 - val_loss: 2.5173\n",
            "Epoch 8/10\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 42ms/step - accuracy: 0.9833 - loss: 0.0506 - val_accuracy: 0.4625 - val_loss: 3.0320\n",
            "Epoch 9/10\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 43ms/step - accuracy: 0.9863 - loss: 0.0418 - val_accuracy: 0.4427 - val_loss: 3.7720\n",
            "Epoch 10/10\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 43ms/step - accuracy: 0.9860 - loss: 0.0405 - val_accuracy: 0.3655 - val_loss: 4.6560\n",
            "\n",
            "Avaliando CNN (SMOTE) no conjunto de teste original...\n",
            "\n",
            "Acurácia da CNN (SMOTE) no conjunto de teste original: 0.7279\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
            "\n",
            "Avaliação completa da CNN (SMOTE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.13      0.44      0.21       290\n",
            "           1       0.93      0.77      0.85      3832\n",
            "           2       0.62      0.62      0.62       835\n",
            "\n",
            "    accuracy                           0.73      4957\n",
            "   macro avg       0.56      0.61      0.56      4957\n",
            "weighted avg       0.83      0.73      0.77      4957\n",
            "\n",
            "F1-score ponderado (CNN SMOTE): 0.770681803784126\n",
            "AUC (ponderado, one-vs-rest) (CNN SMOTE): 0.8580265907375341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- reutiliza dados preparados para a CNN ---\n",
        "\n",
        "# --- Aplicar SMOTE nos dados de TREINO da LSTM (que são os mesmos da CNN) ---\n",
        "\n",
        "print(\"\\nDistribuição das classes no treino da LSTM (antes do balanceamento):\")\n",
        "print(pd.Series(y_train_cnn).value_counts())\n",
        "\n",
        "try:\n",
        "    smote_lstm = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "    X_train_lstm_resampled_smote, y_train_lstm_resampled_smote = smote_lstm.fit_resample(X_train_cnn, y_train_cnn)\n",
        "\n",
        "    print(\"\\nForma dos dados de treino da LSTM após SMOTE:\", X_train_lstm_resampled_smote.shape, y_train_lstm_resampled_smote.shape)\n",
        "    print(\"\\nDistribuição das classes no treino da LSTM (depois do SMOTE):\")\n",
        "    print(pd.Series(y_train_lstm_resampled_smote).value_counts())\n",
        "\n",
        "except NameError:\n",
        "    print(\"Erro: imblearn.over_sampling.SMOTE não encontrado. Certifique-se de que imbalanced-learn está instalado e SMOTE foi importado.\")\n",
        "    raise # relança o erro para interromper execução sem SMOTE\n",
        "\n",
        "\n",
        "# --- constrói o Modelo LSTM ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Construindo o Modelo LSTM...\")\n",
        "\n",
        "model_lstm_smote = Sequential([\n",
        "    # camada de Embedding: reutiliza os mesmos parâmetros da CNN para consistência na base de embedding\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "\n",
        "    # camada LSTM\n",
        "    LSTM(128),\n",
        "\n",
        "    # camada de Dropout\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # camada Densa (Fully Connected)\n",
        "    Dense(64, activation='relu'), # Camada densa adicional\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # camada de saída (igual à CNN)\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model_lstm_smote.compile(optimizer='adam',\n",
        "                   loss='sparse_categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "model_lstm_smote.summary()\n",
        "\n",
        "# --- treino do Modelo LSTM (com dados Oversampled SMOTE) ---\n",
        "epochs_lstm = 15\n",
        "batch_size_lstm = 32\n",
        "\n",
        "print(\"\\nTreinando LSTM com dados Oversampled (SMOTE)...\")\n",
        "history_lstm_smote = model_lstm_smote.fit(X_train_lstm_resampled_smote, y_train_lstm_resampled_smote,\n",
        "                                          epochs=epochs_lstm,\n",
        "                                          batch_size=batch_size_lstm,\n",
        "                                          validation_split=0.1, # validação nos dados reamostrados\n",
        "                                          verbose=1)\n",
        "\n",
        "\n",
        "# --- avaliação do Modelo LSTM ---\n",
        "print(\"\\nAvaliando LSTM (SMOTE) no conjunto de teste original...\")\n",
        "loss_lstm_smote, accuracy_lstm_smote = model_lstm_smote.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
        "print(f\"\\nAcurácia da LSTM (SMOTE) no conjunto de teste original: {accuracy_lstm_smote:.4f}\")\n",
        "\n",
        "# classification report para LSTM (SMOTE)\n",
        "lstm_smote_predictions_probs = model_lstm_smote.predict(X_test_cnn)\n",
        "lstm_smote_predictions = tf.argmax(lstm_smote_predictions_probs, axis=1).numpy()\n",
        "print(\"\\nAvaliação completa da LSTM (SMOTE):\")\n",
        "\n",
        "\n",
        "class_names = [str(cls) for cls in label_encoder.classes_]\n",
        "print(classification_report(y_test_cnn, lstm_smote_predictions, target_names=class_names))\n",
        "\n",
        "# cálculo do F1-score ponderado e AUC para a LSTM (SMOTE)\n",
        "lstm_smote_f1_weighted = f1_score(y_test_cnn, lstm_smote_predictions, average='weighted')\n",
        "\n",
        "try:\n",
        "    lstm_smote_auc = roc_auc_score(y_test_cnn, lstm_smote_predictions_probs, multi_class='ovr', average='weighted')\n",
        "except Exception as e:\n",
        "    lstm_smote_auc = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "\n",
        "print(\"F1-score ponderado (LSTM SMOTE):\", lstm_smote_f1_weighted)\n",
        "print(\"AUC (ponderado, one-vs-rest) (LSTM SMOTE):\", lstm_smote_auc)\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "# dicionário de resultados\n",
        "try:\n",
        "    lstm_smote_report = classification_report(y_test_cnn, lstm_smote_predictions, output_dict=True, target_names=class_names)\n",
        "    results['LSTM (Embedding+Seq, SMOTE)'] = {\n",
        "        'accuracy': accuracy_score(y_test_cnn, lstm_smote_predictions),\n",
        "        'precision (macro)': lstm_smote_report['macro avg']['precision'],\n",
        "        'recall (macro)': lstm_smote_report['macro avg']['recall'],\n",
        "        'f1-score (macro)': lstm_smote_report['macro avg']['f1-score'],\n",
        "        'f1-score (weighted)': lstm_smote_f1_weighted,\n",
        "        'auc (weighted_ovr)': lstm_smote_auc\n",
        "    }\n",
        "except NameError:\n",
        "    print(\"Dicionário 'results' não encontrado. Os resultados da LSTM (SMOTE) não foram armazenados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UQZapM397Ecj",
        "outputId": "ed6dab51-1b43-4ee1-c9ff-24e64becf0a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Distribuição das classes no treino da LSTM (antes do balanceamento):\n",
            "1    15358\n",
            "2     3328\n",
            "0     1140\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Forma dos dados de treino da LSTM após SMOTE: (46074, 100) (46074,)\n",
            "\n",
            "Distribuição das classes no treino da LSTM (depois do SMOTE):\n",
            "0    15358\n",
            "2    15358\n",
            "1    15358\n",
            "Name: count, dtype: int64\n",
            "\n",
            "==================================================\n",
            "Construindo o Modelo LSTM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_16\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_16\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_9 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando LSTM com dados Oversampled (SMOTE)...\n",
            "Epoch 1/15\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 198ms/step - accuracy: 0.3681 - loss: 1.0895 - val_accuracy: 0.0000e+00 - val_loss: 1.2968\n",
            "Epoch 2/15\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 200ms/step - accuracy: 0.3665 - loss: 1.0865 - val_accuracy: 0.0000e+00 - val_loss: 1.3008\n",
            "Epoch 3/15\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 199ms/step - accuracy: 0.3657 - loss: 1.0856 - val_accuracy: 0.0000e+00 - val_loss: 1.3554\n",
            "Epoch 4/15\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 201ms/step - accuracy: 0.3693 - loss: 1.0863 - val_accuracy: 0.0000e+00 - val_loss: 1.3699\n",
            "Epoch 5/15\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 199ms/step - accuracy: 0.3682 - loss: 1.0842 - val_accuracy: 0.0000e+00 - val_loss: 1.3306\n",
            "Epoch 6/15\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 201ms/step - accuracy: 0.3730 - loss: 1.0857 - val_accuracy: 0.0000e+00 - val_loss: 1.3176\n",
            "Epoch 7/15\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 200ms/step - accuracy: 0.3663 - loss: 1.0867 - val_accuracy: 0.0000e+00 - val_loss: 1.3694\n",
            "Epoch 8/15\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 203ms/step - accuracy: 0.3694 - loss: 1.0855 - val_accuracy: 0.0000e+00 - val_loss: 1.3401\n",
            "Epoch 9/15\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 201ms/step - accuracy: 0.3704 - loss: 1.0852 - val_accuracy: 0.0000e+00 - val_loss: 1.3324\n",
            "Epoch 10/15\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 219ms/step - accuracy: 0.3704 - loss: 1.0866 - val_accuracy: 0.0000e+00 - val_loss: 1.3660\n",
            "Epoch 11/15\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 220ms/step - accuracy: 0.3621 - loss: 1.0856 - val_accuracy: 0.0000e+00 - val_loss: 1.3579\n",
            "Epoch 12/15\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 203ms/step - accuracy: 0.3683 - loss: 1.0860 - val_accuracy: 0.0000e+00 - val_loss: 1.3452\n",
            "Epoch 13/15\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 203ms/step - accuracy: 0.3681 - loss: 1.0860 - val_accuracy: 0.0000e+00 - val_loss: 1.3442\n",
            "Epoch 14/15\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 201ms/step - accuracy: 0.3686 - loss: 1.0857 - val_accuracy: 0.0000e+00 - val_loss: 1.3430\n",
            "Epoch 15/15\n",
            "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 201ms/step - accuracy: 0.3719 - loss: 1.0848 - val_accuracy: 0.0000e+00 - val_loss: 1.3499\n",
            "\n",
            "Avaliando LSTM (SMOTE) no conjunto de teste original...\n",
            "\n",
            "Acurácia da LSTM (SMOTE) no conjunto de teste original: 0.7730\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step\n",
            "\n",
            "Avaliação completa da LSTM (SMOTE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       290\n",
            "           1       0.77      1.00      0.87      3832\n",
            "           2       0.00      0.00      0.00       835\n",
            "\n",
            "    accuracy                           0.77      4957\n",
            "   macro avg       0.26      0.33      0.29      4957\n",
            "weighted avg       0.60      0.77      0.67      4957\n",
            "\n",
            "F1-score ponderado (LSTM SMOTE): 0.6740973395206054\n",
            "AUC (ponderado, one-vs-rest) (LSTM SMOTE): 0.5000335249411981\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Undersampling**"
      ],
      "metadata": {
        "id": "wL8N2JdI4U7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como terceira estratégia de balanceamento, foi empregado o método de undersampling aleatório, utilizando a técnica **RandomUnderSampler** da biblioteca `imbalanced-learn`. Diferente do SMOTE, que gera novas amostras para as classes minoritárias, o undersampling reduz o número de instâncias da classe majoritária, removendo exemplos aleatórios até que todas as classes tenham o mesmo número de ocorrências.\n",
        "\n",
        "No experimento, o balanceamento foi aplicado apenas sobre o conjunto de treino, **igualando as classes 0, 1 e 2 com 1.140 exemplos cada** — valor correspondente à classe originalmente menos representada. Essa abordagem visa mitigar o viés dos modelos em favor da classe dominante, porém com o custo potencial de perda de informação relevante.\n",
        "Apesar disso, ela permite a avaliação do impacto da redução de dados no desempenho de algoritmos como **Regressão Logística, Naive Bayes, SVM, Random Forest, LightGBM e redes neurais densas**, todos treinados com a base reamostrada, possibilitando uma comparação direta com os resultados obtidos nas estratégias anteriores de balanceamento."
      ],
      "metadata": {
        "id": "OiW4KiR4zFml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import lightgbm as lgb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Forma dos dados de treino (X_train, y_train) antes do undersampling:\", X_train.shape, y_train.shape)\n",
        "print(\"Forma dos dados de teste (X_test, y_test):\", X_test.shape, y_test.shape)\n",
        "print(\"\\nDistribuição das classes no treino (antes do undersampling):\")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "\n",
        "# --- aplica undersampling aleatório nos dados de TREINO ---\n",
        "try:\n",
        "    # inicializa RandomUnderSampler\n",
        "    # sampling_strategy='auto' remove instâncias da classe majoritária para igualar a classe minoritária\n",
        "    undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "    X_train_resampled_under, y_train_resampled_under = undersampler.fit_resample(X_train, y_train)\n",
        "\n",
        "    print(\"\\nForma dos dados de treino após Undersampling:\", X_train_resampled_under.shape, y_train_resampled_under.shape)\n",
        "    print(\"\\nDistribuição das classes no treino (depois do Undersampling):\")\n",
        "    print(pd.Series(y_train_resampled_under).value_counts())\n",
        "\n",
        "except NameError:\n",
        "    print(\"Erro: imblearn.under_sampling.RandomUnderSampler não encontrado. Certifique-se de que imbalanced-learn está instalado e RandomUnderSampler foi importado.\").\n",
        "    raise # relança o erro"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytQOmwm24MEF",
        "outputId": "06252f9e-4535-4bcd-9633-aaafd8761931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma dos dados de treino (X_train, y_train) antes do undersampling: (19826, 5000) (19826,)\n",
            "Forma dos dados de teste (X_test, y_test): (4957, 5000) (4957,)\n",
            "\n",
            "Distribuição das classes no treino (antes do undersampling):\n",
            "class\n",
            "1    15358\n",
            "2     3328\n",
            "0     1140\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Forma dos dados de treino após Undersampling: (3420, 5000) (3420,)\n",
            "\n",
            "Distribuição das classes no treino (depois do Undersampling):\n",
            "class\n",
            "0    1140\n",
            "1    1140\n",
            "2    1140\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inicializa dicionário\n",
        "# results = {}\n",
        "\n",
        "# 1. Regressão Logística\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Treinando Regressão Logística com Undersampled data...\")\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train_resampled_under, y_train_resampled_under)\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "\n",
        "print(\"Avaliação da Regressão Logística:\")\n",
        "print(classification_report(y_test, lr_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lr_predictions))\n",
        "\n",
        "# cálculo de F1-score ponderado\n",
        "lr_f1_weighted = f1_score(y_test, lr_predictions, average='weighted')\n",
        "print(\"F1-score ponderado:\", lr_f1_weighted)\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    lr_probabilities = lr_model.predict_proba(X_test)\n",
        "    lr_auc = roc_auc_score(y_test, lr_probabilities, multi_class='ovr', average='weighted')\n",
        "except AttributeError:\n",
        "    lr_auc = \"N/A (predict_proba não disponível ou com problema)\"\n",
        "print(\"AUC (ponderado, one-vs-rest):\", lr_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "lr_report = classification_report(y_test, lr_predictions, output_dict=True)\n",
        "results['Logistic Regression (Undersampling)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lr_predictions),\n",
        "    'precision (macro)': lr_report['macro avg']['precision'],\n",
        "    'recall (macro)': lr_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lr_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': lr_f1_weighted,\n",
        "    'auc (weighted_ovr)': lr_auc\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 2. Naive Bayes Multinomial\n",
        "print(\"Treinando Naive Bayes Multinomial com Undersampled data...\")\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_resampled_under, y_train_resampled_under)\n",
        "nb_predictions = nb_model.predict(X_test)\n",
        "\n",
        "print(\"Avaliação do Naive Bayes Multinomial:\")\n",
        "print(classification_report(y_test, nb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, nb_predictions))\n",
        "\n",
        "# cálculo de F1-score ponderado\n",
        "nb_f1_weighted = f1_score(y_test, nb_predictions, average='weighted')\n",
        "print(\"F1-score ponderado:\", nb_f1_weighted)\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    nb_probabilities = nb_model.predict_proba(X_test)\n",
        "    nb_auc = roc_auc_score(y_test, nb_probabilities, multi_class='ovr', average='weighted')\n",
        "except AttributeError:\n",
        "    nb_auc = \"N/A (predict_proba não disponível ou com problema)\"\n",
        "print(\"AUC (ponderado, one-vs-rest):\", nb_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "nb_report = classification_report(y_test, nb_predictions, output_dict=True)\n",
        "results['Multinomial NB (Undersampling)'] = {\n",
        "    'accuracy': accuracy_score(y_test, nb_predictions),\n",
        "    'precision (macro)': nb_report['macro avg']['precision'],\n",
        "    'recall (macro)': nb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': nb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': nb_f1_weighted,\n",
        "    'auc (weighted_ovr)': nb_auc\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 3. Support Vector Machine (SVM)\n",
        "print(\"Treinando SVM (Kernel Linear) com Undersampled data...\")\n",
        "svm_model = SVC(kernel='linear', probability=True)\n",
        "svm_model.fit(X_train_resampled_under, y_train_resampled_under)\n",
        "svm_predictions = svm_model.predict(X_test)\n",
        "\n",
        "print(\"Avaliação do SVM:\")\n",
        "print(classification_report(y_test, svm_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, svm_predictions))\n",
        "\n",
        "# cálculo de F1-score ponderado\n",
        "svm_f1_weighted = f1_score(y_test, svm_predictions, average='weighted')\n",
        "print(\"F1-score ponderado:\", svm_f1_weighted)\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    if hasattr(svm_model, 'predict_proba'):\n",
        "        svm_probabilities = svm_model.predict_proba(X_test)\n",
        "        svm_auc = roc_auc_score(y_test, svm_probabilities, multi_class='ovr', average='weighted')\n",
        "    else:\n",
        "        svm_auc = \"N/A (probability=False)\"\n",
        "except AttributeError:\n",
        "    svm_auc = \"N/A (problema ao obter predict_proba)\"\n",
        "print(\"AUC (ponderado, one-vs-rest):\", svm_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "svm_report = classification_report(y_test, svm_predictions, output_dict=True)\n",
        "results['SVM (Undersampling)'] = {\n",
        "    'accuracy': accuracy_score(y_test, svm_predictions),\n",
        "    'precision (macro)': svm_report['macro avg']['precision'],\n",
        "    'recall (macro)': svm_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': svm_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': svm_f1_weighted,\n",
        "    'auc (weighted_ovr)': svm_auc\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 4. Random Forest Classifier\n",
        "print(\"Treinando Random Forest com Undersampled data...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42) # número de árvores\n",
        "rf_model.fit(X_train_resampled_under, y_train_resampled_under)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "\n",
        "print(\"Avaliação do Random Forest:\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, rf_predictions))\n",
        "\n",
        "# cálculo de F1-score ponderado\n",
        "rf_f1_weighted = f1_score(y_test, rf_predictions, average='weighted')\n",
        "print(\"F1-score ponderado:\", rf_f1_weighted)\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    rf_probabilities = rf_model.predict_proba(X_test)\n",
        "    rf_auc = roc_auc_score(y_test, rf_probabilities, multi_class='ovr', average='weighted')\n",
        "except AttributeError:\n",
        "    rf_auc = \"N/A (predict_proba não disponível ou com problema)\"\n",
        "print(\"AUC (ponderado, one-vs-rest):\", rf_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "rf_report = classification_report(y_test, rf_predictions, output_dict=True)\n",
        "results['Random Forest (Undersampling)'] = {\n",
        "    'accuracy': accuracy_score(y_test, rf_predictions),\n",
        "    'precision (macro)': rf_report['macro avg']['precision'],\n",
        "    'recall (macro)': rf_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': rf_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': rf_f1_weighted,\n",
        "    'auc (weighted_ovr)': rf_auc\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lGaqsUz4Tmm",
        "outputId": "98921818-53de-4445-a75f-beac86ee5cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Treinando Regressão Logística com Undersampled data...\n",
            "Avaliação da Regressão Logística:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.64      0.35       290\n",
            "           1       0.97      0.78      0.86      3832\n",
            "           2       0.69      0.93      0.79       835\n",
            "\n",
            "    accuracy                           0.80      4957\n",
            "   macro avg       0.64      0.78      0.67      4957\n",
            "weighted avg       0.88      0.80      0.82      4957\n",
            "\n",
            "Acurácia:  0.795239055880573\n",
            "F1-score ponderado: 0.8222035141956675\n",
            "AUC (ponderado, one-vs-rest): 0.9176774330625891\n",
            "--------------------------------------------------\n",
            "Treinando Naive Bayes Multinomial com Undersampled data...\n",
            "Avaliação do Naive Bayes Multinomial:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.18      0.72      0.29       290\n",
            "           1       0.94      0.74      0.83      3832\n",
            "           2       0.77      0.71      0.74       835\n",
            "\n",
            "    accuracy                           0.73      4957\n",
            "   macro avg       0.63      0.72      0.62      4957\n",
            "weighted avg       0.87      0.73      0.78      4957\n",
            "\n",
            "Acurácia:  0.7331047004236433\n",
            "F1-score ponderado: 0.7809946752502083\n",
            "AUC (ponderado, one-vs-rest): 0.8886472051823114\n",
            "--------------------------------------------------\n",
            "Treinando SVM (Kernel Linear) com Undersampled data...\n",
            "Avaliação do SVM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.68      0.34       290\n",
            "           1       0.97      0.77      0.86      3832\n",
            "           2       0.71      0.91      0.80       835\n",
            "\n",
            "    accuracy                           0.79      4957\n",
            "   macro avg       0.64      0.79      0.67      4957\n",
            "weighted avg       0.89      0.79      0.82      4957\n",
            "\n",
            "Acurácia:  0.7905991527133347\n",
            "F1-score ponderado: 0.8211528148182387\n",
            "AUC (ponderado, one-vs-rest): 0.925869680825291\n",
            "--------------------------------------------------\n",
            "Treinando Random Forest com Undersampled data...\n",
            "Avaliação do Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.63      0.36       290\n",
            "           1       0.97      0.81      0.88      3832\n",
            "           2       0.73      0.93      0.82       835\n",
            "\n",
            "    accuracy                           0.82      4957\n",
            "   macro avg       0.65      0.79      0.69      4957\n",
            "weighted avg       0.89      0.82      0.84      4957\n",
            "\n",
            "Acurácia:  0.815816017752673\n",
            "F1-score ponderado: 0.8403808861711874\n",
            "AUC (ponderado, one-vs-rest): 0.9305891700199098\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. LightGBM (Gradient Boosting)\n",
        "\n",
        "print(\"Treinando LightGBM com Undersampled data...\")\n",
        "\n",
        "lgb_model = lgb.LGBMClassifier(objective='multiclass', num_class=len(df['class'].unique()), random_state=42)\n",
        "lgb_model.fit(X_train_resampled_under, y_train_resampled_under)\n",
        "lgb_predictions = lgb_model.predict(X_test)\n",
        "\n",
        "print(\"Avaliação do LightGBM:\")\n",
        "print(classification_report(y_test, lgb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lgb_predictions))\n",
        "\n",
        "# cálculo de F1-score ponderado\n",
        "lgb_f1_weighted = f1_score(y_test, lgb_predictions, average='weighted')\n",
        "print(\"F1-score ponderado:\", lgb_f1_weighted)\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    lgb_probabilities = lgb_model.predict_proba(X_test)\n",
        "    lgb_auc = roc_auc_score(y_test, lgb_probabilities, multi_class='ovr', average='weighted')\n",
        "except AttributeError:\n",
        "    lgb_auc = \"N/A (predict_proba não disponível ou com problema)\"\n",
        "print(\"AUC (ponderado, one-vs-rest):\", lgb_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "lgb_report = classification_report(y_test, lgb_predictions, output_dict=True)\n",
        "results['LightGBM (Undersampling)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lgb_predictions),\n",
        "    'precision (macro)': lgb_report['macro avg']['precision'],\n",
        "    'recall (macro)': lgb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lgb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': lgb_f1_weighted,\n",
        "    'auc (weighted_ovr)': lgb_auc\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 6. Rede Neural Densa (MLP) com TensorFlow/Keras\n",
        "print(\"Treinando Rede Neural Densa (MLP) com Undersampled data...\")\n",
        "\n",
        "\n",
        "mlp_model = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train_resampled_under.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(df['class'].unique()), activation='softmax')\n",
        "])\n",
        "\n",
        "mlp_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "mlp_model.summary()\n",
        "\n",
        "# treino do Modelo MLP\n",
        "epochs_mlp = 20\n",
        "batch_size_mlp = 64\n",
        "\n",
        "print(\"\\nTreinando MLP com dados Undersampled...\")\n",
        "\n",
        "history_mlp = mlp_model.fit(X_train_resampled_under, y_train_resampled_under,\n",
        "                          epochs=epochs_mlp,\n",
        "                          batch_size=batch_size_mlp,\n",
        "                          validation_split=0.1,\n",
        "                          verbose=1)\n",
        "\n",
        "\n",
        "# avaliação do Modelo MLP\n",
        "print(\"\\nAvaliando MLP (Undersampling) no conjunto de teste original...\")\n",
        "loss_mlp, accuracy_mlp = mlp_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da MLP (Undersampling) no conjunto de teste original: {accuracy_mlp:.4f}\")\n",
        "\n",
        "# classification report para MLP\n",
        "mlp_predictions_probs = mlp_model.predict(X_test)\n",
        "mlp_predictions = tf.argmax(mlp_predictions_probs, axis=1).numpy()\n",
        "\n",
        "print(\"\\nAvaliação completa da MLP (Undersampling):\")\n",
        "print(classification_report(y_test, mlp_predictions))\n",
        "\n",
        "# cálculo de F1-score ponderado\n",
        "mlp_f1_weighted = f1_score(y_test, mlp_predictions, average='weighted')\n",
        "print(\"F1-score ponderado:\", mlp_f1_weighted)\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    mlp_auc = roc_auc_score(y_test, mlp_predictions_probs, multi_class='ovr', average='weighted')\n",
        "except Exception as e:\n",
        "    mlp_auc = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "print(\"AUC (ponderado, one-vs-rest):\", mlp_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "mlp_report = classification_report(y_test, mlp_predictions, output_dict=True)\n",
        "results['MLP (Undersampling)'] = {\n",
        "    'accuracy': accuracy_score(y_test, mlp_predictions),\n",
        "    'precision (macro)': mlp_report['macro avg']['precision'],\n",
        "    'recall (macro)': mlp_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': mlp_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': mlp_f1_weighted,\n",
        "    'auc (weighted_ovr)': mlp_auc\n",
        "}\n",
        "\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ddoUE7KR4ZX-",
        "outputId": "51d6d5d9-4c13-445f-8722-c8c5c0ee24d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando LightGBM com Undersampled data...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005201 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4141\n",
            "[LightGBM] [Info] Number of data points in the train set: 3420, number of used features: 193\n",
            "[LightGBM] [Info] Start training from score -1.098612\n",
            "[LightGBM] [Info] Start training from score -1.098612\n",
            "[LightGBM] [Info] Start training from score -1.098612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avaliação do LightGBM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.20      0.65      0.31       290\n",
            "           1       0.97      0.76      0.85      3832\n",
            "           2       0.73      0.90      0.81       835\n",
            "\n",
            "    accuracy                           0.78      4957\n",
            "   macro avg       0.64      0.77      0.66      4957\n",
            "weighted avg       0.89      0.78      0.81      4957\n",
            "\n",
            "Acurácia:  0.7778898527335082\n",
            "F1-score ponderado: 0.8141790112878247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC (ponderado, one-vs-rest): 0.9211633741075974\n",
            "--------------------------------------------------\n",
            "Treinando Rede Neural Densa (MLP) com Undersampled data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_17\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_17\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,280,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_27 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_42 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_28 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_43 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,313,539\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,539</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,539\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,539</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando MLP com dados Undersampled...\n",
            "Epoch 1/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.4307 - loss: 1.0778 - val_accuracy: 0.0029 - val_loss: 1.3136\n",
            "Epoch 2/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.6523 - loss: 0.8858 - val_accuracy: 0.8275 - val_loss: 0.6526\n",
            "Epoch 3/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.8600 - loss: 0.4351 - val_accuracy: 0.8392 - val_loss: 0.4701\n",
            "Epoch 4/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9193 - loss: 0.2447 - val_accuracy: 0.7749 - val_loss: 0.5924\n",
            "Epoch 5/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.9554 - loss: 0.1502 - val_accuracy: 0.8480 - val_loss: 0.4409\n",
            "Epoch 6/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.9654 - loss: 0.1261 - val_accuracy: 0.7719 - val_loss: 0.6591\n",
            "Epoch 7/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.9808 - loss: 0.0767 - val_accuracy: 0.7836 - val_loss: 0.7128\n",
            "Epoch 8/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9858 - loss: 0.0605 - val_accuracy: 0.7690 - val_loss: 0.7473\n",
            "Epoch 9/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9899 - loss: 0.0497 - val_accuracy: 0.8158 - val_loss: 0.6877\n",
            "Epoch 10/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.9917 - loss: 0.0372 - val_accuracy: 0.8099 - val_loss: 0.7029\n",
            "Epoch 11/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9927 - loss: 0.0383 - val_accuracy: 0.7865 - val_loss: 0.8387\n",
            "Epoch 12/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9938 - loss: 0.0331 - val_accuracy: 0.7661 - val_loss: 0.9486\n",
            "Epoch 13/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.9951 - loss: 0.0203 - val_accuracy: 0.7661 - val_loss: 0.9120\n",
            "Epoch 14/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9962 - loss: 0.0179 - val_accuracy: 0.7807 - val_loss: 0.8503\n",
            "Epoch 15/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9924 - loss: 0.0197 - val_accuracy: 0.7456 - val_loss: 1.0411\n",
            "Epoch 16/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.9938 - loss: 0.0237 - val_accuracy: 0.8070 - val_loss: 0.8003\n",
            "Epoch 17/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.9965 - loss: 0.0144 - val_accuracy: 0.7339 - val_loss: 1.1763\n",
            "Epoch 18/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.9954 - loss: 0.0145 - val_accuracy: 0.7602 - val_loss: 0.9979\n",
            "Epoch 19/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.9939 - loss: 0.0248 - val_accuracy: 0.7222 - val_loss: 1.1869\n",
            "Epoch 20/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9950 - loss: 0.0148 - val_accuracy: 0.7515 - val_loss: 1.0829\n",
            "\n",
            "Avaliando MLP (Undersampling) no conjunto de teste original...\n",
            "\n",
            "Acurácia da MLP (Undersampling) no conjunto de teste original: 0.6462\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "\n",
            "Avaliação completa da MLP (Undersampling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.14      0.72      0.24       290\n",
            "           1       0.94      0.62      0.75      3832\n",
            "           2       0.65      0.72      0.68       835\n",
            "\n",
            "    accuracy                           0.65      4957\n",
            "   macro avg       0.58      0.69      0.56      4957\n",
            "weighted avg       0.84      0.65      0.71      4957\n",
            "\n",
            "F1-score ponderado: 0.7086657769350246\n",
            "AUC (ponderado, one-vs-rest): 0.8419408239495052\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Pré-processamento para CNN ---\n",
        "\n",
        "# O TfidfVectorizer não é ideal para CNNs, precisamos de uma representação baseada em sequências/embeddings.\n",
        "\n",
        "# 1. Tokenização para CNN (Sequências de Inteiros)\n",
        "max_words = 10000\n",
        "max_sequence_length = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df['processed_text'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df['processed_text'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "print(\"\\nExemplo de sequências padded:\")\n",
        "print(padded_sequences[:5])\n",
        "print(\"\\nForma das sequências padded:\", padded_sequences.shape)\n",
        "\n",
        "# --- prepara labels para CNN ---\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_y = label_encoder.fit_transform(df['class'])\n",
        "\n",
        "print(\"\\nLabels originais:\", df['class'].unique())\n",
        "print(\"Labels codificadas:\", encoded_y)\n",
        "\n",
        "# --- divisão dos dados para CNN (usando as sequências e labels codificadas) ---\n",
        "X_cnn = padded_sequences\n",
        "y_cnn = encoded_y\n",
        "\n",
        "# divisão original em treino e teste para a CNN\n",
        "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(X_cnn, y_cnn, test_size=0.20, random_state=42)\n",
        "\n",
        "print(\"\\nForma dos dados de treino para CNN (antes do undersampling):\", X_train_cnn.shape, y_train_cnn.shape)\n",
        "print(\"Forma dos dados de teste para CNN:\", X_test_cnn.shape, y_test_cnn.shape)\n",
        "print(\"\\nDistribuição das classes no treino da CNN (antes do balanceamento):\")\n",
        "print(pd.Series(y_train_cnn).value_counts())\n",
        "\n",
        "\n",
        "# --- aplica Undersampling Aleatório nos dados de TREINO da CNN ---\n",
        "try:\n",
        "    # inicializa RandomUnderSampler\n",
        "    # sampling_strategy='auto' remove instâncias da classe majoritária para igualar a classe minoritária\n",
        "    undersampler_cnn = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "    X_train_cnn_resampled_under, y_train_cnn_resampled_under = undersampler_cnn.fit_resample(X_train_cnn, y_train_cnn)\n",
        "\n",
        "    print(\"\\nForma dos dados de treino da CNN após Undersampling:\", X_train_cnn_resampled_under.shape, y_train_cnn_resampled_under.shape)\n",
        "    print(\"\\nDistribuição das classes no treino da CNN (depois do Undersampling):\")\n",
        "    print(pd.Series(y_train_cnn_resampled_under).value_counts())\n",
        "\n",
        "except NameError:\n",
        "    print(\"Erro: imblearn.under_sampling.RandomUnderSampler não encontrado. Certifique-se de que imbalanced-learn está instalado e RandomUnderSampler foi importado.\")\n",
        "    raise # relança o erro\n",
        "\n",
        "\n",
        "# --- construção do modelo da CNN\n",
        "embedding_dim = 128\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_filters = 128\n",
        "\n",
        "model_cnn_under = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    Conv1D(filters=num_filters, kernel_size=filter_sizes[0], activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model_cnn_under.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_cnn_under.summary()\n",
        "\n",
        "# --- treino do Modelo CNN (Com dados Undersampled) ---\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "print(\"\\nTreinando CNN com dados Undersampled...\")\n",
        "history_cnn_under = model_cnn_under.fit(X_train_cnn_resampled_under, y_train_cnn_resampled_under,\n",
        "                                        epochs=epochs,\n",
        "                                        batch_size=batch_size,\n",
        "                                        validation_split=0.1,\n",
        "                                        verbose=1)\n",
        "\n",
        "\n",
        "# --- avaliação do Modelo CNN ---\n",
        "print(\"\\nAvaliando CNN (Undersampling) no conjunto de teste original...\")\n",
        "\n",
        "loss_cnn_under, accuracy_cnn_under = model_cnn_under.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
        "print(f\"\\nAcurácia da CNN (Undersampling) no conjunto de teste original: {accuracy_cnn_under:.4f}\")\n",
        "\n",
        "# classification report para CNN (Undersampling)\n",
        "cnn_under_predictions_probs = model_cnn_under.predict(X_test_cnn)\n",
        "cnn_under_predictions = tf.argmax(cnn_under_predictions_probs, axis=1).numpy()\n",
        "\n",
        "print(\"\\nAvaliação completa da CNN (Undersampling):\")\n",
        "print(classification_report(y_test_cnn, cnn_under_predictions))\n",
        "\n",
        "# cálculo de F1-score ponderado e AUC para a CNN (Undersampling)\n",
        "cnn_under_f1_weighted = f1_score(y_test_cnn, cnn_under_predictions, average='weighted')\n",
        "\n",
        "try:\n",
        "    cnn_under_auc = roc_auc_score(y_test_cnn, cnn_under_predictions_probs, multi_class='ovr', average='weighted')\n",
        "except Exception as e:\n",
        "    cnn_under_auc = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "\n",
        "print(\"F1-score ponderado (CNN Undersampling):\", cnn_under_f1_weighted)\n",
        "print(\"AUC (ponderado, one-vs-rest) (CNN Undersampling):\", cnn_under_auc)\n",
        "\n",
        "# dicionário de resultados\n",
        "try:\n",
        "    cnn_under_report = classification_report(y_test_cnn, cnn_under_predictions, output_dict=True)\n",
        "    results['CNN (Embedding+Seq, Undersampling)'] = {\n",
        "        'accuracy': accuracy_score(y_test_cnn, cnn_under_predictions),\n",
        "        'precision (macro)': cnn_under_report['macro avg']['precision'],\n",
        "        'recall (macro)': cnn_under_report['macro avg']['recall'],\n",
        "        'f1-score (macro)': cnn_under_report['macro avg']['f1-score'],\n",
        "        'f1-score (weighted)': cnn_under_f1_weighted,\n",
        "        'auc (weighted_ovr)': cnn_under_auc\n",
        "    }\n",
        "except NameError:\n",
        "    print(\"Dicionário 'results' não encontrado. Os resultados da CNN (Undersampling) não foram armazenados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Vs7mM3v5-5Oc",
        "outputId": "3d0580a5-9191-450e-a990-878bdd325d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Exemplo de sequências padded:\n",
            "[[   3  100  838  920 2932  204   20   38   84   71   17    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   3   96   78 8148 5799   37 2045   78    4  457  381    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   3  635    3   97   14    2  175  256  921   15    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   3   33    5  568    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   3   15  433  235  434  235 4598    2  185   46    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "\n",
            "Forma das sequências padded: (24783, 100)\n",
            "\n",
            "Labels originais: [2 1 0]\n",
            "Labels codificadas: [2 1 1 ... 1 1 2]\n",
            "\n",
            "Forma dos dados de treino para CNN (antes do undersampling): (19826, 100) (19826,)\n",
            "Forma dos dados de teste para CNN: (4957, 100) (4957,)\n",
            "\n",
            "Distribuição das classes no treino da CNN (antes do balanceamento):\n",
            "1    15358\n",
            "2     3328\n",
            "0     1140\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Forma dos dados de treino da CNN após Undersampling: (3420, 100) (3420,)\n",
            "\n",
            "Distribuição das classes no treino da CNN (depois do Undersampling):\n",
            "0    1140\n",
            "1    1140\n",
            "2    1140\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_18\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_18\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_10 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_7          │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_44 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_29 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_45 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_7          │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando CNN com dados Undersampled...\n",
            "Epoch 1/10\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - accuracy: 0.4589 - loss: 1.0203 - val_accuracy: 0.9357 - val_loss: 0.5410\n",
            "Epoch 2/10\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.8182 - loss: 0.4934 - val_accuracy: 0.9240 - val_loss: 0.3105\n",
            "Epoch 3/10\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9094 - loss: 0.2588 - val_accuracy: 0.8392 - val_loss: 0.5293\n",
            "Epoch 4/10\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.9585 - loss: 0.1347 - val_accuracy: 0.8450 - val_loss: 0.5123\n",
            "Epoch 5/10\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9839 - loss: 0.0612 - val_accuracy: 0.8216 - val_loss: 0.5930\n",
            "Epoch 6/10\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.9943 - loss: 0.0283 - val_accuracy: 0.8275 - val_loss: 0.6535\n",
            "Epoch 7/10\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.9962 - loss: 0.0191 - val_accuracy: 0.8480 - val_loss: 0.5837\n",
            "Epoch 8/10\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9976 - loss: 0.0119 - val_accuracy: 0.8187 - val_loss: 0.7366\n",
            "Epoch 9/10\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.9981 - loss: 0.0134 - val_accuracy: 0.8246 - val_loss: 0.8168\n",
            "Epoch 10/10\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.9981 - loss: 0.0085 - val_accuracy: 0.8392 - val_loss: 0.7552\n",
            "\n",
            "Avaliando CNN (Undersampling) no conjunto de teste original...\n",
            "\n",
            "Acurácia da CNN (Undersampling) no conjunto de teste original: 0.7426\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n",
            "\n",
            "Avaliação completa da CNN (Undersampling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.18      0.68      0.28       290\n",
            "           1       0.96      0.73      0.83      3832\n",
            "           2       0.74      0.82      0.78       835\n",
            "\n",
            "    accuracy                           0.74      4957\n",
            "   macro avg       0.63      0.74      0.63      4957\n",
            "weighted avg       0.88      0.74      0.79      4957\n",
            "\n",
            "F1-score ponderado (CNN Undersampling): 0.7895373746160975\n",
            "AUC (ponderado, one-vs-rest) (CNN Undersampling): 0.8975223862868815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- reutiliza dados preparados para a CNN ---\n",
        "\n",
        "# --- aplica Undersampling Aleatório nos dados de TREINO da LSTM (que são os mesmos da CNN) ---\n",
        "\n",
        "print(\"\\nDistribuição das classes no treino da LSTM (antes do balanceamento):\")\n",
        "print(pd.Series(y_train_cnn).value_counts())\n",
        "\n",
        "try:\n",
        "    # inicializa RandomUnderSampler\n",
        "    undersampler_lstm = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "    # aplica Undersampling apenas nos dados de TREINO da LSTM\n",
        "    X_train_lstm_resampled_under, y_train_lstm_resampled_under = undersampler_lstm.fit_resample(X_train_cnn, y_train_cnn)\n",
        "\n",
        "    print(\"\\nForma dos dados de treino da LSTM após Undersampling:\", X_train_lstm_resampled_under.shape, y_train_lstm_resampled_under.shape)\n",
        "    print(\"\\nDistribuição das classes no treino da LSTM (depois do Undersampling):\")\n",
        "    print(pd.Series(y_train_lstm_resampled_under).value_counts())\n",
        "\n",
        "except NameError:\n",
        "    print(\"Erro: imblearn.under_sampling.RandomUnderSampler não encontrado. Certifique-se de que imbalanced-learn está instalado e RandomUnderSampler foi importado.\")\n",
        "    raise # relança o erro\n",
        "\n",
        "\n",
        "# --- Constrói o Modelo LSTM ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Construindo o Modelo LSTM...\")\n",
        "\n",
        "model_lstm_under = Sequential([\n",
        "    # camada de Embedding: Reutiliza os mesmos parâmetros da CNN para consistência na base de embedding\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "\n",
        "    # camada LSTM\n",
        "    LSTM(128),\n",
        "\n",
        "    # camada de Dropout\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # camada Densa (Fully Connected)\n",
        "    Dense(64, activation='relu'), # Camada densa adicional\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # camada de saída (igual à CNN)\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model_lstm_under.compile(optimizer='adam',\n",
        "                   loss='sparse_categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "model_lstm_under.summary()\n",
        "\n",
        "# --- treino do Modelo LSTM (Com dados Undersampled) ---\n",
        "epochs_lstm = 15\n",
        "batch_size_lstm = 32\n",
        "\n",
        "print(\"\\nTreinando LSTM com dados Undersampled...\")\n",
        "history_lstm_under = model_lstm_under.fit(X_train_lstm_resampled_under, y_train_lstm_resampled_under,\n",
        "                                          epochs=epochs_lstm,\n",
        "                                          batch_size=batch_size_lstm,\n",
        "                                          validation_split=0.1,\n",
        "                                          verbose=1)\n",
        "\n",
        "\n",
        "# --- avaliação do Modelo LSTM ---\n",
        "print(\"\\nAvaliando LSTM (Undersampling) no conjunto de teste original...\")\n",
        "\n",
        "loss_lstm_under, accuracy_lstm_under = model_lstm_under.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
        "print(f\"\\nAcurácia da LSTM (Undersampling) no conjunto de teste original: {accuracy_lstm_under:.4f}\")\n",
        "\n",
        "# classification report para LSTM (Undersampling)\n",
        "lstm_under_predictions_probs = model_lstm_under.predict(X_test_cnn)\n",
        "lstm_under_predictions = tf.argmax(lstm_under_predictions_probs, axis=1).numpy()\n",
        "print(\"\\nAvaliação completa da LSTM (Undersampling):\")\n",
        "\n",
        "class_names = [str(cls) for cls in label_encoder.classes_]\n",
        "print(classification_report(y_test_cnn, lstm_under_predictions, target_names=class_names))\n",
        "\n",
        "# cálculo de F1-score ponderado e AUC para a LSTM (Undersampling)\n",
        "lstm_under_f1_weighted = f1_score(y_test_cnn, lstm_under_predictions, average='weighted')\n",
        "\n",
        "try:\n",
        "    lstm_under_auc = roc_auc_score(y_test_cnn, lstm_under_predictions_probs, multi_class='ovr', average='weighted')\n",
        "except Exception as e:\n",
        "    lstm_under_auc = f\"N/A (problema ao calcular AUC: {e})\"\n",
        "\n",
        "print(\"F1-score ponderado (LSTM Undersampling):\", lstm_under_f1_weighted)\n",
        "print(\"AUC (ponderado, one-vs-rest) (LSTM Undersampling):\", lstm_under_auc)\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "# dicionário de resultados\n",
        "try:\n",
        "    lstm_under_report = classification_report(y_test_cnn, lstm_under_predictions, output_dict=True, target_names=class_names)\n",
        "    results['LSTM (Embedding+Seq, Undersampling)'] = {\n",
        "        'accuracy': accuracy_score(y_test_cnn, lstm_under_predictions),\n",
        "        'precision (macro)': lstm_under_report['macro avg']['precision'],\n",
        "        'recall (macro)': lstm_under_report['macro avg']['recall'],\n",
        "        'f1-score (macro)': lstm_under_report['macro avg']['f1-score'],\n",
        "        'f1-score (weighted)': lstm_under_f1_weighted,\n",
        "        'auc (weighted_ovr)': lstm_under_auc\n",
        "    }\n",
        "except NameError:\n",
        "    print(\"Dicionário 'results' não encontrado. Os resultados da LSTM (Undersampling) não foram armazenados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p5ZYFc0g43xF",
        "outputId": "a17f0e62-7d8e-49ef-ef46-0410bca9faf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Distribuição das classes no treino da LSTM (antes do balanceamento):\n",
            "1    15358\n",
            "2     3328\n",
            "0     1140\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Forma dos dados de treino da LSTM após Undersampling: (3420, 100) (3420,)\n",
            "\n",
            "Distribuição das classes no treino da LSTM (depois do Undersampling):\n",
            "0    1140\n",
            "1    1140\n",
            "2    1140\n",
            "Name: count, dtype: int64\n",
            "\n",
            "==================================================\n",
            "Construindo o Modelo LSTM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_19\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_19\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_11 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_30 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_31 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_47 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando LSTM com dados Undersampled...\n",
            "Epoch 1/15\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 209ms/step - accuracy: 0.3655 - loss: 1.0925 - val_accuracy: 0.0000e+00 - val_loss: 1.3191\n",
            "Epoch 2/15\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 198ms/step - accuracy: 0.3719 - loss: 1.0876 - val_accuracy: 0.0000e+00 - val_loss: 1.2524\n",
            "Epoch 3/15\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 205ms/step - accuracy: 0.3779 - loss: 1.0883 - val_accuracy: 0.0000e+00 - val_loss: 1.2695\n",
            "Epoch 4/15\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 204ms/step - accuracy: 0.3729 - loss: 1.0843 - val_accuracy: 0.0000e+00 - val_loss: 1.2767\n",
            "Epoch 5/15\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 199ms/step - accuracy: 0.3586 - loss: 1.0873 - val_accuracy: 0.0000e+00 - val_loss: 1.3320\n",
            "Epoch 6/15\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 215ms/step - accuracy: 0.3750 - loss: 1.0915 - val_accuracy: 0.0000e+00 - val_loss: 1.2613\n",
            "Epoch 7/15\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 217ms/step - accuracy: 0.3658 - loss: 1.0884 - val_accuracy: 0.0000e+00 - val_loss: 1.2891\n",
            "Epoch 8/15\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 221ms/step - accuracy: 0.3624 - loss: 1.0866 - val_accuracy: 0.0000e+00 - val_loss: 1.3020\n",
            "Epoch 9/15\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 205ms/step - accuracy: 0.3709 - loss: 1.0874 - val_accuracy: 0.0000e+00 - val_loss: 1.3034\n",
            "Epoch 10/15\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 230ms/step - accuracy: 0.3832 - loss: 1.0845 - val_accuracy: 0.0000e+00 - val_loss: 1.2951\n",
            "Epoch 11/15\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 222ms/step - accuracy: 0.3813 - loss: 1.0851 - val_accuracy: 0.0000e+00 - val_loss: 1.2953\n",
            "Epoch 12/15\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 213ms/step - accuracy: 0.3750 - loss: 1.0887 - val_accuracy: 0.0000e+00 - val_loss: 1.3590\n",
            "Epoch 13/15\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 216ms/step - accuracy: 0.3728 - loss: 1.0851 - val_accuracy: 0.0000e+00 - val_loss: 1.3534\n",
            "Epoch 14/15\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 207ms/step - accuracy: 0.3848 - loss: 1.0871 - val_accuracy: 0.0000e+00 - val_loss: 1.3644\n",
            "Epoch 15/15\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 201ms/step - accuracy: 0.3644 - loss: 1.0902 - val_accuracy: 0.0000e+00 - val_loss: 1.3537\n",
            "\n",
            "Avaliando LSTM (Undersampling) no conjunto de teste original...\n",
            "\n",
            "Acurácia da LSTM (Undersampling) no conjunto de teste original: 0.7730\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 54ms/step\n",
            "\n",
            "Avaliação completa da LSTM (Undersampling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       290\n",
            "           1       0.77      1.00      0.87      3832\n",
            "           2       0.00      0.00      0.00       835\n",
            "\n",
            "    accuracy                           0.77      4957\n",
            "   macro avg       0.26      0.33      0.29      4957\n",
            "weighted avg       0.60      0.77      0.67      4957\n",
            "\n",
            "F1-score ponderado (LSTM Undersampling): 0.6740973395206054\n",
            "AUC (ponderado, one-vs-rest) (LSTM Undersampling): 0.4985053575678161\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Resultados**"
      ],
      "metadata": {
        "id": "yIzkmiNXCC1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Dataset Original**"
      ],
      "metadata": {
        "id": "VVK1y6MJCHoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Modelo                        | Acurácia | F1-score Ponderado | AUC (ponderado, OVR)     |\n",
        "|-------------------------------|----------|---------------------|---------------------------|\n",
        "| Regressão Logística           | 0.8923   | 0.8784              | 0.9387                    |\n",
        "| Naive Bayes Multinomial       | 0.8322   | 0.7882              | 0.8860                    |\n",
        "| SVM (Kernel Linear)           | 0.8983   | 0.8824              | 0.9361 |\n",
        "| Random Forest                 | 0.8911   | 0.8769              | 0.9392                    |\n",
        "| LightGBM                      | 0.8963   | 0.8866              | 0.9453                    |\n",
        "| MLP (Perceptron Multicamadas) | 0.8500   | 0.8464              | 0.8944                    |\n",
        "| CNN (Convolutional NN)        | 0.8700   | 0.8674              | 0.9170                    |\n",
        "| LSTM                          | 0.7700   | 0.6740              | 0.5010 *(classe 0 e 2 sem predição)* |\n"
      ],
      "metadata": {
        "id": "3yPi-EAPIDRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Dataset Balanceado**"
      ],
      "metadata": {
        "id": "aP75jgq3CM-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Quadro Resumo dos Modelos com `class_weight`**\n",
        "\n",
        "| Modelo                  | Acurácia  | F1-score Ponderado | AUC (ponderado, OVR)    |\n",
        "| ----------------------- | --------- | ------------------ | ----------------------- |\n",
        "| Regressão Logística     | 0.8376     | 0.8538              | 0.9313                   |\n",
        "| Naive Bayes Multinomial | 0.8322     | 0.7882              | 0.8860                   |\n",
        "| SVM (Kernel Linear)     | 0.8374     | 0.8572              | 0.9409 |\n",
        "| Random Forest           | 0.8907 | 0.8802          | 0.9394              |\n",
        "| LightGBM                | 0.8485     | 0.8646              | 0.9386                   |\n",
        "| MLP (Rede Neural)       | 0.8300     | 0.8420              | 0.8977                   |\n",
        "| CNN (Rede Neural)       | 0.8300     | 0.8455              | 0.9078                   |\n",
        "| LSTM (Rede Neural)      | 0.0600     | 0.0065              | 0.5994                   |\n"
      ],
      "metadata": {
        "id": "c46v1NUiI10y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Quadro Resumo dos Modelos com Oversampling**\n",
        "\n",
        "| Modelo                  | Acurácia | F1-score Ponderado | AUC (ponderado, OVR)    |\n",
        "| ----------------------- | -------- | ------------------ | ----------------------- |\n",
        "| Regressão Logística     | 0.8396    | 0.8572              | 0.9266                  |\n",
        "| Naive Bayes Multinomial | 0.7906    | 0.8159              | 0.8967                  |\n",
        "| SVM (Kernel Linear)     | 0.8363    | 0.8576              | 0.9238 |\n",
        "| Random Forest           | 0.8757    | 0.8787              | 0.9341                  |\n",
        "| LightGBM                | 0.8723    | 0.8811              | 0.9389                  |\n",
        "| MLP (Rede Neural densa) | 0.8600    | 0.8456              | 0.8815                  |\n",
        "| CNN                     | 0.7300    | 0.7706              | 0.8580                  |\n",
        "| LSTM                    | 0.7700   | 0.6741              | 0.5000                  |\n",
        "\n"
      ],
      "metadata": {
        "id": "pBuGutVFJfDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Quadro Resumo dos Modelos com Undersampling**\n",
        "\n",
        "| Modelo                  | Acurácia | F1-score Ponderado | AUC (ponderado, OVR)    |\n",
        "| ----------------------- | -------- | ------------------ | ----------------------- |\n",
        "| Regressão Logística     | 0.7952   | 0.8222             | 0.9177                  |\n",
        "| Naive Bayes Multinomial | 0.7331   | 0.7810             | 0.8886                  |\n",
        "| SVM (Kernel Linear)     | 0.7906   | 0.8212             | 0.9259 |\n",
        "| Random Forest           | 0.8158   | 0.8404             | 0.9306                  |\n",
        "| LightGBM            | 0.7779   | 0.8142             | 0.9212                  |\n",
        "| MLP                     | 0.6500   | 0.7087             | 0.8419                  |\n",
        "| CNN                     | 0.7400   | 0.7895             | 0.8975                  |\n",
        "| LSTM                    | 0.7700   | 0.6741             | 0.4985                  |\n"
      ],
      "metadata": {
        "id": "CDyzkzNBKQIz"
      }
    }
  ]
}