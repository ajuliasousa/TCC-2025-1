{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajuliasousa/TCC-2025-1/blob/main/TCC_Modelos_Tradicionais_DL_Dataset_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN-GPL6RJ9D6"
      },
      "source": [
        "### **Panorama Geral do Trabalho**\n",
        "\n",
        "**Título:**\n",
        "\n",
        "Avaliação Comparativa de Modelos de Machine Learning e Deep Learning para Detecção de Discurso de Ódio com Diferentes Técnicas de Representação e Balanceamento de Dados\n",
        "\n",
        "**Objetivo Geral:**\n",
        "\n",
        "Avaliar e comparar o desempenho de diferentes algoritmos de classificação, incluindo modelos tradicionais de Machine Learning (ML) e arquiteturas de Deep Learning (DL), na tarefa de detecção de discurso de ódio em dados textuais, analisando o impacto de distintas abordagens de representação vetorial e técnicas de balanceamento de classes.\n",
        "\n",
        "**Objetivos Específicos:**\n",
        "\n",
        "1.\tRealizar o pré-processamento e a limpeza de um corpus textual rotulado para discurso de ódio.\n",
        "2.\tRepresentar os dados textuais por meio de TF-IDF e embeddings (pré-treinados ou ajustados).\n",
        "3.\tTreinar e avaliar modelos de ML com TF-IDF: Naive Bayes, Regressão Logística, SVM, Random Forest, LightGBM, MLP.\n",
        "4.\tPreparar dados com tokenização, vocabulário e padding para modelos DL com embeddings.\n",
        "5.\tTreinar redes profundas: CNN e LSTM com embeddings.\n",
        "6.\tAplicar diferentes estratégias de balanceamento de dados: oversampling, undersampling e uso de class_weight.\n",
        "7.\tAvaliar os modelos usando métricas: F1-score ponderado, AUC, acurácia.\n",
        "8.\tComparar sistematicamente o desempenho dos modelos em diferentes cenários e técnicas de representação.\n",
        "9.\tDiscutir os impactos do tipo de vetorização e do balanceamento nos resultados obtidos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUUTFj-_KATf"
      },
      "source": [
        "### **Dataset 2: Twitter Sentiment Analysis**\n",
        "\n",
        "**Link:**\n",
        "https://www.kaggle.com/datasets/arkhoshghalb/twitter-sentiment-analysis-hatred-speech/data\n",
        "\n",
        "Neste projeto, foi utilizado o conjunto de dados disponibilizado pela **Analytics Vidhya** no Kaggle, intitulado Twitter Sentiment Analysis for Hate Speech Detection¹. O objetivo principal deste dataset é fornecer uma base para a **detecção automática de discursos de ódio em publicações do Twitter**, com foco específico em manifestações de cunho **racista ou sexista**.\n",
        "\n",
        "Cada amostra do conjunto é composta pelo texto completo de um tweet e um rótulo binário associado: '1' indica que o tweet contém discurso de ódio (com conteúdo racista ou sexista), enquanto '0' representa tweets livres desse tipo de conteúdo. Para preservar a privacidade, menções a outros usuários foram substituídas por \"@user\".\n",
        "\n",
        "O dataset serve como base para o desenvolvimento e avaliação de modelos de classificação supervisionada, sendo amplamente utilizado em tarefas de **Processamento de Linguagem Natural (PLN)** voltadas à análise de sentimentos e moderação de conteúdo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Uv2Rw1KNah"
      },
      "source": [
        "### **Bibliotecas e Visão do Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4hyC2lUKVdU",
        "outputId": "7ed7719e-f193-4fcc-8768-8faa343292be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# bibliotecas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_file_path = '/content/drive/MyDrive/TCC/Datasets/Twitter hate speech/train_tweets.csv'\n",
        "test_file_path = '/content/drive/MyDrive/TCC/Datasets/Twitter hate speech/test_tweets.csv'\n",
        "\n",
        "df_train = pd.read_csv(train_file_path)\n",
        "df_test = pd.read_csv(test_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knalaEcsKXso",
        "outputId": "f82ce4f4-d79a-40b0-8e75-5138d929a5b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset de Treino:\n",
            "Primeiras linhas:\n",
            "   id  label                                              tweet\n",
            "0   1      0   @user when a father is dysfunctional and is s...\n",
            "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
            "2   3      0                                bihday your majesty\n",
            "3   4      0  #model   i love u take with u all the time in ...\n",
            "4   5      0             factsguide: society now    #motivation\n",
            "\n",
            "Informações:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 31962 entries, 0 to 31961\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      31962 non-null  int64 \n",
            " 1   label   31962 non-null  int64 \n",
            " 2   tweet   31962 non-null  object\n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 749.2+ KB\n",
            "\n",
            "Descrição estatística:\n",
            "                 id         label\n",
            "count  31962.000000  31962.000000\n",
            "mean   15981.500000      0.070146\n",
            "std     9226.778988      0.255397\n",
            "min        1.000000      0.000000\n",
            "25%     7991.250000      0.000000\n",
            "50%    15981.500000      0.000000\n",
            "75%    23971.750000      0.000000\n",
            "max    31962.000000      1.000000\n",
            "\n",
            "Distribuição da coluna 'label':\n",
            "label\n",
            "0    92.98542\n",
            "1     7.01458\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "==================================================\n",
            "\n",
            "Dataset de Teste:\n",
            "Primeiras linhas:\n",
            "      id                                              tweet\n",
            "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
            "1  31964   @user #white #supremacists want everyone to s...\n",
            "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
            "3  31966  is the hp and the cursed child book up for res...\n",
            "4  31967    3rd #bihday to my amazing, hilarious #nephew...\n",
            "\n",
            "Informações:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17197 entries, 0 to 17196\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      17197 non-null  int64 \n",
            " 1   tweet   17197 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 268.8+ KB\n",
            "\n",
            "Descrição estatística:\n",
            "                 id\n",
            "count  17197.000000\n",
            "mean   40561.000000\n",
            "std     4964.490625\n",
            "min    31963.000000\n",
            "25%    36262.000000\n",
            "50%    40561.000000\n",
            "75%    44860.000000\n",
            "max    49159.000000\n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset de Treino:\")\n",
        "print(\"Primeiras linhas:\")\n",
        "print(df_train.head())\n",
        "print(\"\\nInformações:\")\n",
        "df_train.info()\n",
        "print(\"\\nDescrição estatística:\")\n",
        "print(df_train.describe())\n",
        "print(\"\\nDistribuição da coluna 'label':\")\n",
        "print(df_train['label'].value_counts(normalize=True) * 100)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"Dataset de Teste:\")\n",
        "print(\"Primeiras linhas:\")\n",
        "print(df_test.head())\n",
        "print(\"\\nInformações:\")\n",
        "df_test.info()\n",
        "print(\"\\nDescrição estatística:\")\n",
        "print(df_test.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A etapa de estatística descritiva permitiu uma visão inicial da composição e distribuição dos dados utilizados neste projeto. O **dataset de treino** é composto por **31.962 tweets**, cada um identificado por um id, acompanhado de um label binário e o conteúdo textual do tweet.\n",
        "\n",
        "A variável label é a variável-alvo do modelo e representa a presença (1) ou ausência (0) de discurso de ódio no texto. A análise da distribuição mostra um forte desbalanceamento: **apenas 7% dos tweets são classificados como contendo discurso de ódio**, enquanto os demais 93% não apresentam esse tipo de conteúdo. Esse desequilíbrio reforça a importância de aplicar técnicas de balanceamento durante o treinamento dos modelos.\n",
        "\n",
        "Já o **dataset de teste** contém **17.197 tweets** e serve exclusivamente para avaliação, não incluindo os rótulos (`label`). Ambos os conjuntos não apresentam valores ausentes, e os identificadores (`id`) variam em um intervalo contínuo. A análise exploratória também revelou que os dados estão bem estruturados e prontos para serem utilizados nas etapas subsequentes de pré-processamento e modelagem."
      ],
      "metadata": {
        "id": "8Wma-kqo--Tf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLZryRJdNwoJ"
      },
      "source": [
        "### **Pré-processamento Textual**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Limpeza com clean_text()**\n",
        "\n",
        "Função que aplica regras para \"limpar\" os textos brutos:\n",
        "\n",
        "    Remove URLs: tira links da internet (ex: http://...).\n",
        "\n",
        "    Remove menções: elimina nomes de usuários (@usuario).\n",
        "\n",
        "    Remove hashtags: exclui palavras precedidas de # (ou poderia apenas remover o símbolo).\n",
        "\n",
        "    Remove caracteres não alfabéticos: exclui números, pontuações e símbolos, mantendo letras e acentuação.\n",
        "\n",
        "    Converte para minúsculas: uniformiza o texto.\n",
        "\n",
        "    Remove espaços extras: com strip().\n",
        "\n",
        "Resultado: uma versão mais \"limpa\" do tweet.\n",
        "\n",
        "**2. Tokenização e Processamento Avançado**\n",
        "\n",
        "Com a função preprocess_text_advanced():\n",
        "\n",
        "    Tokenização: divide o texto em palavras (tokens) com word_tokenize.\n",
        "\n",
        "    Remoção de stopwords: elimina palavras muito comuns em inglês (ex: the, and, is) que pouco contribuem para a análise.\n",
        "\n",
        "    Lematização: reduz as palavras à sua forma base (ex: running vira run, cars vira car), usando o WordNetLemmatizer."
      ],
      "metadata": {
        "id": "5PCZq8is_btW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PGZ5tU4KnOQ",
        "outputId": "5f40e2bc-9849-4ee0-f4ec-838a9579d540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 'punkt_tab'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'punkt_tab' downloaded.\n",
            "\n",
            "Exemplo de tokens processados no dataset de Treino:\n",
            "                                               tweet  \\\n",
            "0   @user when a father is dysfunctional and is s...   \n",
            "1  @user @user thanks for #lyft credit i can't us...   \n",
            "2                                bihday your majesty   \n",
            "3  #model   i love u take with u all the time in ...   \n",
            "4             factsguide: society now    #motivation   \n",
            "\n",
            "                                    processed_tokens  \n",
            "0  [father, dysfunctional, selfish, drag, kid, dy...  \n",
            "1  [thanks, credit, cant, use, cause, dont, offer...  \n",
            "2                                  [bihday, majesty]  \n",
            "3                       [love, u, take, u, time, ur]  \n",
            "4                              [factsguide, society]  \n",
            "\n",
            "Exemplo de tokens processados no dataset de Teste:\n",
            "                                               tweet  \\\n",
            "0  #studiolife #aislife #requires #passion #dedic...   \n",
            "1   @user #white #supremacists want everyone to s...   \n",
            "2  safe ways to heal your #acne!!    #altwaystohe...   \n",
            "3  is the hp and the cursed child book up for res...   \n",
            "4    3rd #bihday to my amazing, hilarious #nephew...   \n",
            "\n",
            "                                    processed_tokens  \n",
            "0                                             [find]  \n",
            "1                   [want, everyone, see, new, here]  \n",
            "2                                  [safe, way, heal]  \n",
            "3  [hp, cursed, child, book, reservation, already...  \n",
            "4  [rd, amazing, hilarious, eli, ahmir, uncle, da...  \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "\n",
        "# regras de limpeza\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text) # remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text) # remove menções\n",
        "    text = re.sub(r'#\\w+', '', text) # remove hashtags (ou pode mantê-las sem o #)\n",
        "    # removendo acentos para simplificar a lematização com WordNetLemmatizer\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A) # remove caracteres não alfabéticos\n",
        "    text = text.lower() # converte para minúsculas\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "df_train['cleaned_tweet'] = df_train['tweet'].apply(clean_text)\n",
        "\n",
        "df_test['cleaned_tweet'] = df_test['tweet'].apply(clean_text)\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    print(\"Downloading 'punkt_tab'...\")\n",
        "    nltk.download('punkt_tab')\n",
        "    print(\"'punkt_tab' downloaded.\")\n",
        "\n",
        "\n",
        "# incializa lematizador e stop words para inglês\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text_advanced(text):\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return []\n",
        "    tokens = word_tokenize(text)\n",
        "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word.isalpha()] # Adicionado isalpha() para garantir que são palavras\n",
        "    return processed_tokens\n",
        "\n",
        "# aplicação do pré-processamento\n",
        "df_train['processed_tokens'] = df_train['cleaned_tweet'].apply(preprocess_text_advanced)\n",
        "df_test['processed_tokens'] = df_test['cleaned_tweet'].apply(preprocess_text_advanced)\n",
        "\n",
        "\n",
        "print(\"\\nExemplo de tokens processados no dataset de Treino:\")\n",
        "print(df_train[['tweet', 'processed_tokens']].head())\n",
        "\n",
        "print(\"\\nExemplo de tokens processados no dataset de Teste:\")\n",
        "print(df_test[['tweet', 'processed_tokens']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGL9oMu7bH88"
      },
      "source": [
        "### **Vetorização com  TF-IDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para transformar os textos dos tweets em uma representação numérica adequada aos algoritmos de aprendizado de máquina, foi utilizado o método de vetorização **TF-IDF (Term Frequency-Inverse Document Frequency).**\n",
        "\n",
        "Esse processo converte cada tweet em um vetor que reflete a relevância de cada termo no contexto do corpus, penalizando palavras muito frequentes e destacando termos mais informativos. Antes da vetorização, os tokens de cada tweet foram recombinados em strings, já que o `TfidfVectorizer` opera sobre textos contínuos.\n",
        "\n",
        "A vetorização foi aplicada **separadamente nos conjuntos de treino e teste** para evitar vazamento de dados: o vocabulário foi aprendido apenas a partir dos textos de treino e, em seguida, utilizado para transformar os dados de teste. Como resultado, obteve-se uma matriz esparsa de características com as 5.000 palavras mais relevantes do corpus, gerando uma matriz de tamanho (31.962, 5000) para o conjunto de treino e (17.197, 5000) para o conjunto de teste.\n",
        "\n",
        "Para fins exploratórios e para permitir testes mais amplos, também foi criada uma matriz TF-IDF combinando os dois conjuntos, resultando em uma estrutura final de (49.159, 5000). Essa vetorização garante uma representação consistente entre os conjuntos e prepara os dados para as etapas subsequentes de modelagem supervisionada."
      ],
      "metadata": {
        "id": "inu0aCcGACBf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktc9o8kwLjHC",
        "outputId": "ab468c52-d92d-4e13-f5fa-829148869579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forma da matriz TF-IDF de Treino:\n",
            "(31962, 5000)\n",
            "\n",
            "Forma da matriz TF-IDF de Teste:\n",
            "(17197, 5000)\n",
            "\n",
            "Forma da matriz TF-IDF do dataframe combinado:\n",
            "(49159, 5000)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# --- combina df_train e df_test em um único df ---\n",
        "df_train_temp = df_train.copy() # Evita SettingWithCopyWarning\n",
        "df_train_temp = df_train_temp.rename(columns={'label': 'class'})\n",
        "\n",
        "# aplica o vetorizador APENAS aos textos de treino para aprender o vocabulário\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(df_train['processed_tokens'].apply(lambda tokens: ' '.join(tokens)))\n",
        "X_test_tfidf = tfidf_vectorizer.transform(df_test['processed_tokens'].apply(lambda tokens: ' '.join(tokens)))\n",
        "\n",
        "\n",
        "print(\"\\nForma da matriz TF-IDF de Treino:\")\n",
        "print(X_train_tfidf.shape)\n",
        "print(\"\\nForma da matriz TF-IDF de Teste:\")\n",
        "print(X_test_tfidf.shape)\n",
        "\n",
        "# renomeia coluna 'label' para 'class' no df_train\n",
        "df_train_renamed = df_train.rename(columns={'label': 'class'})\n",
        "df_test_placeholder_class = df_test.copy()\n",
        "df_test_placeholder_class['class'] = -1\n",
        "\n",
        "# concatena os dataframes\n",
        "df = pd.concat([df_train_renamed[['processed_tokens', 'class']], df_test_placeholder_class[['processed_tokens', 'class']]], ignore_index=True)\n",
        "\n",
        "# inicializa o TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_tokens'].apply(lambda tokens: ' '.join(tokens)))\n",
        "\n",
        "print(\"\\nForma da matriz TF-IDF do dataframe combinado:\")\n",
        "print(tfidf_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Aplicação em dataset desbalanceado**"
      ],
      "metadata": {
        "id": "ReKqtJ_gr2qu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6G441n_bOqA"
      },
      "source": [
        "#### **Divisão em treino/ teste**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzsuXIjDMZpT",
        "outputId": "32c9fc29-9c86-4b60-df52-c7d48eccfeda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma dos dados de treino (X_train_split, y_train_split): (25569, 5000) (25569,)\n",
            "Forma dos dados de validação (X_val_split, y_val_split): (6393, 5000) (6393,)\n",
            "\n",
            "Variáveis para treino/teste dos modelos (split do TREINO original):\n",
            "Forma de X_train: (25569, 5000) y_train: (25569,)\n",
            "Forma de X_test: (6393, 5000) y_test: (6393,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "X_split = X_train_tfidf # usa a matriz TF-IDF vetorizada do dataset de TREINO original\n",
        "y_split = df_train['label'] # usa as labels do dataset de TREINO original (coluna 'label')\n",
        "\n",
        "# Dividir os dados de TREINO originais em conjuntos de TREINO e VALIDAÇÃO\n",
        "# Chama-se X_train_split, X_val_split, etc. para evitar conflito\n",
        "# com X_train_tfidf e X_test_tfidf que representam os datasets originais.\n",
        "# test_size=0.20 significa 20% dos dados de TREINO originais para VALIDAÇÃO\n",
        "# random_state para reprodutibilidade\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_split, y_split, test_size=0.20, random_state=42, stratify=y_split\n",
        ")\n",
        "\n",
        "print(\"Forma dos dados de treino (X_train_split, y_train_split):\", X_train_split.shape, y_train_split.shape)\n",
        "print(\"Forma dos dados de validação (X_val_split, y_val_split):\", X_val_split.shape, y_val_split.shape)\n",
        "\n",
        "X_train = X_train_split\n",
        "X_test = X_val_split\n",
        "y_train = y_train_split\n",
        "y_test = y_val_split\n",
        "\n",
        "print(\"\\nVariáveis para treino/teste dos modelos (split do TREINO original):\")\n",
        "print(\"Forma de X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
        "print(\"Forma de X_test:\", X_test.shape, \"y_test:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-oIDIWZbtPh"
      },
      "source": [
        "#### **Modelos Tradicionais**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quatro modelos de classificação supervisionada foram treinados para prever categorias de tweets (discurso de ódio, linguagem ofensiva ou nenhum dos dois), utilizando a matriz TF-IDF como entrada. Os modelos testados foram: **Regressão Logística, Naive Bayes Multinomial, Support Vector Machine (SVM com kernel linear) e Random Forest**. Cada modelo foi ajustado com os dados de treino e avaliado com os dados de teste usando métricas como **acurácia, F1-score (ponderado) e AUC (curva ROC, ponderada)**. Os resultados foram armazenados em um dicionário para facilitar a comparação de desempenho entre os classificadores.\n",
        "\n",
        "**Regressão Logística**\n",
        "\n",
        "A Regressão Logística foi utilizada como um modelo linear de base para classificação. Ela estima a probabilidade de um tweet pertencer a cada uma das  classes com base nas palavras mais relevantes (extraídas via TF-IDF). O modelo foi treinado com um número maior de iterações (max_iter=1000) para garantir a convergência, dado o tamanho da matriz. Por oferecer suporte ao método predict_proba, foi possível calcular a métrica AUC ponderada (one-vs-rest), o que fornece uma medida robusta da capacidade do modelo em distinguir entre as classes.\n",
        "\n",
        "**Naive Bayes Multinomial**\n",
        "\n",
        "O modelo Naive Bayes Multinomial é especialmente adequado para tarefas de classificação de texto, por assumir que as características (neste caso, palavras) ocorrem de forma independente. Ele é simples, eficiente e frequentemente usado como forte baseline em PLN. No experimento, ele também permitiu a geração de probabilidades de classe (predict_proba), o que possibilitou o cálculo da AUC . Apesar de suas suposições simplificadas, o Naive Bayes costuma ter desempenho competitivo quando os dados estão bem vetorizados.\n",
        "\n",
        "**Support Vector Machine (SVM)**\n",
        "\n",
        "O SVM foi utilizado com kernel linear, uma configuração comum e eficaz para dados textuais de alta dimensionalidade, como é o caso da matriz TF-IDF. Foi ativada a opção probability=True para possibilitar o cálculo da AUC, embora isso torne o treinamento mais custoso computacionalmente. O SVM busca encontrar hiperplanos que melhor separam as classes, sendo especialmente útil quando há margens claras entre categorias. Apesar de não ser naturalmente probabilístico, sua robustez o torna uma escolha frequente em tarefas de classificação com múltiplas classes.\n",
        "\n",
        "**Random Forest**\n",
        "\n",
        "O modelo Random Forest foi treinado com 100 árvores de decisão, combinando os resultados de várias árvores para aumentar a estabilidade e a precisão da predição. Como um modelo de ensemble, ele lida bem com dados complexos e é menos sensível a overfitting do que uma única árvore. Também oferece suporte a predict_proba, permitindo calcular a AUC ponderada. Sua capacidade de capturar interações não lineares entre os termos dos textos pode ser vantajosa em relação a modelos lineares, especialmente quando o texto contém padrões mais sutis."
      ],
      "metadata": {
        "id": "_O193NxRBoCR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAGE1eTyMTTM",
        "outputId": "263fdc4d-65c0-4d71-b66c-de9109ad442d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando Regressão Logística...\n",
            "Avaliação da Regressão Logística:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97      5945\n",
            "           1       0.91      0.24      0.38       448\n",
            "\n",
            "    accuracy                           0.94      6393\n",
            "   macro avg       0.93      0.62      0.67      6393\n",
            "weighted avg       0.94      0.94      0.93      6393\n",
            "\n",
            "Acurácia:  0.9449397778820585\n",
            "Weighted F1-score: 0.9294819411928334\n",
            "AUC: 0.8939741529496577\n",
            "--------------------------------------------------\n",
            "Treinando Naive Bayes Multinomial...\n",
            "Avaliação do Naive Bayes Multinomial:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97      5945\n",
            "           1       0.96      0.23      0.38       448\n",
            "\n",
            "    accuracy                           0.95      6393\n",
            "   macro avg       0.95      0.62      0.67      6393\n",
            "weighted avg       0.95      0.95      0.93      6393\n",
            "\n",
            "Acurácia:  0.9457218833098702\n",
            "Weighted F1-score: 0.9299568965040391\n",
            "AUC: 0.8658604169169769\n",
            "--------------------------------------------------\n",
            "Treinando SVM (Kernel Linear)...\n",
            "Avaliação do SVM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97      5945\n",
            "           1       0.89      0.35      0.50       448\n",
            "\n",
            "    accuracy                           0.95      6393\n",
            "   macro avg       0.92      0.67      0.74      6393\n",
            "weighted avg       0.95      0.95      0.94      6393\n",
            "\n",
            "Acurácia:  0.9510402002189895\n",
            "Weighted F1-score: 0.9408625073775384\n",
            "AUC: 0.8590614862429412\n",
            "--------------------------------------------------\n",
            "Treinando Random Forest...\n",
            "Avaliação do Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.98      5945\n",
            "           1       0.78      0.50      0.61       448\n",
            "\n",
            "    accuracy                           0.95      6393\n",
            "   macro avg       0.87      0.74      0.79      6393\n",
            "weighted avg       0.95      0.95      0.95      6393\n",
            "\n",
            "Acurácia:  0.9549507273580479\n",
            "Weighted F1-score: 0.9503531495962668\n",
            "AUC: 0.8849094752493091\n",
            "\n",
            "Resultados acumulados até agora:\n",
            "{'Logistic Regression': {'accuracy': 0.9449397778820585, 'precision (macro)': 0.929654238517192, 'recall (macro)': 0.6174625285353839, 'f1-score (macro)': 0.6735430005884053, 'f1-score (weighted)': 0.9294819411928334, 'auc': np.float64(0.8939741529496577)}, 'Multinomial NB': {'accuracy': 0.9457218833098702, 'precision (macro)': 0.9543598420920467, 'recall (macro)': 0.6168510828427249, 'f1-score (macro)': 0.6743222874431132, 'f1-score (weighted)': 0.9299568965040391, 'auc': np.float64(0.8658604169169769)}, 'SVM': {'accuracy': 0.9510402002189895, 'precision (macro)': 0.9192965124293526, 'recall (macro)': 0.6713089856421963, 'f1-score (macro)': 0.7359292562806259, 'f1-score (weighted)': 0.9408625073775384, 'auc': np.float64(0.8590614862429412)}, 'Random Forest': {'accuracy': 0.9549507273580479, 'precision (macro)': 0.8705432705432705, 'recall (macro)': 0.7446173254835997, 'f1-score (macro)': 0.7923976186180769, 'f1-score (weighted)': 0.9503531495962668, 'auc': np.float64(0.8849094752493091)}}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "results = {}\n",
        "\n",
        "# 1. Regressão Logística\n",
        "print(\"Treinando Regressão Logística...\")\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "lr_predictions_proba = lr_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação da Regressão Logística:\")\n",
        "print(classification_report(y_test, lr_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lr_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, lr_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, lr_predictions_proba))\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "lr_report = classification_report(y_test, lr_predictions, output_dict=True)\n",
        "results['Logistic Regression'] = {\n",
        "    'accuracy': accuracy_score(y_test, lr_predictions),\n",
        "    'precision (macro)': lr_report['macro avg']['precision'],\n",
        "    'recall (macro)': lr_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lr_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lr_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, lr_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 2. Naive Bayes Multinomial\n",
        "print(\"Treinando Naive Bayes Multinomial...\")\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_predictions = nb_model.predict(X_test)\n",
        "nb_predictions_proba = nb_model.predict_proba(X_test)[:, 1] # classificação binária\n",
        "\n",
        "print(\"Avaliação do Naive Bayes Multinomial:\")\n",
        "print(classification_report(y_test, nb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, nb_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, nb_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, nb_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "nb_report = classification_report(y_test, nb_predictions, output_dict=True)\n",
        "results['Multinomial NB'] = {\n",
        "    'accuracy': accuracy_score(y_test, nb_predictions),\n",
        "    'precision (macro)': nb_report['macro avg']['precision'],\n",
        "    'recall (macro)': nb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': nb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, nb_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, nb_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 3. Support Vector Machine (SVM)\n",
        "print(\"Treinando SVM (Kernel Linear)...\")\n",
        "svm_model = SVC(kernel='linear', probability=True)\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_predictions = svm_model.predict(X_test)\n",
        "svm_predictions_proba = svm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do SVM:\")\n",
        "print(classification_report(y_test, svm_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, svm_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, svm_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, svm_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "svm_report = classification_report(y_test, svm_predictions, output_dict=True)\n",
        "results['SVM'] = {\n",
        "    'accuracy': accuracy_score(y_test, svm_predictions),\n",
        "    'precision (macro)': svm_report['macro avg']['precision'],\n",
        "    'recall (macro)': svm_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': svm_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, svm_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, svm_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 4. Random Forest Classifier\n",
        "print(\"Treinando Random Forest...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "rf_predictions_proba = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do Random Forest:\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, rf_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, rf_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, rf_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "rf_report = classification_report(y_test, rf_predictions, output_dict=True)\n",
        "results['Random Forest'] = {\n",
        "    'accuracy': accuracy_score(y_test, rf_predictions),\n",
        "    'precision (macro)': rf_report['macro avg']['precision'],\n",
        "    'recall (macro)': rf_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': rf_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, rf_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, rf_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"\\nResultados acumulados até agora:\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oirSDRZibcG",
        "outputId": "842d9100-c1eb-47a7-81bd-d367a1d26042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# instalações\n",
        "!pip install tensorflow\n",
        "\n",
        "# bibliotecas para a CNN\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s0C8wz0nQSuc",
        "outputId": "49a25203-a7d2-47c7-b091-5b59469cad0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\n",
            "\n",
            "==================================================\n",
            "Treinando LightGBM...\n",
            "[LightGBM] [Info] Number of positive: 1794, number of negative: 23775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.263833 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 28584\n",
            "[LightGBM] [Info] Number of data points in the train set: 25569, number of used features: 1119\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070163 -> initscore=-2.584187\n",
            "[LightGBM] [Info] Start training from score -2.584187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avaliação do LightGBM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.99      0.97      5945\n",
            "           1       0.79      0.34      0.48       448\n",
            "\n",
            "    accuracy                           0.95      6393\n",
            "   macro avg       0.87      0.67      0.73      6393\n",
            "weighted avg       0.94      0.95      0.94      6393\n",
            "\n",
            "Acurácia:  0.9475989363366182\n",
            "Weighted F1-score: 0.9378357517455937\n",
            "AUC: 0.8625382974888863\n",
            "--------------------------------------------------\n",
            "Treinando Rede Neural Densa (MLP)...\n",
            "\n",
            "Treinando MLP...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 37ms/step - accuracy: 0.9229 - loss: 0.2954 - val_accuracy: 0.9404 - val_loss: 0.1822\n",
            "Epoch 2/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - accuracy: 0.9476 - loss: 0.1446 - val_accuracy: 0.9484 - val_loss: 0.1727\n",
            "Epoch 3/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.9597 - loss: 0.1124 - val_accuracy: 0.9499 - val_loss: 0.1743\n",
            "Epoch 4/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.9701 - loss: 0.0871 - val_accuracy: 0.9504 - val_loss: 0.1817\n",
            "Epoch 5/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 36ms/step - accuracy: 0.9770 - loss: 0.0710 - val_accuracy: 0.9520 - val_loss: 0.1964\n",
            "Epoch 6/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.9830 - loss: 0.0549 - val_accuracy: 0.9493 - val_loss: 0.2256\n",
            "Epoch 7/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.9876 - loss: 0.0410 - val_accuracy: 0.9514 - val_loss: 0.2500\n",
            "Epoch 8/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 38ms/step - accuracy: 0.9896 - loss: 0.0328 - val_accuracy: 0.9485 - val_loss: 0.2533\n",
            "Epoch 9/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.9913 - loss: 0.0295 - val_accuracy: 0.9501 - val_loss: 0.2865\n",
            "Epoch 10/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 38ms/step - accuracy: 0.9924 - loss: 0.0237 - val_accuracy: 0.9501 - val_loss: 0.2955\n",
            "Epoch 11/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 32ms/step - accuracy: 0.9934 - loss: 0.0220 - val_accuracy: 0.9496 - val_loss: 0.3008\n",
            "Epoch 12/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - accuracy: 0.9938 - loss: 0.0199 - val_accuracy: 0.9490 - val_loss: 0.3196\n",
            "Epoch 13/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 35ms/step - accuracy: 0.9940 - loss: 0.0196 - val_accuracy: 0.9503 - val_loss: 0.3350\n",
            "Epoch 14/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.9956 - loss: 0.0151 - val_accuracy: 0.9499 - val_loss: 0.3342\n",
            "Epoch 15/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 36ms/step - accuracy: 0.9952 - loss: 0.0169 - val_accuracy: 0.9489 - val_loss: 0.3380\n",
            "Epoch 16/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.9948 - loss: 0.0179 - val_accuracy: 0.9495 - val_loss: 0.3553\n",
            "Epoch 17/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - accuracy: 0.9955 - loss: 0.0144 - val_accuracy: 0.9512 - val_loss: 0.3827\n",
            "Epoch 18/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.9952 - loss: 0.0162 - val_accuracy: 0.9492 - val_loss: 0.3596\n",
            "Epoch 19/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - accuracy: 0.9968 - loss: 0.0131 - val_accuracy: 0.9457 - val_loss: 0.3718\n",
            "Epoch 20/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - accuracy: 0.9959 - loss: 0.0156 - val_accuracy: 0.9463 - val_loss: 0.3880\n",
            "\n",
            "Resumo do Modelo MLP:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,280,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,939,845\u001b[0m (15.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,939,845</span> (15.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,281\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,281</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,626,564\u001b[0m (10.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,626,564</span> (10.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando MLP no conjunto de teste...\n",
            "\n",
            "Acurácia da MLP no conjunto de teste: 0.9463\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
            "\n",
            "Avaliação completa da MLP:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97      5945\n",
            "           1       0.64      0.53      0.58       448\n",
            "\n",
            "    accuracy                           0.95      6393\n",
            "   macro avg       0.80      0.75      0.78      6393\n",
            "weighted avg       0.94      0.95      0.94      6393\n",
            "\n",
            "Weighted F1-score: 0.9439306639414655\n",
            "AUC: 0.8655735612159077\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# --- Modelos Avançados com TF-IDF ---\n",
        "\n",
        "# 5. LightGBM (Gradient Boosting)\n",
        "!pip install lightgbm\n",
        "\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Treinando LightGBM...\")\n",
        "\n",
        "# Para problema binário, num_classes será 2\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# converte matriz esparsa para o formato LightGBM\n",
        "# uso de objective='binary' para classificação binária\n",
        "lgb_model = lgb.LGBMClassifier(objective='binary', random_state=42)\n",
        "lgb_model.fit(X_train, y_train)\n",
        "lgb_predictions = lgb_model.predict(X_test)\n",
        "lgb_predictions_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do LightGBM:\")\n",
        "print(classification_report(y_test, lgb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lgb_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, lgb_predictions, average='weighted'))\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    # Para AUC em problema binário, use y_test (1D) e as probabilidades da classe positiva (1D)\n",
        "    lgb_auc_score = roc_auc_score(y_test, lgb_predictions_proba)\n",
        "    print(\"AUC:\", lgb_auc_score)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    lgb_auc_score = None\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "lgb_report = classification_report(y_test, lgb_predictions, output_dict=True)\n",
        "results['LightGBM'] = {\n",
        "    'accuracy': accuracy_score(y_test, lgb_predictions),\n",
        "    'precision (macro)': lgb_report['macro avg']['precision'],\n",
        "    'recall (macro)': lgb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lgb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lgb_predictions, average='weighted'),\n",
        "    'auc': lgb_auc_score\n",
        "}\n",
        "\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 6. Rede Neural Densa (MLP) com TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "print(\"Treinando Rede Neural Densa (MLP)...\")\n",
        "\n",
        "\n",
        "num_classes_mlp = len(np.unique(y_train))\n",
        "\n",
        "mlp_model = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    # Para classificação binária com saída de probabilidade única, uso de 1 unidade com ativação 'sigmoid'\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "# uso de 'binary_crossentropy' para a função de perda com a última camada 'sigmoid'\n",
        "mlp_model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "epochs_mlp = 20\n",
        "batch_size_mlp = 64\n",
        "\n",
        "\n",
        "print(\"\\nTreinando MLP...\")\n",
        "history_mlp = mlp_model.fit(X_train, y_train,\n",
        "                          epochs=epochs_mlp,\n",
        "                          batch_size=batch_size_mlp,\n",
        "                          validation_data=(X_test, y_test),\n",
        "                          verbose=1)\n",
        "\n",
        "\n",
        "print(\"\\nResumo do Modelo MLP:\")\n",
        "mlp_model.summary()\n",
        "\n",
        "# avaliação do Modelo MLP\n",
        "print(\"\\nAvaliando MLP no conjunto de teste...\")\n",
        "loss_mlp, accuracy_mlp = mlp_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da MLP no conjunto de teste: {accuracy_mlp:.4f}\")\n",
        "\n",
        "# gera previsões de probabilidade (saída sigmoid é a probabilidade da classe positiva)\n",
        "mlp_predictions_proba_positive = mlp_model.predict(X_test)\n",
        "\n",
        "# obtém as classes preditas (limiar 0.5)\n",
        "mlp_predictions = (mlp_predictions_proba_positive >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da MLP:\")\n",
        "print(classification_report(y_test, mlp_predictions))\n",
        "\n",
        "# cálculo de F1-score ponderado para MLP\n",
        "mlp_weighted_f1 = f1_score(y_test, mlp_predictions, average='weighted')\n",
        "print(\"Weighted F1-score:\", mlp_weighted_f1)\n",
        "\n",
        "# cálculo de AUC (para problema binário)\n",
        "try:\n",
        "    mlp_auc_score = roc_auc_score(y_test, mlp_predictions_proba_positive)\n",
        "    print(\"AUC:\", mlp_auc_score)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    mlp_auc_score = None\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "mlp_report = classification_report(y_test, mlp_predictions, output_dict=True)\n",
        "results['MLP (TF-IDF)'] = {\n",
        "    'accuracy': accuracy_mlp,\n",
        "    'precision (macro)': mlp_report['macro avg']['precision'],\n",
        "    'recall (macro)': mlp_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': mlp_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': mlp_weighted_f1,\n",
        "    'auc': mlp_auc_score\n",
        "}\n",
        "\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaGFQTW_dhin"
      },
      "source": [
        "#### **Modelos de DL**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para explorar abordagens mais profundas de aprendizado, foi implementada **uma rede neural convolucional (CNN)** voltada para a classificação de textos. Como esse tipo de modelo trabalha melhor com sequências de palavras em vez de vetores TF-IDF, os tweets foram tokenizados e convertidos em **sequências inteiras**, com padding para garantir um comprimento fixo.\n",
        "\n",
        "O modelo foi construído com uma camada de embedding (para mapear palavras em vetores densos), seguida por uma **camada convolucional 1D** que captura padrões locais no texto e uma **camada de pooling** que extrai as informações mais relevantes. Camadas densas e dropout foram adicionadas para refinar o aprendizado e reduzir overfitting. O modelo foi treinado por 10 épocas e avaliado com base em métricas como acurácia, F1-score e AUC. Essa arquitetura permite capturar melhor a estrutura local e semântica dos textos, sendo especialmente útil para dados curtos e ruidosos como tweets."
      ],
      "metadata": {
        "id": "f6RaD9nWFzIM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YeetVhS1NMOM",
        "outputId": "40be1e95-7258-4e8a-8620-819111ba1300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Exemplo de sequências padded (Treino):\n",
            "[[  22 9294 2872 2396  142 6435    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [  81 1538   17  254  376   21  836 6436 3621 9295    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [  26 2873    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   6    3   24    3    7   42    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [1930 1030    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "\n",
            "Forma das sequências padded (Treino): (31962, 100)\n",
            "\n",
            "Exemplo de sequências padded (Teste):\n",
            "[[  71    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [  23   86   19   12  559    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [ 504   29  979    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    1  181  207 6046  240  170    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [ 824  104 2886    1    1 2315 3591    6  166    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "\n",
            "Forma das sequências padded (Teste): (17197, 100)\n",
            "\n",
            "Labels originais de Treino: [0 1]\n",
            "Labels de Treino codificadas: [0 0 0 ... 0 1 0]\n",
            "Classes conhecidas pelo LabelEncoder: [0 1]\n",
            "\n",
            "Forma dos dados de treino para CNN (split do TREINO ORIGINAL): (25569, 100) (25569,)\n",
            "Forma dos dados de validação para CNN (split do TREINO ORIGINAL): (6393, 100) (6393,)\n",
            "Forma dos dados de teste FINAL para CNN (dataset de TESTE ORIGINAL): (17197, 100)\n",
            "\n",
            "Treinando CNN...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 58ms/step - accuracy: 0.9248 - loss: 0.2498 - val_accuracy: 0.9499 - val_loss: 0.1576\n",
            "Epoch 2/10\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 55ms/step - accuracy: 0.9660 - loss: 0.0986 - val_accuracy: 0.9520 - val_loss: 0.1579\n",
            "Epoch 3/10\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 62ms/step - accuracy: 0.9862 - loss: 0.0460 - val_accuracy: 0.9551 - val_loss: 0.2079\n",
            "Epoch 4/10\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 58ms/step - accuracy: 0.9944 - loss: 0.0226 - val_accuracy: 0.9485 - val_loss: 0.2629\n",
            "Epoch 5/10\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 53ms/step - accuracy: 0.9965 - loss: 0.0132 - val_accuracy: 0.9463 - val_loss: 0.3401\n",
            "Epoch 6/10\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 55ms/step - accuracy: 0.9970 - loss: 0.0104 - val_accuracy: 0.9463 - val_loss: 0.3448\n",
            "Epoch 7/10\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 56ms/step - accuracy: 0.9978 - loss: 0.0083 - val_accuracy: 0.9476 - val_loss: 0.4247\n",
            "Epoch 8/10\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 55ms/step - accuracy: 0.9973 - loss: 0.0091 - val_accuracy: 0.9415 - val_loss: 0.4387\n",
            "Epoch 9/10\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 55ms/step - accuracy: 0.9977 - loss: 0.0077 - val_accuracy: 0.9471 - val_loss: 0.5338\n",
            "Epoch 10/10\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 57ms/step - accuracy: 0.9972 - loss: 0.0096 - val_accuracy: 0.9415 - val_loss: 0.5089\n",
            "\n",
            "Resumo do Modelo CNN:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m49,280\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,037,765\u001b[0m (15.40 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,037,765</span> (15.40 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,345,921\u001b[0m (5.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,345,921</span> (5.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,691,844\u001b[0m (10.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,691,844</span> (10.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando CNN no conjunto de validação...\n",
            "\n",
            "Acurácia da CNN no conjunto de validação: 0.9415\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
            "\n",
            "Avaliação completa da CNN no conjunto de validação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97      5945\n",
            "           1       0.59      0.56      0.57       448\n",
            "\n",
            "    accuracy                           0.94      6393\n",
            "   macro avg       0.78      0.77      0.77      6393\n",
            "weighted avg       0.94      0.94      0.94      6393\n",
            "\n",
            "Weighted F1-score: 0.9408798084526357\n",
            "AUC: 0.8630799065841643\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# --- pré-processamento para CNN/LSTM (Baseado nos datasets ORIGINAIS) ---\n",
        "\n",
        "max_words = 10000\n",
        "max_sequence_length = 100\n",
        "\n",
        "# --- tokenização ---\n",
        "train_texts = df_train['processed_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "test_texts = df_test['processed_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "# seta o tokenizer SOMENTE nos textos de TREINO\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# converte textos para sequências de inteiros\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# --- Padding das Sequências ---\n",
        "train_padded_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_padded_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "print(\"\\nExemplo de sequências padded (Treino):\")\n",
        "print(train_padded_sequences[:5])\n",
        "print(\"\\nForma das sequências padded (Treino):\", train_padded_sequences.shape)\n",
        "\n",
        "print(\"\\nExemplo de sequências padded (Teste):\")\n",
        "print(test_padded_sequences[:5])\n",
        "print(\"\\nForma das sequências padded (Teste):\", test_padded_sequences.shape)\n",
        "\n",
        "\n",
        "# --- prepara labels para CNN/LSTM ---\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "# seta o LabelEncoder SOMENTE nas labels de TREINO\n",
        "encoded_y_train_original = label_encoder.fit_transform(df_train['label'])\n",
        "\n",
        "print(\"\\nLabels originais de Treino:\", df_train['label'].unique())\n",
        "print(\"Labels de Treino codificadas:\", encoded_y_train_original)\n",
        "print(\"Classes conhecidas pelo LabelEncoder:\", label_encoder.classes_)\n",
        "\n",
        "# --- divide dados para CNN/LSTM (usando as sequências e labels codificadas DO TREINO ORIGINAL) ---\n",
        "\n",
        "X_cnn_train_val = train_padded_sequences\n",
        "y_cnn_train_val = encoded_y_train_original\n",
        "\n",
        "# divisão dos dados de TREINO ORIGINAL em conjuntos de TREINO e VALIDAÇÃO para CNN/LSTM\n",
        "X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(\n",
        "    X_cnn_train_val, y_cnn_train_val, test_size=0.20, random_state=42, stratify=y_cnn_train_val\n",
        ")\n",
        "\n",
        "# O dataset de TESTE ORIGINAL (df_test) já está vetorizado/padded em test_padded_sequences.\n",
        "# nomeado de X_test_cnn_final para clareza.\n",
        "\n",
        "X_test_cnn_final = test_padded_sequences\n",
        "\n",
        "print(\"\\nForma dos dados de treino para CNN (split do TREINO ORIGINAL):\", X_train_cnn.shape, y_train_cnn.shape)\n",
        "print(\"Forma dos dados de validação para CNN (split do TREINO ORIGINAL):\", X_val_cnn.shape, y_val_cnn.shape)\n",
        "print(\"Forma dos dados de teste FINAL para CNN (dataset de TESTE ORIGINAL):\", X_test_cnn_final.shape)\n",
        "\n",
        "\n",
        "# --- constrói o Modelo CNN ---\n",
        "\n",
        "embedding_dim = 128\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_filters = 128\n",
        "\n",
        "# como o problema é binário, podemos usar 1 unidade com sigmoid para saída de probabilidade única\n",
        "num_classes_cnn = len(label_encoder.classes_)\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    # camada de Embedding: mapeia palavras (índices) para vetores densos\n",
        "    # input_dim = Vocabulário total (max_words + 1 para OOV ou apenas max_words se oov_token não for contado separadamente)\n",
        "    # Keras Tokenizer inclui o OOV token no num_words se especificado, então o tamanho é num_words\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "\n",
        "    # Para simplificar, vamos usar uma única camada Conv e depois MaxPooling.\n",
        "    Conv1D(filters=num_filters, kernel_size=filter_sizes[0], activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "\n",
        "    # Camada Densa (Fully Connected)\n",
        "    Dense(128, activation='relu'),\n",
        "    # Camada de Dropout\n",
        "    Dropout(0.5),\n",
        "    # Camada de saída: 1 unidade com ativação 'sigmoid' para classificação binária\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# --- treino do Modelo CNN ---\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "print(\"\\nTreinando CNN...\")\n",
        "history = model.fit(X_train_cnn, y_train_cnn,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(X_val_cnn, y_val_cnn),\n",
        "                    verbose=1)\n",
        "\n",
        "print(\"\\nResumo do Modelo CNN:\")\n",
        "model.summary()\n",
        "\n",
        "# --- Avaliação o Modelo CNN (no conjunto de VALIDAÇÃO) ---\n",
        "print(\"\\nAvaliando CNN no conjunto de validação...\")\n",
        "loss_cnn_val, accuracy_cnn_val = model.evaluate(X_val_cnn, y_val_cnn, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da CNN no conjunto de validação: {accuracy_cnn_val:.4f}\")\n",
        "\n",
        "cnn_predictions_proba_positive_val = model.predict(X_val_cnn)\n",
        "\n",
        "# obtém as classes preditas (limiar 0.5)\n",
        "cnn_predictions_val = (cnn_predictions_proba_positive_val >= 0.5).astype(int).flatten()\n",
        "print(\"\\nAvaliação completa da CNN no conjunto de validação:\")\n",
        "print(classification_report(y_val_cnn, cnn_predictions_val))\n",
        "\n",
        "# cálculo de F1-score ponderado para CNN\n",
        "cnn_weighted_f1_val = f1_score(y_val_cnn, cnn_predictions_val, average='weighted')\n",
        "print(\"Weighted F1-score:\", cnn_weighted_f1_val)\n",
        "\n",
        "# cálculo AUC para CNN (problema binário)\n",
        "try:\n",
        "    cnn_auc_score_val = roc_auc_score(y_val_cnn, cnn_predictions_proba_positive_val)\n",
        "    print(\"AUC:\", cnn_auc_score_val)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    cnn_auc_score_val = None\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "# dicionário de resultados\n",
        "\n",
        "cnn_report_val = classification_report(y_val_cnn, cnn_predictions_val, output_dict=True)\n",
        "results['CNN (Embedding+Seq) Validation'] = {\n",
        "    'accuracy': accuracy_cnn_val,\n",
        "    'precision (macro)': cnn_report_val['macro avg']['precision'],\n",
        "    'recall (macro)': cnn_report_val['macro avg']['recall'],\n",
        "    'f1-score (macro)': cnn_report_val['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': cnn_weighted_f1_val,\n",
        "    'auc': cnn_auc_score_val\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5Rswna5mV5bO",
        "outputId": "25b950fc-223e-45f4-c90f-2e2c0108deb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Construindo o Modelo LSTM...\n",
            "\n",
            "Treinando LSTM...\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 314ms/step - accuracy: 0.9276 - loss: 0.2874 - val_accuracy: 0.9299 - val_loss: 0.2546\n",
            "Epoch 2/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 295ms/step - accuracy: 0.9302 - loss: 0.2656 - val_accuracy: 0.9299 - val_loss: 0.2539\n",
            "Epoch 3/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 284ms/step - accuracy: 0.9327 - loss: 0.2562 - val_accuracy: 0.9299 - val_loss: 0.2540\n",
            "Epoch 4/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 288ms/step - accuracy: 0.9306 - loss: 0.2607 - val_accuracy: 0.9299 - val_loss: 0.2548\n",
            "Epoch 5/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 289ms/step - accuracy: 0.9297 - loss: 0.2616 - val_accuracy: 0.9299 - val_loss: 0.2552\n",
            "Epoch 6/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 280ms/step - accuracy: 0.9306 - loss: 0.2586 - val_accuracy: 0.9299 - val_loss: 0.2539\n",
            "Epoch 7/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 285ms/step - accuracy: 0.9283 - loss: 0.2629 - val_accuracy: 0.9299 - val_loss: 0.2590\n",
            "Epoch 8/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 283ms/step - accuracy: 0.9320 - loss: 0.2544 - val_accuracy: 0.9299 - val_loss: 0.2543\n",
            "Epoch 9/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 292ms/step - accuracy: 0.9320 - loss: 0.2533 - val_accuracy: 0.9299 - val_loss: 0.2540\n",
            "Epoch 10/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 289ms/step - accuracy: 0.9310 - loss: 0.2552 - val_accuracy: 0.9299 - val_loss: 0.2539\n",
            "Epoch 11/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 286ms/step - accuracy: 0.9292 - loss: 0.2605 - val_accuracy: 0.9299 - val_loss: 0.2539\n",
            "Epoch 12/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 295ms/step - accuracy: 0.9294 - loss: 0.2579 - val_accuracy: 0.9299 - val_loss: 0.2542\n",
            "Epoch 13/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 292ms/step - accuracy: 0.9283 - loss: 0.2611 - val_accuracy: 0.9299 - val_loss: 0.2540\n",
            "Epoch 14/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 296ms/step - accuracy: 0.9290 - loss: 0.2586 - val_accuracy: 0.9299 - val_loss: 0.2543\n",
            "Epoch 15/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 282ms/step - accuracy: 0.9301 - loss: 0.2560 - val_accuracy: 0.9299 - val_loss: 0.2543\n",
            "\n",
            "Resumo do Modelo LSTM:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,259,717\u001b[0m (16.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,259,717</span> (16.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,419,905\u001b[0m (5.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,419,905</span> (5.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,839,812\u001b[0m (10.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,839,812</span> (10.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando LSTM no conjunto de validação...\n",
            "\n",
            "Acurácia da LSTM no conjunto de validação: 0.9299\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 85ms/step\n",
            "\n",
            "Avaliação completa da LSTM no conjunto de validação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      1.00      0.96      5945\n",
            "           1       0.00      0.00      0.00       448\n",
            "\n",
            "    accuracy                           0.93      6393\n",
            "   macro avg       0.46      0.50      0.48      6393\n",
            "weighted avg       0.86      0.93      0.90      6393\n",
            "\n",
            "Weighted F1-score: 0.8961572925201334\n",
            "AUC: 0.5004432371140214\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# --- reutiliza dados preparados para a CNN ---\n",
        "\n",
        "\n",
        "# --- constrói o Modelo LSTM ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Construindo o Modelo LSTM...\")\n",
        "\n",
        "# como o problema é binário, a última camada da LSTM deve ter 1 unidade com ativação 'sigmoid'.\n",
        "num_classes_lstm = len(label_encoder.classes_)\n",
        "\n",
        "model_lstm = Sequential([\n",
        "    # camada de Embedding: reutiliza os mesmos parâmetros da CNN para consistência na base de embedding\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "\n",
        "    # Camada LSTM\n",
        "    LSTM(128),\n",
        "\n",
        "    # Camada de Dropout\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Camada Densa (Fully Connected)\n",
        "    Dense(64, activation='relu'), # Camada densa adicional\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Camada de saída: 1 unidade com ativação 'sigmoid' para classificação binária\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_lstm.compile(optimizer='adam',\n",
        "                   loss='binary_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# --- treino do Modelo LSTM ---\n",
        "epochs_lstm = 15\n",
        "batch_size_lstm = 32\n",
        "\n",
        "print(\"\\nTreinando LSTM...\")\n",
        "\n",
        "history_lstm = model_lstm.fit(X_train_cnn, y_train_cnn,\n",
        "                              epochs=epochs_lstm,\n",
        "                              batch_size=batch_size_lstm,\n",
        "                              validation_data=(X_val_cnn, y_val_cnn),\n",
        "                              verbose=1)\n",
        "\n",
        "print(\"\\nResumo do Modelo LSTM:\")\n",
        "model_lstm.summary()\n",
        "\n",
        "# --- avaliação do Modelo LSTM (no conjunto de VALIDAÇÃO) ---\n",
        "print(\"\\nAvaliando LSTM no conjunto de validação...\")\n",
        "\n",
        "loss_lstm_val, accuracy_lstm_val = model_lstm.evaluate(X_val_cnn, y_val_cnn, verbose=0)\n",
        "print(f\"\\nAcurácia da LSTM no conjunto de validação: {accuracy_lstm_val:.4f}\")\n",
        "\n",
        "# gera previsões (classes) e probabilidades para calcular métricas\n",
        "# saída da camada Dense(1, 'sigmoid') é a probabilidade da classe positiva, forma (n_samples, 1)\n",
        "lstm_predictions_proba_positive_val = model_lstm.predict(X_val_cnn)\n",
        "\n",
        "# obtém as classes preditas\n",
        "lstm_predictions_val = (lstm_predictions_proba_positive_val >= 0.5).astype(int).flatten()\n",
        "print(\"\\nAvaliação completa da LSTM no conjunto de validação:\")\n",
        "print(classification_report(y_val_cnn, lstm_predictions_val))\n",
        "\n",
        "\n",
        "# cálculo de F1-score ponderado para LSTM\n",
        "lstm_weighted_f1_val = f1_score(y_val_cnn, lstm_predictions_val, average='weighted')\n",
        "print(\"Weighted F1-score:\", lstm_weighted_f1_val)\n",
        "\n",
        "# cálculo de AUC para LSTM (problema binário)\n",
        "try:\n",
        "    lstm_auc_score_val = roc_auc_score(y_val_cnn, lstm_predictions_proba_positive_val)\n",
        "    print(\"AUC:\", lstm_auc_score_val)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    lstm_auc_score_val = None\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "# dicionário de resultados\n",
        "\n",
        "lstm_report_val = classification_report(y_val_cnn, lstm_predictions_val, output_dict=True) # Não precisamos de target_names aqui\n",
        "\n",
        "results['LSTM (Embedding+Seq) Validation'] = {\n",
        "    'accuracy': accuracy_lstm_val,\n",
        "    'precision (macro)': lstm_report_val['macro avg']['precision'],\n",
        "    'recall (macro)': lstm_report_val['macro avg']['recall'],\n",
        "    'f1-score (macro)': lstm_report_val['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': lstm_weighted_f1_val,\n",
        "    'auc': lstm_auc_score_val\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGQsnDqrwSlJ"
      },
      "source": [
        "### **Balanceamento do dataset e retreino dos modelos**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Class_weight**"
      ],
      "metadata": {
        "id": "DC2dbirgsFGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta etapa do experimento, foi aplicado o **balanceamento de classes por meio do parâmetro class_weight**, com o objetivo de mitigar o impacto do desbalanceamento dos dados na performance dos modelos.\n",
        "\n",
        "Utilizando a função `compute_class_weight` da biblioteca `sklearn`, foram calculados pesos proporcionais à frequência das classes no conjunto de treinamento. Esses pesos foram incorporados diretamente ao processo de treinamento em modelos que suportam esse recurso, como **Regressão Logística, SVM, Random Forest, LightGBM e Redes Neurais com Keras**.\n",
        "\n",
        "A inclusão dos pesos penaliza erros cometidos em classes minoritárias, forçando os algoritmos a considerarem com maior atenção esses exemplos menos frequentes. Essa estratégia é especialmente eficaz em cenários de classificação desbalanceada, como o de detecção de discursos ofensivos, contribuindo para **melhorias em métricas como recall e F1-score das classes minoritárias, sem necessariamente comprometer a acurácia geral**.\n",
        "\n",
        "Modelos como o Naive Bayes Multinomial, que não oferecem suporte direto a class_weight, foram mantidos como referência sem ajuste neste momento."
      ],
      "metadata": {
        "id": "_P2enMvoIest"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, f1_score\n",
        "import pandas as pd\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"Treinando modelos tradicionais COM Class Weight...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 1. Regressão Logística\n",
        "print(\"Treinando Regressão Logística (Class Weight)...\")\n",
        "lr_model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42) # random_state para reprodutibilidade\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "lr_predictions_proba = lr_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação da Regressão Logística (Class Weight):\")\n",
        "print(classification_report(y_test, lr_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lr_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, lr_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, lr_predictions_proba))\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "lr_report = classification_report(y_test, lr_predictions, output_dict=True)\n",
        "results['Logistic Regression (Class Weight)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lr_predictions),\n",
        "    'precision (macro)': lr_report['macro avg']['precision'],\n",
        "    'recall (macro)': lr_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lr_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lr_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, lr_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 2. Naive Bayes Multinomial\n",
        "# Nota: MultinomialNB NÃO tem o parâmetro class_weight.\n",
        "# Naive Bayes já lida com a frequência das classes inerentemente na sua formulação de probabilidade.\n",
        "# Aplicar class_weight a ele não é diretamente possível via parâmetro.\n",
        "# O modelo será treinado sem class_weight como no código original e adicionado ao dicionário\n",
        "\n",
        "print(\"Treinando Naive Bayes Multinomial (Não suporta class_weight)...\")\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_predictions = nb_model.predict(X_test)\n",
        "nb_predictions_proba = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do Naive Bayes Multinomial:\")\n",
        "print(classification_report(y_test, nb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, nb_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, nb_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, nb_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "nb_report = classification_report(y_test, nb_predictions, output_dict=True)\n",
        "results['Multinomial NB'] = {\n",
        "    'accuracy': accuracy_score(y_test, nb_predictions),\n",
        "    'precision (macro)': nb_report['macro avg']['precision'],\n",
        "    'recall (macro)': nb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': nb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, nb_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, nb_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 3. Support Vector Machine (SVM)\n",
        "print(\"Treinando SVM (Kernel Linear) (Class Weight)...\")\n",
        "svm_model = SVC(kernel='linear', probability=True, class_weight='balanced', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_predictions = svm_model.predict(X_test)\n",
        "svm_predictions_proba = svm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do SVM (Class Weight):\")\n",
        "print(classification_report(y_test, svm_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, svm_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, svm_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, svm_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "svm_report = classification_report(y_test, svm_predictions, output_dict=True)\n",
        "results['SVM (Class Weight)'] = {\n",
        "    'accuracy': accuracy_score(y_test, svm_predictions),\n",
        "    'precision (macro)': svm_report['macro avg']['precision'],\n",
        "    'recall (macro)': svm_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': svm_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, svm_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, svm_predictions_proba)\n",
        "}\n",
        "\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 4. Random Forest Classifier\n",
        "print(\"Treinando Random Forest (Class Weight)...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "rf_predictions_proba = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do Random Forest (Class Weight):\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, rf_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, rf_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, rf_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "rf_report = classification_report(y_test, rf_predictions, output_dict=True)\n",
        "results['Random Forest (Class Weight)'] = {\n",
        "    'accuracy': accuracy_score(y_test, rf_predictions),\n",
        "    'precision (macro)': rf_report['macro avg']['precision'],\n",
        "    'recall (macro)': rf_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': rf_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, rf_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, rf_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"\\nResultados acumulados (incluindo Class Weight) até agora:\")\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waFbBJrlhJ1L",
        "outputId": "10462923-417f-4d9f-c97e-1e4b90f52424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando modelos tradicionais COM Class Weight...\n",
            "--------------------------------------------------\n",
            "Treinando Regressão Logística (Class Weight)...\n",
            "Avaliação da Regressão Logística (Class Weight):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.91      0.94      5945\n",
            "           1       0.36      0.71      0.48       448\n",
            "\n",
            "    accuracy                           0.89      6393\n",
            "   macro avg       0.67      0.81      0.71      6393\n",
            "weighted avg       0.93      0.89      0.91      6393\n",
            "\n",
            "Acurácia:  0.8911309244486156\n",
            "Weighted F1-score: 0.9067862044203401\n",
            "AUC: 0.8983885017421603\n",
            "--------------------------------------------------\n",
            "Treinando Naive Bayes Multinomial (Não suporta class_weight)...\n",
            "Avaliação do Naive Bayes Multinomial:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97      5945\n",
            "           1       0.96      0.23      0.38       448\n",
            "\n",
            "    accuracy                           0.95      6393\n",
            "   macro avg       0.95      0.62      0.67      6393\n",
            "weighted avg       0.95      0.95      0.93      6393\n",
            "\n",
            "Acurácia:  0.9457218833098702\n",
            "Weighted F1-score: 0.9299568965040391\n",
            "AUC: 0.8658604169169769\n",
            "--------------------------------------------------\n",
            "Treinando SVM (Kernel Linear) (Class Weight)...\n",
            "Avaliação do SVM (Class Weight):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.90      0.94      5945\n",
            "           1       0.34      0.69      0.45       448\n",
            "\n",
            "    accuracy                           0.88      6393\n",
            "   macro avg       0.66      0.79      0.69      6393\n",
            "weighted avg       0.93      0.88      0.90      6393\n",
            "\n",
            "Acurácia:  0.8839355545127483\n",
            "Weighted F1-score: 0.9012864967288586\n",
            "AUC: 0.8861920656614202\n",
            "--------------------------------------------------\n",
            "Treinando Random Forest (Class Weight)...\n",
            "Avaliação do Random Forest (Class Weight):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.98      5945\n",
            "           1       0.79      0.49      0.61       448\n",
            "\n",
            "    accuracy                           0.96      6393\n",
            "   macro avg       0.88      0.74      0.79      6393\n",
            "weighted avg       0.95      0.96      0.95      6393\n",
            "\n",
            "Acurácia:  0.9554199906147348\n",
            "Weighted F1-score: 0.9505507893701322\n",
            "AUC: 0.8904027243782291\n",
            "\n",
            "Resultados acumulados (incluindo Class Weight) até agora:\n",
            "{'Logistic Regression (Class Weight)': {'accuracy': 0.8911309244486156, 'precision (macro)': 0.6675737512985835, 'recall (macro)': 0.8052437522527935, 'f1-score (macro)': 0.7075800145949481, 'f1-score (weighted)': 0.9067862044203401, 'auc': np.float64(0.8983885017421603)}, 'Multinomial NB': {'accuracy': 0.9457218833098702, 'precision (macro)': 0.9543598420920467, 'recall (macro)': 0.6168510828427249, 'f1-score (macro)': 0.6743222874431132, 'f1-score (weighted)': 0.9299568965040391, 'auc': np.float64(0.8658604169169769)}, 'SVM (Class Weight)': {'accuracy': 0.8839355545127483, 'precision (macro)': 0.6561996273376731, 'recall (macro)': 0.7920872506908566, 'f1-score (macro)': 0.6939427372463398, 'f1-score (weighted)': 0.9012864967288586, 'auc': np.float64(0.8861920656614202)}, 'Random Forest (Class Weight)': {'accuracy': 0.9554199906147348, 'precision (macro)': 0.8774933960837281, 'recall (macro)': 0.7417737369337979, 'f1-score (macro)': 0.7921720956329683, 'f1-score (weighted)': 0.9505507893701322, 'auc': np.float64(0.8904027243782291)}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Modelos Avançados com TF-IDF (com Class Weight) ---\n",
        "\n",
        "# importação de bibliotecas\n",
        "import lightgbm as lgb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight # Importar a função\n",
        "\n",
        "\n",
        "# --- cálculo de Class Weights ---\n",
        "# os pesos das classes baseados na distribuição das classes no conjunto de treino original (y_train)\n",
        "# 'balanced' ajusta os pesos inversamente proporcionais à frequência da classe.\n",
        "classes = np.unique(y_train)\n",
        "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
        "class_weight_dict = dict(zip(classes, class_weights))\n",
        "\n",
        "print(\"\\nPesos das classes para TF-IDF (Class Weight):\")\n",
        "print(class_weight_dict)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Treinando Modelos Avançados com TF-IDF usando CLASS WEIGHT...\")\n",
        "\n",
        "# 5. LightGBM (Gradient Boosting) (com Class Weight)\n",
        "print(\"Treinando LightGBM com Class Weight...\")\n",
        "\n",
        "lgb_model_cw = lgb.LGBMClassifier(objective='binary', random_state=42, class_weight=class_weight_dict)\n",
        "lgb_model_cw.fit(X_train, y_train)\n",
        "lgb_predictions_cw = lgb_model_cw.predict(X_test)\n",
        "lgb_predictions_proba_cw = lgb_model_cw.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do LightGBM (Class Weight):\")\n",
        "print(classification_report(y_test, lgb_predictions_cw))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lgb_predictions_cw))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, lgb_predictions_cw, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, lgb_predictions_proba_cw))\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "lgb_report_cw = classification_report(y_test, lgb_predictions_cw, output_dict=True)\n",
        "results['LightGBM (Class Weight)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lgb_predictions_cw),\n",
        "    'precision (macro)': lgb_report_cw['macro avg']['precision'],\n",
        "    'recall (macro)': lgb_report_cw['macro avg']['recall'],\n",
        "    'f1-score (macro)': lgb_report_cw['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lgb_predictions_cw, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, lgb_predictions_proba_cw)\n",
        "}\n",
        "\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 6. Rede Neural Densa (MLP) com TensorFlow/Keras (com Class Weight)\n",
        "print(\"Treinando Rede Neural Densa (MLP) com Class Weight...\")\n",
        "\n",
        "# reutiliza a estrutura do modelo MLP\n",
        "# o número de classes será 2 para a saída sigmoid\n",
        "num_classes_mlp_cw = len(np.unique(y_train))\n",
        "print(f\"Número de classes em y_train para MLP (Class Weight): {num_classes_mlp_cw}\")\n",
        "\n",
        "mlp_model_cw = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "mlp_model_cw.compile(optimizer='adam',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# treino do Modelo MLP\n",
        "epochs_mlp_cw = 20\n",
        "batch_size_mlp_cw = 64\n",
        "\n",
        "\n",
        "print(\"\\nTreinando MLP (Class Weight)...\")\n",
        "\n",
        "\n",
        "history_mlp_cw = mlp_model_cw.fit(X_train, y_train,\n",
        "                                  epochs=epochs_mlp_cw,\n",
        "                                  batch_size=batch_size_mlp_cw,\n",
        "                                  validation_data=(X_test, y_test),\n",
        "                                  class_weight=class_weight_dict,\n",
        "                                  verbose=1)\n",
        "\n",
        "\n",
        "print(\"\\nResumo do Modelo MLP (Class Weight):\")\n",
        "mlp_model_cw.summary()\n",
        "\n",
        "# avaliação do Modelo MLP\n",
        "print(\"\\nAvaliando MLP (Class Weight) no conjunto de teste...\")\n",
        "loss_mlp_cw, accuracy_mlp_cw = mlp_model_cw.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da MLP (Class Weight) no conjunto de teste: {accuracy_mlp_cw:.4f}\")\n",
        "\n",
        "# gera previsões de probabilidade e classes\n",
        "mlp_predictions_proba_positive_cw = mlp_model_cw.predict(X_test)\n",
        "mlp_predictions_cw = (mlp_predictions_proba_positive_cw >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da MLP (Class Weight):\")\n",
        "print(classification_report(y_test, mlp_predictions_cw))\n",
        "\n",
        "mlp_weighted_f1_cw = f1_score(y_test, mlp_predictions_cw, average='weighted')\n",
        "print(\"Weighted F1-score:\", mlp_weighted_f1_cw)\n",
        "\n",
        "try:\n",
        "    mlp_auc_score_cw = roc_auc_score(y_test, mlp_predictions_proba_positive_cw)\n",
        "    print(\"AUC:\", mlp_auc_score_cw)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    mlp_auc_score_cw = None\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "mlp_report_cw = classification_report(y_test, mlp_predictions_cw, output_dict=True)\n",
        "results['MLP (Class Weight)'] = {\n",
        "    'accuracy': accuracy_mlp_cw,\n",
        "    'precision (macro)': mlp_report_cw['macro avg']['precision'],\n",
        "    'recall (macro)': mlp_report_cw['macro avg']['recall'],\n",
        "    'f1-score (macro)': mlp_report_cw['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': mlp_weighted_f1_cw,\n",
        "    'auc': mlp_auc_score_cw\n",
        "}\n",
        "\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SX_cvIpLkYMf",
        "outputId": "deeea6b9-a11a-47d1-b6bb-13944bb7e89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pesos das classes para TF-IDF (Class Weight):\n",
            "{np.int64(0): np.float64(0.5377287066246057), np.int64(1): np.float64(7.1262541806020065)}\n",
            "\n",
            "==================================================\n",
            "Treinando Modelos Avançados com TF-IDF usando CLASS WEIGHT...\n",
            "Treinando LightGBM com Class Weight...\n",
            "[LightGBM] [Info] Number of positive: 1794, number of negative: 23775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202329 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 28584\n",
            "[LightGBM] [Info] Number of data points in the train set: 25569, number of used features: 1119\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avaliação do LightGBM (Class Weight):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.90      0.93      5945\n",
            "           1       0.32      0.65      0.43       448\n",
            "\n",
            "    accuracy                           0.88      6393\n",
            "   macro avg       0.64      0.77      0.68      6393\n",
            "weighted avg       0.93      0.88      0.90      6393\n",
            "\n",
            "Acurácia:  0.8781479743469419\n",
            "Weighted F1-score: 0.8963882467427354\n",
            "AUC: 0.8658388276462814\n",
            "--------------------------------------------------\n",
            "Treinando Rede Neural Densa (MLP) com Class Weight...\n",
            "Número de classes em y_train para MLP (Class Weight): 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando MLP (Class Weight)...\n",
            "Epoch 1/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 28ms/step - accuracy: 0.8316 - loss: 0.6129 - val_accuracy: 0.7543 - val_loss: 0.4663\n",
            "Epoch 2/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.8305 - loss: 0.3341 - val_accuracy: 0.8275 - val_loss: 0.3551\n",
            "Epoch 3/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.8864 - loss: 0.2312 - val_accuracy: 0.8619 - val_loss: 0.2976\n",
            "Epoch 4/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 28ms/step - accuracy: 0.9187 - loss: 0.1577 - val_accuracy: 0.9088 - val_loss: 0.2718\n",
            "Epoch 5/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.9606 - loss: 0.1058 - val_accuracy: 0.9224 - val_loss: 0.2551\n",
            "Epoch 6/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9681 - loss: 0.0911 - val_accuracy: 0.9248 - val_loss: 0.2631\n",
            "Epoch 7/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.9756 - loss: 0.0771 - val_accuracy: 0.9340 - val_loss: 0.2524\n",
            "Epoch 8/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9782 - loss: 0.0669 - val_accuracy: 0.9337 - val_loss: 0.2631\n",
            "Epoch 9/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.9833 - loss: 0.0565 - val_accuracy: 0.9158 - val_loss: 0.3086\n",
            "Epoch 10/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9825 - loss: 0.0545 - val_accuracy: 0.9265 - val_loss: 0.2814\n",
            "Epoch 11/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.9840 - loss: 0.0588 - val_accuracy: 0.9382 - val_loss: 0.2853\n",
            "Epoch 12/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9863 - loss: 0.0434 - val_accuracy: 0.9360 - val_loss: 0.3059\n",
            "Epoch 13/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9882 - loss: 0.0441 - val_accuracy: 0.9402 - val_loss: 0.3076\n",
            "Epoch 14/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.9895 - loss: 0.0360 - val_accuracy: 0.9398 - val_loss: 0.3233\n",
            "Epoch 15/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.9893 - loss: 0.0406 - val_accuracy: 0.9388 - val_loss: 0.3442\n",
            "Epoch 16/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.9892 - loss: 0.0399 - val_accuracy: 0.9270 - val_loss: 0.3383\n",
            "Epoch 17/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.9896 - loss: 0.0420 - val_accuracy: 0.9373 - val_loss: 0.3624\n",
            "Epoch 18/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.9897 - loss: 0.0403 - val_accuracy: 0.9304 - val_loss: 0.3695\n",
            "Epoch 19/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.9909 - loss: 0.0348 - val_accuracy: 0.9396 - val_loss: 0.3721\n",
            "Epoch 20/20\n",
            "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 27ms/step - accuracy: 0.9884 - loss: 0.0422 - val_accuracy: 0.9385 - val_loss: 0.4043\n",
            "\n",
            "Resumo do Modelo MLP (Class Weight):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,280,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,939,845\u001b[0m (15.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,939,845</span> (15.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,281\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,281</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,626,564\u001b[0m (10.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,626,564</span> (10.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando MLP (Class Weight) no conjunto de teste...\n",
            "\n",
            "Acurácia da MLP (Class Weight) no conjunto de teste: 0.9385\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "\n",
            "Avaliação completa da MLP (Class Weight):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.96      0.97      5945\n",
            "           1       0.56      0.59      0.58       448\n",
            "\n",
            "    accuracy                           0.94      6393\n",
            "   macro avg       0.76      0.78      0.77      6393\n",
            "weighted avg       0.94      0.94      0.94      6393\n",
            "\n",
            "Weighted F1-score: 0.9394150008513594\n",
            "AUC: 0.8605885047458848\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importação de bibliotecas necessárias\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- reutiliza dados preparados anteriormente ---\n",
        "\n",
        "max_words = 10000\n",
        "max_sequence_length = 100\n",
        "\n",
        "# --- tokenização ---\n",
        "train_texts = df_train['processed_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# converte textos para sequências de inteiros\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "\n",
        "# --- padding das Sequências ---\n",
        "train_padded_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# --- prepara labels para CNN/LSTM ---\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_y_train_original = label_encoder.fit_transform(df_train['label'])\n",
        "\n",
        "# ---divide dados para CNN/LSTM (usando as sequências e labels codificadas) ---\n",
        "X_cnn_train_val = train_padded_sequences # Sequências\n",
        "y_cnn_train_val = encoded_y_train_original # Labels\n",
        "\n",
        "X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(\n",
        "    X_cnn_train_val, y_cnn_train_val, test_size=0.20, random_state=42, stratify=y_cnn_train_val\n",
        ")\n",
        "\n",
        "print(\"Variável y_train_cnn regenerada.\")\n",
        "print(\"Forma de y_train_cnn:\", y_train_cnn.shape)\n",
        "print(\"Primeiros 5 elementos de y_train_cnn:\", y_train_cnn[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUq2mWGtkS4s",
        "outputId": "48679d0e-9a7f-4990-aecf-e323de95193f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variável y_train_cnn regenerada.\n",
            "Forma de y_train_cnn: (25569,)\n",
            "Primeiros 5 elementos de y_train_cnn: [0 1 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeShXMUD4G-k",
        "outputId": "02462b75-a914-4866-c707-e038f0c0a4f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pesos das classes calculados:\n",
            "{np.int64(0): np.float64(0.5377287066246057), np.int64(1): np.float64(7.1262541806020065)}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# nomes das classes\n",
        "classes = np.unique(y_train_cnn)\n",
        "\n",
        "# cálculo dos pesos das classes\n",
        "# 'balanced' ajusta automaticamente os pesos inversamente proporcionais às frequências das classes\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_cnn)\n",
        "class_weight_dict = dict(zip(classes, class_weights))\n",
        "\n",
        "print(\"Pesos das classes calculados:\")\n",
        "print(class_weight_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UJU8f-Wj5M_C",
        "outputId": "3ec03b8e-5719-49d5-9022-7f07c023b9ca",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Exemplo de sequências padded (Treino):\n",
            "[[  22 9294 2872 2396  142 6435    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [  81 1538   17  254  376   21  836 6436 3621 9295    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [  26 2873    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   6    3   24    3    7   42    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [1930 1030    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "\n",
            "Forma das sequências padded (Treino): (31962, 100)\n",
            "\n",
            "Exemplo de sequências padded (Teste):\n",
            "[[  71    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [  23   86   19   12  559    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [ 504   29  979    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    1  181  207 6046  240  170    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [ 824  104 2886    1    1 2315 3591    6  166    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n",
            "\n",
            "Forma das sequências padded (Teste): (17197, 100)\n",
            "\n",
            "Labels originais de Treino: [0 1]\n",
            "Labels de Treino codificadas: [0 0 0 ... 0 1 0]\n",
            "Classes conhecidas pelo LabelEncoder: [0 1]\n",
            "\n",
            "Forma dos dados de treino para CNN (split do TREINO ORIGINAL): (25569, 100) (25569,)\n",
            "Forma dos dados de validação para CNN (split do TREINO ORIGINAL): (6393, 100) (6393,)\n",
            "Forma dos dados de teste FINAL para CNN (dataset de TESTE ORIGINAL): (17197, 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando CNN com class_weight...\n",
            "Epoch 1/10\n",
            "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 39ms/step - accuracy: 0.7417 - loss: 0.5572 - val_accuracy: 0.8788 - val_loss: 0.3422\n",
            "Epoch 2/10\n",
            "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 38ms/step - accuracy: 0.9236 - loss: 0.2170 - val_accuracy: 0.9050 - val_loss: 0.2548\n",
            "Epoch 3/10\n",
            "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 39ms/step - accuracy: 0.9682 - loss: 0.0961 - val_accuracy: 0.9077 - val_loss: 0.2837\n",
            "Epoch 4/10\n",
            "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 38ms/step - accuracy: 0.9824 - loss: 0.0517 - val_accuracy: 0.9214 - val_loss: 0.3098\n",
            "Epoch 5/10\n",
            "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 38ms/step - accuracy: 0.9871 - loss: 0.0383 - val_accuracy: 0.9237 - val_loss: 0.3663\n",
            "Epoch 6/10\n",
            "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 38ms/step - accuracy: 0.9908 - loss: 0.0272 - val_accuracy: 0.9320 - val_loss: 0.3916\n",
            "Epoch 7/10\n",
            "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 39ms/step - accuracy: 0.9893 - loss: 0.0311 - val_accuracy: 0.9288 - val_loss: 0.4189\n",
            "Epoch 8/10\n",
            "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 38ms/step - accuracy: 0.9922 - loss: 0.0280 - val_accuracy: 0.9312 - val_loss: 0.4803\n",
            "Epoch 9/10\n",
            "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 38ms/step - accuracy: 0.9949 - loss: 0.0203 - val_accuracy: 0.9226 - val_loss: 0.4725\n",
            "Epoch 10/10\n",
            "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 39ms/step - accuracy: 0.9954 - loss: 0.0181 - val_accuracy: 0.8960 - val_loss: 0.7083\n",
            "\n",
            "Resumo do Modelo CNN:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m49,280\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m258\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,038,152\u001b[0m (15.40 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,038,152</span> (15.40 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,346,050\u001b[0m (5.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,346,050</span> (5.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,692,102\u001b[0m (10.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,692,102</span> (10.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando CNN no conjunto de validação...\n",
            "\n",
            "Acurácia da CNN no conjunto de validação: 0.8905\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
            "\n",
            "Avaliação completa da CNN no conjunto de validação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.90      0.94      5945\n",
            "           1       0.36      0.70      0.47       448\n",
            "\n",
            "    accuracy                           0.89      6393\n",
            "   macro avg       0.67      0.80      0.71      6393\n",
            "weighted avg       0.93      0.89      0.91      6393\n",
            "\n",
            "Weighted F1-score: 0.9062961205915044\n",
            "AUC: 0.8799242310464976\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score # Import f1_score and roc_auc_score\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# --- pré-processamento para CNN ---\n",
        "results = {}\n",
        "\n",
        "max_words = 10000\n",
        "max_sequence_length = 100\n",
        "\n",
        "# --- tokenização ---\n",
        "train_texts = df_train['processed_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "test_texts = df_test['processed_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# converte textos para sequências de inteiros\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# --- padding das Sequências ---\n",
        "train_padded_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_padded_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "print(\"\\nExemplo de sequências padded (Treino):\")\n",
        "print(train_padded_sequences[:5])\n",
        "print(\"\\nForma das sequências padded (Treino):\", train_padded_sequences.shape)\n",
        "\n",
        "print(\"\\nExemplo de sequências padded (Teste):\")\n",
        "print(test_padded_sequences[:5])\n",
        "print(\"\\nForma das sequências padded (Teste):\", test_padded_sequences.shape)\n",
        "\n",
        "# --- prepara labels para CNN/LSTM ---\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_y_train_original = label_encoder.fit_transform(df_train['label'])\n",
        "\n",
        "print(\"\\nLabels originais de Treino:\", df_train['label'].unique())\n",
        "print(\"Labels de Treino codificadas:\", encoded_y_train_original)\n",
        "print(\"Classes conhecidas pelo LabelEncoder:\", label_encoder.classes_)\n",
        "\n",
        "\n",
        "# --- divide dados para CNN/LSTM (usando as sequências e labels codificadas) ---\n",
        "X_cnn_train_val = train_padded_sequences\n",
        "y_cnn_train_val = encoded_y_train_original\n",
        "\n",
        "X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(\n",
        "    X_cnn_train_val, y_cnn_train_val, test_size=0.20, random_state=42, stratify=y_cnn_train_val\n",
        ")\n",
        "\n",
        "X_test_cnn_final = test_padded_sequences\n",
        "\n",
        "\n",
        "print(\"\\nForma dos dados de treino para CNN (split do TREINO ORIGINAL):\", X_train_cnn.shape, y_train_cnn.shape)\n",
        "print(\"Forma dos dados de validação para CNN (split do TREINO ORIGINAL):\", X_val_cnn.shape, y_val_cnn.shape)\n",
        "print(\"Forma dos dados de teste FINAL para CNN (dataset de TESTE ORIGINAL):\", X_test_cnn_final.shape)\n",
        "\n",
        "\n",
        "# --- constrói o Modelo CNN ---\n",
        "\n",
        "\n",
        "embedding_dim = 128\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_filters = 128\n",
        "\n",
        "num_classes_cnn = len(label_encoder.classes_)\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    # Camada de Embedding: mapeia palavras (índices) para vetores densos\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    # Múltiplas camadas Convolucionais seguidas de GlobalMaxPooling\n",
        "    Conv1D(filters=num_filters, kernel_size=filter_sizes[0], activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    # Camada Densa (Fully Connected)\n",
        "    Dense(128, activation='relu'),\n",
        "    # Camada de Dropout\n",
        "    Dropout(0.5),\n",
        "    # Camada de saída\n",
        "    Dense(num_classes_cnn, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# --- treino do Modelo CNN ---\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "print(\"\\nTreinando CNN com class_weight...\")\n",
        "history = model.fit(X_train_cnn, y_train_cnn,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_split=0.1,\n",
        "                    verbose=1,\n",
        "                    class_weight=class_weight_dict)\n",
        "\n",
        "print(\"\\nResumo do Modelo CNN:\")\n",
        "model.summary()\n",
        "\n",
        "# --- avaliação do modelo\n",
        "print(\"\\nAvaliando CNN no conjunto de validação...\")\n",
        "loss_cnn_val, accuracy_cnn_val = model.evaluate(X_val_cnn, y_val_cnn, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da CNN no conjunto de validação: {accuracy_cnn_val:.4f}\")\n",
        "\n",
        "# classification report para CNN\n",
        "cnn_predictions_val_probs = model.predict(X_val_cnn)\n",
        "cnn_predictions_val = tf.argmax(cnn_predictions_val_probs, axis=1).numpy()\n",
        "\n",
        "print(\"\\nAvaliação completa da CNN no conjunto de validação:\")\n",
        "class_names = [str(cls) for cls in label_encoder.classes_]\n",
        "print(classification_report(y_val_cnn, cnn_predictions_val, target_names=class_names))\n",
        "\n",
        "cnn_weighted_f1_val = f1_score(y_val_cnn, cnn_predictions_val, average='weighted')\n",
        "print(\"Weighted F1-score:\", cnn_weighted_f1_val)\n",
        "\n",
        "try:\n",
        "    cnn_auc_score_val = roc_auc_score(y_val_cnn, cnn_predictions_val_probs[:, 1])\n",
        "    print(\"AUC:\", cnn_auc_score_val)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    cnn_auc_score_val = None\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "# dicionário de resultados\n",
        "\n",
        "cnn_report_val = classification_report(y_val_cnn, cnn_predictions_val, output_dict=True, target_names=class_names)\n",
        "results['CNN (Embedding+Seq)'] = {\n",
        "    'accuracy': accuracy_cnn_val,\n",
        "    'precision (macro)': cnn_report_val['macro avg']['precision'],\n",
        "    'recall (macro)': cnn_report_val['macro avg']['recall'],\n",
        "    'f1-score (macro)': cnn_report_val['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': cnn_weighted_f1_val,\n",
        "    'auc': cnn_auc_score_val\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UQZapM397Ecj",
        "outputId": "b7895b4e-0110-4894-987b-2e9aa706918f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Construindo o Modelo LSTM...\n",
            "\n",
            "Treinando LSTM...\n",
            "Epoch 1/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 178ms/step - accuracy: 0.4969 - loss: 0.6982 - val_accuracy: 0.0701 - val_loss: 0.7218\n",
            "Epoch 2/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 191ms/step - accuracy: 0.3160 - loss: 0.7091 - val_accuracy: 0.0701 - val_loss: 0.7011\n",
            "Epoch 3/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 178ms/step - accuracy: 0.5294 - loss: 0.6965 - val_accuracy: 0.0701 - val_loss: 0.7007\n",
            "Epoch 4/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 179ms/step - accuracy: 0.5229 - loss: 0.6868 - val_accuracy: 0.9299 - val_loss: 0.6928\n",
            "Epoch 5/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 190ms/step - accuracy: 0.5502 - loss: 0.6956 - val_accuracy: 0.9299 - val_loss: 0.6841\n",
            "Epoch 6/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 179ms/step - accuracy: 0.6055 - loss: 0.6979 - val_accuracy: 0.0701 - val_loss: 0.6940\n",
            "Epoch 7/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 191ms/step - accuracy: 0.4831 - loss: 0.7004 - val_accuracy: 0.0701 - val_loss: 0.6944\n",
            "Epoch 8/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 190ms/step - accuracy: 0.4312 - loss: 0.6973 - val_accuracy: 0.0701 - val_loss: 0.7259\n",
            "Epoch 9/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 178ms/step - accuracy: 0.3856 - loss: 0.6933 - val_accuracy: 0.0701 - val_loss: 0.6997\n",
            "Epoch 10/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 179ms/step - accuracy: 0.3897 - loss: 0.6936 - val_accuracy: 0.9299 - val_loss: 0.6831\n",
            "Epoch 11/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 178ms/step - accuracy: 0.5154 - loss: 0.6890 - val_accuracy: 0.9299 - val_loss: 0.6903\n",
            "Epoch 12/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 179ms/step - accuracy: 0.5744 - loss: 0.6878 - val_accuracy: 0.0701 - val_loss: 0.7164\n",
            "Epoch 13/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 180ms/step - accuracy: 0.3908 - loss: 0.6836 - val_accuracy: 0.0701 - val_loss: 0.6981\n",
            "Epoch 14/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 177ms/step - accuracy: 0.5954 - loss: 0.6848 - val_accuracy: 0.0701 - val_loss: 0.7109\n",
            "Epoch 15/15\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 179ms/step - accuracy: 0.2224 - loss: 0.6893 - val_accuracy: 0.0701 - val_loss: 0.7024\n",
            "\n",
            "Resumo do Modelo LSTM:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,259,717\u001b[0m (16.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,259,717</span> (16.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,419,905\u001b[0m (5.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,419,905</span> (5.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,839,812\u001b[0m (10.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,839,812</span> (10.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando LSTM no conjunto de validação...\n",
            "\n",
            "Acurácia da LSTM no conjunto de validação: 0.0701\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 49ms/step\n",
            "\n",
            "Avaliação completa da LSTM no conjunto de validação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      5945\n",
            "           1       0.07      1.00      0.13       448\n",
            "\n",
            "    accuracy                           0.07      6393\n",
            "   macro avg       0.04      0.50      0.07      6393\n",
            "weighted avg       0.00      0.07      0.01      6393\n",
            "\n",
            "Weighted F1-score: 0.0091782890094146\n",
            "AUC: 0.5\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# --- reutiliza dados preparados para a CNN ---\n",
        "\n",
        "# --- constrói o Modelo LSTM ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Construindo o Modelo LSTM...\")\n",
        "\n",
        "num_classes_lstm = len(label_encoder.classes_)\n",
        "if num_classes_lstm != 2:\n",
        "    print(f\"WARNING: Found {num_classes_lstm} classes. Model is configured for binary (2) classes.\")\n",
        "\n",
        "\n",
        "model_lstm = Sequential([\n",
        "    # camada de Embedding: Reutiliza os mesmos parâmetros da CNN para consistência na base de embedding\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    # camada LSTM\n",
        "    LSTM(128),\n",
        "    # Camada de Dropout\n",
        "    Dropout(0.5),\n",
        "    # Camada Densa (Fully Connected)\n",
        "    Dense(64, activation='relu'), # Camada densa adicional\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "model_lstm.compile(optimizer='adam',\n",
        "                   loss='binary_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# --- treino do Modelo LSTM ---\n",
        "epochs_lstm = 15\n",
        "batch_size_lstm = 32\n",
        "\n",
        "print(\"\\nTreinando LSTM...\")\n",
        "\n",
        "history_lstm = model_lstm.fit(X_train_cnn, y_train_cnn,\n",
        "                              epochs=epochs_lstm,\n",
        "                              batch_size=batch_size_lstm,\n",
        "                              validation_data=(X_val_cnn, y_val_cnn),\n",
        "                              class_weight=class_weight_dict,\n",
        "                              verbose=1)\n",
        "\n",
        "print(\"\\nResumo do Modelo LSTM:\")\n",
        "model_lstm.summary()\n",
        "\n",
        "# --- avaliação do modelo\n",
        "print(\"\\nAvaliando LSTM no conjunto de validação...\")\n",
        "\n",
        "loss_lstm_val, accuracy_lstm_val = model_lstm.evaluate(X_val_cnn, y_val_cnn, verbose=0)\n",
        "print(f\"\\nAcurácia da LSTM no conjunto de validação: {accuracy_lstm_val:.4f}\")\n",
        "\n",
        "lstm_predictions_proba_positive_val = model_lstm.predict(X_val_cnn)\n",
        "lstm_predictions_val = (lstm_predictions_proba_positive_val >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da LSTM no conjunto de validação:\")\n",
        "print(classification_report(y_val_cnn, lstm_predictions_val))\n",
        "\n",
        "# cálculo de F1-score ponderado para LSTM\n",
        "lstm_weighted_f1_val = f1_score(y_val_cnn, lstm_predictions_val, average='weighted')\n",
        "print(\"Weighted F1-score:\", lstm_weighted_f1_val)\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    lstm_auc_score_val = roc_auc_score(y_val_cnn, lstm_predictions_proba_positive_val)\n",
        "    print(\"AUC:\", lstm_auc_score_val)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    lstm_auc_score_val = None\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "\n",
        "lstm_report_val = classification_report(y_val_cnn, lstm_predictions_val, output_dict=True)\n",
        "# dicionário de resultados\n",
        "results['LSTM (Embedding+Seq) Validation'] = {\n",
        "    'accuracy': accuracy_lstm_val,\n",
        "    'precision (macro)': lstm_report_val['macro avg']['precision'],\n",
        "    'recall (macro)': lstm_report_val['macro avg']['recall'],\n",
        "    'f1-score (macro)': lstm_report_val['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': lstm_weighted_f1_val,\n",
        "    'auc': lstm_auc_score_val\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Oversampling**"
      ],
      "metadata": {
        "id": "vmNe7p3YsJPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como segunda abordagem para tratar o desbalanceamento entre as classes, foi utilizado o método de oversampling sintético **SMOTE (Synthetic Minority Over-sampling Technique)**. Essa técnica atua gerando novos exemplos sintéticos para as classes minoritárias com base na interpolação entre amostras reais vizinhas no espaço de atributos, equilibrando a distribuição sem simplesmente duplicar instâncias existentes.\n",
        "\n",
        "O SMOTE foi aplicado exclusivamente ao conjunto de treino, preservando a distribuição real nos dados de teste. Com isso, as duas classes (0 e 1) passaram a ter exatamente o mesmo número de instâncias (23.775), conforme evidenciado após o reamostramento.\n",
        "\n",
        "Esse balanceamento favorece o aprendizado de modelos como Regressão Logística, SVM, Random Forest, LightGBM e MLP, aumentando sua capacidade de generalização para exemplos das classes originalmente sub-representadas. A técnica é especialmente vantajosa por evitar overfitting comum em métodos de duplicação, ao mesmo tempo em que permite o uso direto dos classificadores."
      ],
      "metadata": {
        "id": "q5Wnki65Uqv3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB2rOau4weQw",
        "outputId": "f54acfbc-81fb-42ed-d37b-28d3828492d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T1kdoKHwzSq",
        "outputId": "941de75a-b48d-402d-dacc-2a14ed2c9028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Distribuição das classes no treino (antes do balanceamento):\n",
            "label\n",
            "0    23775\n",
            "1     1794\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Forma dos dados de treino após SMOTE: (47550, 5000) (47550,)\n",
            "\n",
            "Distribuição das classes no treino (depois do SMOTE):\n",
            "label\n",
            "0    23775\n",
            "1    23775\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nDistribuição das classes no treino (antes do balanceamento):\")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "# inicializa SMOTE\n",
        "# random_state para reprodutibilidade\n",
        "# sampling_strategy='auto' reamostra todas as classes minoritárias para igualar a classe majoritária\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "# aplica SMOTE apenas nos dados de TREINO\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"\\nForma dos dados de treino após SMOTE:\", X_train_resampled.shape, y_train_resampled.shape)\n",
        "print(\"\\nDistribuição das classes no treino (depois do SMOTE):\")\n",
        "print(y_train_resampled.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdx1A1zaxRKA",
        "outputId": "f27bae22-48fe-42ab-83f3-cab9ed25810c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando modelos tradicionais COM SMOTE...\n",
            "--------------------------------------------------\n",
            "Treinando Regressão Logística com SMOTE-balanced data...\n",
            "Avaliação da Regressão Logística (SMOTE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.85      0.91      5945\n",
            "           1       0.26      0.71      0.39       448\n",
            "\n",
            "    accuracy                           0.84      6393\n",
            "   macro avg       0.62      0.78      0.65      6393\n",
            "weighted avg       0.93      0.84      0.87      6393\n",
            "\n",
            "Acurácia:  0.8413890192397935\n",
            "Weighted F1-score: 0.8722965095263364\n",
            "AUC: 0.8772295145981016\n",
            "--------------------------------------------------\n",
            "Treinando Naive Bayes Multinomial com SMOTE-balanced data...\n",
            "Avaliação do Naive Bayes Multinomial (SMOTE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.89      0.93      5945\n",
            "           1       0.34      0.72      0.46       448\n",
            "\n",
            "    accuracy                           0.88      6393\n",
            "   macro avg       0.66      0.81      0.70      6393\n",
            "weighted avg       0.93      0.88      0.90      6393\n",
            "\n",
            "Acurácia:  0.8815892382293133\n",
            "Weighted F1-score: 0.9003040713818317\n",
            "AUC: 0.8926082091793823\n",
            "--------------------------------------------------\n",
            "Treinando SVM (Kernel Linear) com SMOTE-balanced data...\n",
            "Avaliação do SVM (SMOTE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.85      0.91      5945\n",
            "           1       0.26      0.69      0.38       448\n",
            "\n",
            "    accuracy                           0.84      6393\n",
            "   macro avg       0.62      0.77      0.64      6393\n",
            "weighted avg       0.92      0.84      0.87      6393\n",
            "\n",
            "Acurácia:  0.8429532300954169\n",
            "Weighted F1-score: 0.872916255279533\n",
            "AUC: 0.8542256773399015\n",
            "--------------------------------------------------\n",
            "Treinando Random Forest com SMOTE-balanced data...\n",
            "Avaliação do Random Forest (SMOTE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.94      0.95      5945\n",
            "           1       0.43      0.61      0.51       448\n",
            "\n",
            "    accuracy                           0.92      6393\n",
            "   macro avg       0.70      0.78      0.73      6393\n",
            "weighted avg       0.93      0.92      0.92      6393\n",
            "\n",
            "Acurácia:  0.9161582981385891\n",
            "Weighted F1-score: 0.9227552191473184\n",
            "AUC: 0.8839890213865194\n",
            "\n",
            "Resultados acumulados (incluindo SMOTE) até agora:\n",
            "{'Logistic Regression (SMOTE)': {'accuracy': 0.8413890192397935, 'precision (macro)': 0.620044942374266, 'recall (macro)': 0.7815944896671874, 'f1-score (macro)': 0.6475630781703492, 'f1-score (weighted)': 0.8722965095263364, 'auc': np.float64(0.8772295145981016)}, 'Multinomial NB (SMOTE)': {'accuracy': 0.8815892382293133, 'precision (macro)': 0.6573593065242886, 'recall (macro)': 0.8063051934398655, 'f1-score (macro)': 0.6965903330860039, 'f1-score (weighted)': 0.9003040713818317, 'auc': np.float64(0.8926082091793823)}, 'SVM (SMOTE)': {'accuracy': 0.8429532300954169, 'precision (macro)': 0.6176985915354669, 'recall (macro)': 0.7700519268893428, 'f1-score (macro)': 0.6447905635384164, 'f1-score (weighted)': 0.872916255279533, 'auc': np.float64(0.8542256773399015)}, 'Random Forest (SMOTE)': {'accuracy': 0.9161582981385891, 'precision (macro)': 0.7002967675119052, 'recall (macro)': 0.775357818695182, 'f1-score (macro)': 0.7298654596527165, 'f1-score (weighted)': 0.9227552191473184, 'auc': np.float64(0.8839890213865194)}}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"Treinando modelos tradicionais COM SMOTE...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 1. Regressão Logística\n",
        "print(\"Treinando Regressão Logística com SMOTE-balanced data...\")\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train_resampled, y_train_resampled)\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "lr_predictions_proba = lr_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação da Regressão Logística (SMOTE):\")\n",
        "print(classification_report(y_test, lr_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lr_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, lr_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, lr_predictions_proba))\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "lr_report = classification_report(y_test, lr_predictions, output_dict=True)\n",
        "results['Logistic Regression (SMOTE)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lr_predictions),\n",
        "    'precision (macro)': lr_report['macro avg']['precision'],\n",
        "    'recall (macro)': lr_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lr_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lr_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, lr_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 2. Naive Bayes Multinomial (geralmente bom para dados textuais esparsos como TF-IDF)\n",
        "print(\"Treinando Naive Bayes Multinomial com SMOTE-balanced data...\")\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_resampled, y_train_resampled)\n",
        "nb_predictions = nb_model.predict(X_test)\n",
        "nb_predictions_proba = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do Naive Bayes Multinomial (SMOTE):\")\n",
        "print(classification_report(y_test, nb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, nb_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, nb_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, nb_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "nb_report = classification_report(y_test, nb_predictions, output_dict=True)\n",
        "results['Multinomial NB (SMOTE)'] = {\n",
        "    'accuracy': accuracy_score(y_test, nb_predictions),\n",
        "    'precision (macro)': nb_report['macro avg']['precision'],\n",
        "    'recall (macro)': nb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': nb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, nb_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, nb_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 3. Support Vector Machine (SVM)\n",
        "print(\"Treinando SVM (Kernel Linear) com SMOTE-balanced data...\")\n",
        "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
        "svm_model.fit(X_train_resampled, y_train_resampled)\n",
        "svm_predictions = svm_model.predict(X_test)\n",
        "svm_predictions_proba = svm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do SVM (SMOTE):\")\n",
        "print(classification_report(y_test, svm_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, svm_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, svm_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, svm_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "svm_report = classification_report(y_test, svm_predictions, output_dict=True)\n",
        "results['SVM (SMOTE)'] = {\n",
        "    'accuracy': accuracy_score(y_test, svm_predictions),\n",
        "    'precision (macro)': svm_report['macro avg']['precision'],\n",
        "    'recall (macro)': svm_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': svm_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, svm_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, svm_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 4. Random Forest Classifier\n",
        "print(\"Treinando Random Forest com SMOTE-balanced data...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_resampled, y_train_resampled)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "rf_predictions_proba = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do Random Forest (SMOTE):\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, rf_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, rf_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, rf_predictions_proba))\n",
        "\n",
        "# dicionário de resultados\n",
        "rf_report = classification_report(y_test, rf_predictions, output_dict=True)\n",
        "results['Random Forest (SMOTE)'] = {\n",
        "    'accuracy': accuracy_score(y_test, rf_predictions),\n",
        "    'precision (macro)': rf_report['macro avg']['precision'],\n",
        "    'recall (macro)': rf_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': rf_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, rf_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, rf_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"\\nResultados acumulados (incluindo SMOTE) até agora:\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9DmJa4aryxuX",
        "outputId": "5d63c22a-7b90-4b1c-be65-4246c615ce83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\n",
            "\n",
            "==================================================\n",
            "Treinando LightGBM com SMOTE-balanced data...\n",
            "Número de classes em y_train_resampled para LightGBM: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 23775, number of negative: 23775\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 4.319328 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 63019\n",
            "[LightGBM] [Info] Number of data points in the train set: 47550, number of used features: 2103\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avaliação do LightGBM (SMOTE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.92      0.94      5945\n",
            "           1       0.37      0.60      0.46       448\n",
            "\n",
            "    accuracy                           0.90      6393\n",
            "   macro avg       0.67      0.76      0.70      6393\n",
            "weighted avg       0.93      0.90      0.91      6393\n",
            "\n",
            "Acurácia:  0.899734084154544\n",
            "Weighted F1-score: 0.9104894689727291\n",
            "AUC: 0.8555636489246665\n",
            "--------------------------------------------------\n",
            "Treinando Rede Neural Densa (MLP) com SMOTE-balanced data...\n",
            "Número de classes em y_train_resampled para MLP: 2\n",
            "\n",
            "Treinando MLP (SMOTE)...\n",
            "Epoch 1/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 24ms/step - accuracy: 0.8146 - loss: 0.3747 - val_accuracy: 0.9143 - val_loss: 0.2597\n",
            "Epoch 2/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.9712 - loss: 0.0772 - val_accuracy: 0.9193 - val_loss: 0.3080\n",
            "Epoch 3/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.9774 - loss: 0.0514 - val_accuracy: 0.9157 - val_loss: 0.3273\n",
            "Epoch 4/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 23ms/step - accuracy: 0.9799 - loss: 0.0433 - val_accuracy: 0.9484 - val_loss: 0.3325\n",
            "Epoch 5/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 25ms/step - accuracy: 0.9800 - loss: 0.0393 - val_accuracy: 0.9160 - val_loss: 0.3447\n",
            "Epoch 6/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 24ms/step - accuracy: 0.9828 - loss: 0.0365 - val_accuracy: 0.9415 - val_loss: 0.3527\n",
            "Epoch 7/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.9824 - loss: 0.0336 - val_accuracy: 0.9158 - val_loss: 0.3665\n",
            "Epoch 8/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - accuracy: 0.9829 - loss: 0.0325 - val_accuracy: 0.9166 - val_loss: 0.3636\n",
            "Epoch 9/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9836 - loss: 0.0300 - val_accuracy: 0.9462 - val_loss: 0.3901\n",
            "Epoch 10/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9837 - loss: 0.0293 - val_accuracy: 0.9490 - val_loss: 0.4044\n",
            "Epoch 11/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 24ms/step - accuracy: 0.9839 - loss: 0.0298 - val_accuracy: 0.9473 - val_loss: 0.4148\n",
            "Epoch 12/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - accuracy: 0.9840 - loss: 0.0287 - val_accuracy: 0.9506 - val_loss: 0.4261\n",
            "Epoch 13/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9848 - loss: 0.0277 - val_accuracy: 0.9479 - val_loss: 0.4413\n",
            "Epoch 14/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - accuracy: 0.9856 - loss: 0.0277 - val_accuracy: 0.9504 - val_loss: 0.4561\n",
            "Epoch 15/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.9853 - loss: 0.0278 - val_accuracy: 0.9495 - val_loss: 0.4395\n",
            "Epoch 16/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - accuracy: 0.9854 - loss: 0.0270 - val_accuracy: 0.9479 - val_loss: 0.4518\n",
            "Epoch 17/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 24ms/step - accuracy: 0.9861 - loss: 0.0266 - val_accuracy: 0.9506 - val_loss: 0.4837\n",
            "Epoch 18/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9859 - loss: 0.0268 - val_accuracy: 0.9479 - val_loss: 0.4838\n",
            "Epoch 19/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - accuracy: 0.9862 - loss: 0.0257 - val_accuracy: 0.9499 - val_loss: 0.5106\n",
            "Epoch 20/20\n",
            "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 22ms/step - accuracy: 0.9852 - loss: 0.0261 - val_accuracy: 0.9493 - val_loss: 0.5186\n",
            "\n",
            "Resumo do Modelo MLP (SMOTE):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,280,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,939,845\u001b[0m (15.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,939,845</span> (15.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,281\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,281</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,626,564\u001b[0m (10.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,626,564</span> (10.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando MLP (SMOTE) no conjunto de teste...\n",
            "\n",
            "Acurácia da MLP (SMOTE) no conjunto de teste: 0.9493\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\n",
            "Avaliação completa da MLP (SMOTE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97      5945\n",
            "           1       0.68      0.52      0.59       448\n",
            "\n",
            "    accuracy                           0.95      6393\n",
            "   macro avg       0.82      0.75      0.78      6393\n",
            "weighted avg       0.94      0.95      0.95      6393\n",
            "\n",
            "Weighted F1-score: 0.9461434018924282\n",
            "AUC: 0.8662283731827467\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# --- Modelos Avançados com TF-IDF ---\n",
        "\n",
        "# 5. LightGBM (Gradient Boosting)\n",
        "!pip install lightgbm\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Treinando LightGBM com SMOTE-balanced data...\")\n",
        "\n",
        "num_classes_lgbm = len(np.unique(y_train_resampled))\n",
        "print(f\"Número de classes em y_train_resampled para LightGBM: {num_classes_lgbm}\")\n",
        "\n",
        "lgb_model = lgb.LGBMClassifier(objective='binary', random_state=42)\n",
        "lgb_model.fit(X_train_resampled, y_train_resampled)\n",
        "lgb_predictions = lgb_model.predict(X_test)\n",
        "lgb_predictions_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do LightGBM (SMOTE):\")\n",
        "print(classification_report(y_test, lgb_predictions))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lgb_predictions))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, lgb_predictions, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, lgb_predictions_proba))\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "lgb_report = classification_report(y_test, lgb_predictions, output_dict=True)\n",
        "results['LightGBM (SMOTE)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lgb_predictions),\n",
        "    'precision (macro)': lgb_report['macro avg']['precision'],\n",
        "    'recall (macro)': lgb_report['macro avg']['recall'],\n",
        "    'f1-score (macro)': lgb_report['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lgb_predictions, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, lgb_predictions_proba)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 6. Rede Neural Densa (MLP) com TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "print(\"Treinando Rede Neural Densa (MLP) com SMOTE-balanced data...\")\n",
        "\n",
        "num_classes_mlp_smote = len(np.unique(y_train_resampled))\n",
        "print(f\"Número de classes em y_train_resampled para MLP: {num_classes_mlp_smote}\")\n",
        "\n",
        "# constrói o Modelo MLP\n",
        "mlp_model_smote = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train_resampled.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "mlp_model_smote.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# treino do Modelo MLP\n",
        "epochs_mlp_smote = 20\n",
        "batch_size_mlp_smote = 64\n",
        "\n",
        "\n",
        "print(\"\\nTreinando MLP (SMOTE)...\")\n",
        "history_mlp_smote = mlp_model_smote.fit(X_train_resampled, y_train_resampled,\n",
        "                          epochs=epochs_mlp_smote,\n",
        "                          batch_size=batch_size_mlp_smote,\n",
        "                          validation_data=(X_test, y_test),\n",
        "                          verbose=1)\n",
        "\n",
        "\n",
        "print(\"\\nResumo do Modelo MLP (SMOTE):\")\n",
        "mlp_model_smote.summary()\n",
        "\n",
        "# avaliação do modelo\n",
        "print(\"\\nAvaliando MLP (SMOTE) no conjunto de teste...\")\n",
        "loss_mlp_smote, accuracy_mlp_smote = mlp_model_smote.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da MLP (SMOTE) no conjunto de teste: {accuracy_mlp_smote:.4f}\")\n",
        "\n",
        "mlp_predictions_proba_positive_smote = mlp_model_smote.predict(X_test)\n",
        "mlp_predictions_smote = (mlp_predictions_proba_positive_smote >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da MLP (SMOTE):\")\n",
        "print(classification_report(y_test, mlp_predictions_smote))\n",
        "\n",
        "# cálculo de F1-score ponderado para MLP (SMOTE)\n",
        "mlp_weighted_f1_smote = f1_score(y_test, mlp_predictions_smote, average='weighted')\n",
        "print(\"Weighted F1-score:\", mlp_weighted_f1_smote)\n",
        "\n",
        "# cálculo de AUC\n",
        "try:\n",
        "    mlp_auc_score_smote = roc_auc_score(y_test, mlp_predictions_proba_positive_smote)\n",
        "    print(\"AUC:\", mlp_auc_score_smote)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    mlp_auc_score_smote = None\n",
        "\n",
        "\n",
        "# dicionário de resultados\n",
        "mlp_report_smote = classification_report(y_test, mlp_predictions_smote, output_dict=True)\n",
        "results['MLP (SMOTE)'] = {\n",
        "    'accuracy': accuracy_mlp_smote,\n",
        "    'precision (macro)': mlp_report_smote['macro avg']['precision'],\n",
        "    'recall (macro)': mlp_report_smote['macro avg']['recall'],\n",
        "    'f1-score (macro)': mlp_report_smote['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': mlp_weighted_f1_smote,\n",
        "    'auc': mlp_auc_score_smote\n",
        "}\n",
        "\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reutiliza TensorFlow e bibliotecas de pré-processamento\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, LSTM # Importar LSTM também\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- pré-processamento para Modelos DL (CNN/LSTM) ---\n",
        "\n",
        "max_words = 10000\n",
        "max_sequence_length = 100\n",
        "\n",
        "train_texts = df_train['processed_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "test_texts = df_test['processed_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# converte textos para sequências de inteiros\n",
        "train_sequences_original = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences_original = tokenizer.texts_to_sequences(test_texts) # Para avaliação final\n",
        "\n",
        "# padding das Sequências\n",
        "train_padded_sequences_original = pad_sequences(train_sequences_original, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_padded_sequences_original = pad_sequences(test_sequences_original, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "print(\"\\nForma das sequências padded (Treino Original):\", train_padded_sequences_original.shape)\n",
        "print(\"Forma das sequências padded (Teste Original):\", test_padded_sequences_original.shape)\n",
        "\n",
        "# prepara labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_y_train_original = label_encoder.fit_transform(df_train['label'])\n",
        "\n",
        "print(\"\\nLabels de Treino codificadas (Originais):\", encoded_y_train_original)\n",
        "print(\"Classes conhecidas pelo LabelEncoder:\", label_encoder.classes_)\n",
        "\n",
        "\n",
        "# --- divisão em Treino e Validação ---\n",
        "X_train_pre_oversample, X_val_dl, y_train_pre_oversample, y_val_dl = train_test_split(\n",
        "    train_padded_sequences_original, encoded_y_train_original, test_size=0.20, random_state=42, stratify=encoded_y_train_original\n",
        ")\n",
        "\n",
        "X_test_dl_final = test_padded_sequences_original\n",
        "\n",
        "\n",
        "print(\"\\nForma dos dados de treino (Pré-Oversampling):\", X_train_pre_oversample.shape, y_train_pre_oversample.shape)\n",
        "print(\"Forma dos dados de validação:\", X_val_dl.shape, y_val_dl.shape)\n",
        "print(\"Forma dos dados de teste FINAL:\", X_test_dl_final.shape)\n",
        "\n",
        "# verificação\n",
        "print(\"\\nDistribuição das classes no conjunto de treino (Pré-Oversampling):\")\n",
        "print(pd.Series(y_train_pre_oversample).value_counts(normalize=True) * 100)\n",
        "\n",
        "\n",
        "# --- aplicação do Oversampling ---\n",
        "\n",
        "print(\"\\nAplicando Oversampling (SMOTE) no conjunto de treino...\")\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "X_train_oversample, y_train_oversample = smote.fit_resample(X_train_pre_oversample, y_train_pre_oversample)\n",
        "\n",
        "print(\"\\nForma dos dados de treino (Após Oversampling):\", X_train_oversample.shape, y_train_oversample.shape)\n",
        "print(\"\\nDistribuição das classes no conjunto de treino (Após Oversampling):\")\n",
        "print(pd.Series(y_train_oversample).value_counts(normalize=True) * 100)\n",
        "\n",
        "X_train_dl = X_train_oversample\n",
        "y_train_dl = y_train_oversample\n",
        "\n",
        "X_val_dl = X_val_dl\n",
        "y_val_dl = y_val_dl\n",
        "\n",
        "# --- construção e treino do Modelo CNN ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Construindo e Treinando o Modelo CNN com Oversampling...\")\n",
        "\n",
        "embedding_dim = 128\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_filters = 128\n",
        "\n",
        "model_cnn_oversample = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    Conv1D(filters=num_filters, kernel_size=filter_sizes[0], activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_cnn_oversample.compile(optimizer='adam',\n",
        "                             loss='binary_crossentropy',\n",
        "                             metrics=['accuracy'])\n",
        "\n",
        "epochs_cnn = 10\n",
        "batch_size_cnn = 32\n",
        "\n",
        "print(\"\\nTreinando CNN com dados oversampleados...\")\n",
        "history_cnn_oversample = model_cnn_oversample.fit(X_train_dl, y_train_dl,\n",
        "                                                 epochs=epochs_cnn,\n",
        "                                                 batch_size=batch_size_cnn,\n",
        "                                                 validation_data=(X_val_dl, y_val_dl),\n",
        "                                                 verbose=1)\n",
        "\n",
        "print(\"\\nResumo do Modelo CNN (Oversample):\")\n",
        "model_cnn_oversample.summary()\n",
        "\n",
        "# --- avaliação do modelo\n",
        "print(\"\\nAvaliando CNN (Oversample) no conjunto de validação...\")\n",
        "loss_cnn_os_val, accuracy_cnn_os_val = model_cnn_oversample.evaluate(X_val_dl, y_val_dl, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da CNN (Oversample) no conjunto de validação: {accuracy_cnn_os_val:.4f}\")\n",
        "\n",
        "cnn_os_predictions_proba_val = model_cnn_oversample.predict(X_val_dl)\n",
        "cnn_os_predictions_val = (cnn_os_predictions_proba_val >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da CNN (Oversample) no conjunto de validação:\")\n",
        "print(classification_report(y_val_dl, cnn_os_predictions_val))\n",
        "\n",
        "cnn_os_weighted_f1_val = f1_score(y_val_dl, cnn_os_predictions_val, average='weighted')\n",
        "print(\"Weighted F1-score:\", cnn_os_weighted_f1_val)\n",
        "\n",
        "try:\n",
        "    cnn_os_auc_score_val = roc_auc_score(y_val_dl, cnn_os_predictions_proba_val)\n",
        "    print(\"AUC:\", cnn_os_auc_score_val)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    cnn_os_auc_score_val = None\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "# dicionário de resultados\n",
        "cnn_os_report_val = classification_report(y_val_dl, cnn_os_predictions_val, output_dict=True)\n",
        "results = {}\n",
        "results['CNN (Embedding+Seq) Oversample Validation'] = {\n",
        "    'accuracy': accuracy_cnn_os_val,\n",
        "    'precision (macro)': cnn_os_report_val['macro avg']['precision'],\n",
        "    'recall (macro)': cnn_os_report_val['macro avg']['recall'],\n",
        "    'f1-score (macro)': cnn_os_report_val['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': cnn_os_weighted_f1_val,\n",
        "    'auc': cnn_os_auc_score_val\n",
        "}"
      ],
      "metadata": {
        "id": "T95vvtxLTlNB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e6c06ba-c148-44f3-c67d-ec86741a1f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forma das sequências padded (Treino Original): (31962, 100)\n",
            "Forma das sequências padded (Teste Original): (17197, 100)\n",
            "\n",
            "Labels de Treino codificadas (Originais): [0 0 0 ... 0 1 0]\n",
            "Classes conhecidas pelo LabelEncoder: [0 1]\n",
            "\n",
            "Forma dos dados de treino (Pré-Oversampling): (25569, 100) (25569,)\n",
            "Forma dos dados de validação: (6393, 100) (6393,)\n",
            "Forma dos dados de teste FINAL: (17197, 100)\n",
            "\n",
            "Distribuição das classes no conjunto de treino (Pré-Oversampling):\n",
            "0    92.983691\n",
            "1     7.016309\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Aplicando Oversampling (SMOTE) no conjunto de treino...\n",
            "\n",
            "Forma dos dados de treino (Após Oversampling): (47550, 100) (47550,)\n",
            "\n",
            "Distribuição das classes no conjunto de treino (Após Oversampling):\n",
            "0    50.0\n",
            "1    50.0\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "==================================================\n",
            "Construindo e Treinando o Modelo CNN com Oversampling...\n",
            "\n",
            "Treinando CNN com dados oversampleados...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 39ms/step - accuracy: 0.6959 - loss: 0.5670 - val_accuracy: 0.8281 - val_loss: 0.4218\n",
            "Epoch 2/10\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 41ms/step - accuracy: 0.8687 - loss: 0.3090 - val_accuracy: 0.7866 - val_loss: 0.4938\n",
            "Epoch 3/10\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 43ms/step - accuracy: 0.9446 - loss: 0.1436 - val_accuracy: 0.7923 - val_loss: 0.6299\n",
            "Epoch 4/10\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 39ms/step - accuracy: 0.9682 - loss: 0.0818 - val_accuracy: 0.7877 - val_loss: 0.8746\n",
            "Epoch 5/10\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 40ms/step - accuracy: 0.9747 - loss: 0.0613 - val_accuracy: 0.7765 - val_loss: 0.9993\n",
            "Epoch 6/10\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 40ms/step - accuracy: 0.9801 - loss: 0.0510 - val_accuracy: 0.7918 - val_loss: 0.9323\n",
            "Epoch 7/10\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 39ms/step - accuracy: 0.9817 - loss: 0.0457 - val_accuracy: 0.7651 - val_loss: 1.2621\n",
            "Epoch 8/10\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 39ms/step - accuracy: 0.9826 - loss: 0.0433 - val_accuracy: 0.7626 - val_loss: 1.4689\n",
            "Epoch 9/10\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 39ms/step - accuracy: 0.9844 - loss: 0.0383 - val_accuracy: 0.8165 - val_loss: 1.1416\n",
            "Epoch 10/10\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 39ms/step - accuracy: 0.9848 - loss: 0.0356 - val_accuracy: 0.7640 - val_loss: 1.7791\n",
            "\n",
            "Resumo do Modelo CNN (Oversample):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m49,280\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,037,765\u001b[0m (15.40 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,037,765</span> (15.40 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,345,921\u001b[0m (5.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,345,921</span> (5.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,691,844\u001b[0m (10.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,691,844</span> (10.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando CNN (Oversample) no conjunto de validação...\n",
            "\n",
            "Acurácia da CNN (Oversample) no conjunto de validação: 0.7640\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
            "\n",
            "Avaliação completa da CNN (Oversample) no conjunto de validação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.77      0.86      5945\n",
            "           1       0.19      0.72      0.30       448\n",
            "\n",
            "    accuracy                           0.76      6393\n",
            "   macro avg       0.58      0.74      0.58      6393\n",
            "weighted avg       0.92      0.76      0.82      6393\n",
            "\n",
            "Weighted F1-score: 0.8189128757635127\n",
            "AUC: 0.8038445797789259\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- construção e treino do Modelo LSTM ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Construindo e Treinando o Modelo LSTM com Oversampling...\")\n",
        "\n",
        "# reutiliza parâmetros de embedding e sequência\n",
        "\n",
        "model_lstm_oversample = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    LSTM(128),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_lstm_oversample.compile(optimizer='adam',\n",
        "                              loss='binary_crossentropy',\n",
        "                              metrics=['accuracy'])\n",
        "\n",
        "epochs_lstm = 15\n",
        "batch_size_lstm = 32\n",
        "\n",
        "print(\"\\nTreinando LSTM com dados oversampleados...\")\n",
        "history_lstm_oversample = model_lstm_oversample.fit(X_train_dl, y_train_dl,\n",
        "                                                    epochs=epochs_lstm,\n",
        "                                                    batch_size=batch_size_lstm,\n",
        "                                                    validation_data=(X_val_dl, y_val_dl),\n",
        "                                                    verbose=1)\n",
        "\n",
        "print(\"\\nResumo do Modelo LSTM (Oversample):\")\n",
        "model_lstm_oversample.summary()\n",
        "\n",
        "\n",
        "# --- avaliação do modelo\n",
        "print(\"\\nAvaliando LSTM (Oversample) no conjunto de validação...\")\n",
        "loss_lstm_os_val, accuracy_lstm_os_val = model_lstm_oversample.evaluate(X_val_dl, y_val_dl, verbose=0)\n",
        "print(f\"\\nAcurácia da LSTM (Oversample) no conjunto de validação: {accuracy_lstm_os_val:.4f}\")\n",
        "\n",
        "lstm_os_predictions_proba_val = model_lstm_oversample.predict(X_val_dl)\n",
        "lstm_os_predictions_val = (lstm_os_predictions_proba_val >= 0.5).astype(int).flatten()\n",
        "\n",
        "\n",
        "print(\"\\nAvaliação completa da LSTM (Oversample) no conjunto de validação:\")\n",
        "print(classification_report(y_val_dl, lstm_os_predictions_val))\n",
        "\n",
        "lstm_os_weighted_f1_val = f1_score(y_val_dl, lstm_os_predictions_val, average='weighted')\n",
        "print(\"Weighted F1-score:\", lstm_os_weighted_f1_val)\n",
        "\n",
        "try:\n",
        "    lstm_os_auc_score_val = roc_auc_score(y_val_dl, lstm_os_predictions_proba_val)\n",
        "    print(\"AUC:\", lstm_os_auc_score_val)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    lstm_os_auc_score_val = None\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "# dicionário de resultados\n",
        "lstm_os_report_val = classification_report(y_val_dl, lstm_os_predictions_val, output_dict=True)\n",
        "results = {}\n",
        "results['LSTM (Embedding+Seq) Oversample Validation'] = {\n",
        "    'accuracy': accuracy_lstm_os_val,\n",
        "    'precision (macro)': lstm_os_report_val['macro avg']['precision'],\n",
        "    'recall (macro)': lstm_os_report_val['macro avg']['recall'],\n",
        "    'f1-score (macro)': lstm_os_report_val['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': lstm_os_weighted_f1_val,\n",
        "    'auc': lstm_os_auc_score_val\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dc8xPV1ewQD8",
        "outputId": "43be16ce-a40b-4e0c-9fe1-8861967f0c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Construindo e Treinando o Modelo LSTM com Oversampling...\n",
            "\n",
            "Treinando LSTM com dados oversampleados...\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 203ms/step - accuracy: 0.4965 - loss: 0.6946 - val_accuracy: 0.0701 - val_loss: 0.7013\n",
            "Epoch 2/15\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 197ms/step - accuracy: 0.5025 - loss: 0.6935 - val_accuracy: 0.0701 - val_loss: 0.6974\n",
            "Epoch 3/15\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 204ms/step - accuracy: 0.5034 - loss: 0.6932 - val_accuracy: 0.0701 - val_loss: 0.6998\n",
            "Epoch 4/15\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 198ms/step - accuracy: 0.4974 - loss: 0.6933 - val_accuracy: 0.0701 - val_loss: 0.7023\n",
            "Epoch 5/15\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 203ms/step - accuracy: 0.4954 - loss: 0.6934 - val_accuracy: 0.0701 - val_loss: 0.6968\n",
            "Epoch 6/15\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 206ms/step - accuracy: 0.5010 - loss: 0.6932 - val_accuracy: 0.9299 - val_loss: 0.6888\n",
            "Epoch 7/15\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 203ms/step - accuracy: 0.4956 - loss: 0.6932 - val_accuracy: 0.0701 - val_loss: 0.6960\n",
            "Epoch 8/15\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 198ms/step - accuracy: 0.4947 - loss: 0.6933 - val_accuracy: 0.0701 - val_loss: 0.6963\n",
            "Epoch 9/15\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 201ms/step - accuracy: 0.4983 - loss: 0.6932 - val_accuracy: 0.9299 - val_loss: 0.6858\n",
            "Epoch 10/15\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 197ms/step - accuracy: 0.4975 - loss: 0.6932 - val_accuracy: 0.0701 - val_loss: 0.6972\n",
            "Epoch 11/15\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 203ms/step - accuracy: 0.5021 - loss: 0.6932 - val_accuracy: 0.9299 - val_loss: 0.6896\n",
            "Epoch 12/15\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 201ms/step - accuracy: 0.5015 - loss: 0.6933 - val_accuracy: 0.0701 - val_loss: 0.6945\n",
            "Epoch 13/15\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 205ms/step - accuracy: 0.4962 - loss: 0.6932 - val_accuracy: 0.9299 - val_loss: 0.6911\n",
            "Epoch 14/15\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 205ms/step - accuracy: 0.4944 - loss: 0.6933 - val_accuracy: 0.9299 - val_loss: 0.6908\n",
            "Epoch 15/15\n",
            "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 197ms/step - accuracy: 0.5018 - loss: 0.6932 - val_accuracy: 0.0701 - val_loss: 0.6966\n",
            "\n",
            "Resumo do Modelo LSTM (Oversample):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,259,717\u001b[0m (16.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,259,717</span> (16.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,419,905\u001b[0m (5.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,419,905</span> (5.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,839,812\u001b[0m (10.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,839,812</span> (10.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando LSTM (Oversample) no conjunto de validação...\n",
            "\n",
            "Acurácia da LSTM (Oversample) no conjunto de validação: 0.0701\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step\n",
            "\n",
            "Avaliação completa da LSTM (Oversample) no conjunto de validação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      5945\n",
            "           1       0.07      1.00      0.13       448\n",
            "\n",
            "    accuracy                           0.07      6393\n",
            "   macro avg       0.04      0.50      0.07      6393\n",
            "weighted avg       0.00      0.07      0.01      6393\n",
            "\n",
            "Weighted F1-score: 0.0091782890094146\n",
            "AUC: 0.5000841042893187\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Undersampling**"
      ],
      "metadata": {
        "id": "EwRnENMusP7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como terceira estratégia de balanceamento, foi empregado o método de undersampling aleatório, utilizando a técnica **RandomUnderSampler** da biblioteca `imbalanced-learn`. Diferente do SMOTE, que gera novas amostras para as classes minoritárias, o undersampling reduz o número de instâncias da classe majoritária, removendo exemplos aleatórios até que todas as classes tenham o mesmo número de ocorrências.\n",
        "\n",
        "No experimento, o balanceamento foi aplicado apenas sobre o conjunto de treino, **igualando as classes 0 e 1 com 1.794 exemplos cada** — valor correspondente à classe originalmente menos representada. Essa abordagem visa mitigar o viés dos modelos em favor da classe dominante, porém com o custo potencial de perda de informação relevante.\n",
        "Apesar disso, ela permite a avaliação do impacto da redução de dados no desempenho de algoritmos como **Regressão Logística, Naive Bayes, SVM, Random Forest, LightGBM e redes neurais densas**, todos treinados com a base reamostrada, possibilitando uma comparação direta com os resultados obtidos nas estratégias anteriores de balanceamento."
      ],
      "metadata": {
        "id": "hGSJQ0yNeqv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importação de bibliotecas\n",
        "!pip install imbalanced-learn\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\nDistribuição das classes no treino TF-IDF (antes do undersampling):\")\n",
        "print(pd.Series(y_train).value_counts())\n",
        "\n",
        "# inicializa RandomUnderSampler\n",
        "# random_state para reprodutibilidade\n",
        "# sampling_strategy='auto' subamostra todas as classes majoritárias para igualar a classe minoritária\n",
        "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "# aplica Undersampling nos dados de treino\n",
        "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"\\nForma dos dados de treino TF-IDF após Undersampling:\", X_train_rus.shape, y_train_rus.shape)\n",
        "print(\"\\nDistribuição das classes no treino TF-IDF (depois do Undersampling):\")\n",
        "print(pd.Series(y_train_rus).value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88qg0xMBKb62",
        "outputId": "f31902b9-f678-462d-95e4-2c94c23c3550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "\n",
            "Distribuição das classes no treino TF-IDF (antes do undersampling):\n",
            "label\n",
            "0    23775\n",
            "1     1794\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Forma dos dados de treino TF-IDF após Undersampling: (3588, 5000) (3588,)\n",
            "\n",
            "Distribuição das classes no treino TF-IDF (depois do Undersampling):\n",
            "label\n",
            "0    1794\n",
            "1    1794\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Treinando modelos tradicionais COM UNDERSAMPLING...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 1. Regressão Logística (Undersampling)\n",
        "print(\"Treinando Regressão Logística com Undersampling-balanced data...\")\n",
        "lr_model_rus = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model_rus.fit(X_train_rus, y_train_rus)\n",
        "lr_predictions_rus = lr_model_rus.predict(X_test)\n",
        "lr_predictions_proba_rus = lr_model_rus.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação da Regressão Logística (Undersampling):\")\n",
        "print(classification_report(y_test, lr_predictions_rus))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lr_predictions_rus))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, lr_predictions_rus, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, lr_predictions_proba_rus))\n",
        "\n",
        "lr_report_rus = classification_report(y_test, lr_predictions_rus, output_dict=True)\n",
        "results['Logistic Regression (Undersample)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lr_predictions_rus),\n",
        "    'precision (macro)': lr_report_rus['macro avg']['precision'],\n",
        "    'recall (macro)': lr_report_rus['macro avg']['recall'],\n",
        "    'f1-score (macro)': lr_report_rus['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lr_predictions_rus, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, lr_predictions_proba_rus)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 2. Naive Bayes Multinomial (Undersampling)\n",
        "print(\"Treinando Naive Bayes Multinomial com Undersampling-balanced data...\")\n",
        "nb_model_rus = MultinomialNB()\n",
        "nb_model_rus.fit(X_train_rus, y_train_rus)\n",
        "nb_predictions_rus = nb_model_rus.predict(X_test)\n",
        "nb_predictions_proba_rus = nb_model_rus.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do Naive Bayes Multinomial (Undersampling):\")\n",
        "print(classification_report(y_test, nb_predictions_rus))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, nb_predictions_rus))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, nb_predictions_rus, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, nb_predictions_proba_rus))\n",
        "\n",
        "nb_report_rus = classification_report(y_test, nb_predictions_rus, output_dict=True)\n",
        "results['Multinomial NB (Undersample)'] = {\n",
        "    'accuracy': accuracy_score(y_test, nb_predictions_rus),\n",
        "    'precision (macro)': nb_report_rus['macro avg']['precision'],\n",
        "    'recall (macro)': nb_report_rus['macro avg']['recall'],\n",
        "    'f1-score (macro)': nb_report_rus['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, nb_predictions_rus, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, nb_predictions_proba_rus)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 3. Support Vector Machine (SVM) (Undersampling)\n",
        "print(\"Treinando SVM (Kernel Linear) com Undersampling-balanced data...\")\n",
        "svm_model_rus = SVC(kernel='linear', probability=True, random_state=42)\n",
        "svm_model_rus.fit(X_train_rus, y_train_rus)\n",
        "svm_predictions_rus = svm_model_rus.predict(X_test)\n",
        "svm_predictions_proba_rus = svm_model_rus.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do SVM (Undersampling):\")\n",
        "print(classification_report(y_test, svm_predictions_rus))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, svm_predictions_rus))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, svm_predictions_rus, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, svm_predictions_proba_rus))\n",
        "\n",
        "svm_report_rus = classification_report(y_test, svm_predictions_rus, output_dict=True)\n",
        "results['SVM (Undersample)'] = {\n",
        "    'accuracy': accuracy_score(y_test, svm_predictions_rus),\n",
        "    'precision (macro)': svm_report_rus['macro avg']['precision'],\n",
        "    'recall (macro)': svm_report_rus['macro avg']['recall'],\n",
        "    'f1-score (macro)': svm_report_rus['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, svm_predictions_rus, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, svm_predictions_proba_rus)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 4. Random Forest Classifier (Undersampling)\n",
        "print(\"Treinando Random Forest com Undersampling-balanced data...\")\n",
        "rf_model_rus = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model_rus.fit(X_train_rus, y_train_rus)\n",
        "rf_predictions_rus = rf_model_rus.predict(X_test)\n",
        "rf_predictions_proba_rus = rf_model_rus.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do Random Forest (Undersampling):\")\n",
        "print(classification_report(y_test, rf_predictions_rus))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, rf_predictions_rus))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, rf_predictions_rus, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, rf_predictions_proba_rus))\n",
        "\n",
        "rf_report_rus = classification_report(y_test, rf_predictions_rus, output_dict=True)\n",
        "results['Random Forest (Undersample)'] = {\n",
        "    'accuracy': accuracy_score(y_test, rf_predictions_rus),\n",
        "    'precision (macro)': rf_report_rus['macro avg']['precision'],\n",
        "    'recall (macro)': rf_report_rus['macro avg']['recall'],\n",
        "    'f1-score (macro)': rf_report_rus['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, rf_predictions_rus, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, rf_predictions_proba_rus)\n",
        "}\n",
        "\n",
        "print(\"\\nResultados acumulados (incluindo SMOTE e Undersampling) até agora:\")\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vsp4FHwKf6D",
        "outputId": "ab4f4aa4-80ca-476e-e2ee-663775f8e6b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Treinando modelos tradicionais COM UNDERSAMPLING...\n",
            "--------------------------------------------------\n",
            "Treinando Regressão Logística com Undersampling-balanced data...\n",
            "Avaliação da Regressão Logística (Undersampling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.85      0.91      5945\n",
            "           1       0.27      0.74      0.39       448\n",
            "\n",
            "    accuracy                           0.84      6393\n",
            "   macro avg       0.62      0.79      0.65      6393\n",
            "weighted avg       0.93      0.84      0.87      6393\n",
            "\n",
            "Acurácia:  0.8391991240419209\n",
            "Weighted F1-score: 0.8712894020616981\n",
            "AUC: 0.8765397843325725\n",
            "--------------------------------------------------\n",
            "Treinando Naive Bayes Multinomial com Undersampling-balanced data...\n",
            "Avaliação do Naive Bayes Multinomial (Undersampling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.81      0.89      5945\n",
            "           1       0.23      0.78      0.36       448\n",
            "\n",
            "    accuracy                           0.81      6393\n",
            "   macro avg       0.61      0.79      0.62      6393\n",
            "weighted avg       0.93      0.81      0.85      6393\n",
            "\n",
            "Acurácia:  0.8069763804160801\n",
            "Weighted F1-score: 0.8494902599109054\n",
            "AUC: 0.8799659077255797\n",
            "--------------------------------------------------\n",
            "Treinando SVM (Kernel Linear) com Undersampling-balanced data...\n",
            "Avaliação do SVM (Undersampling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.83      0.90      5945\n",
            "           1       0.24      0.75      0.37       448\n",
            "\n",
            "    accuracy                           0.82      6393\n",
            "   macro avg       0.61      0.79      0.63      6393\n",
            "weighted avg       0.93      0.82      0.86      6393\n",
            "\n",
            "Acurácia:  0.8201157516033162\n",
            "Weighted F1-score: 0.858200113114133\n",
            "AUC: 0.8728042773038568\n",
            "--------------------------------------------------\n",
            "Treinando Random Forest com Undersampling-balanced data...\n",
            "Avaliação do Random Forest (Undersampling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.84      0.90      5945\n",
            "           1       0.26      0.75      0.39       448\n",
            "\n",
            "    accuracy                           0.83      6393\n",
            "   macro avg       0.62      0.80      0.65      6393\n",
            "weighted avg       0.93      0.83      0.87      6393\n",
            "\n",
            "Acurácia:  0.8337243860472392\n",
            "Weighted F1-score: 0.8676157421786599\n",
            "AUC: 0.880761519283912\n",
            "\n",
            "Resultados acumulados (incluindo SMOTE e Undersampling) até agora:\n",
            "{'Logistic Regression (Undersample)': {'accuracy': 0.8391991240419209, 'precision (macro)': 0.6224560521003524, 'recall (macro)': 0.7948645695662622, 'f1-score (macro)': 0.6502364468302245, 'f1-score (weighted)': 0.8712894020616981, 'auc': np.float64(0.8765397843325725)}, 'Multinomial NB (Undersample)': {'accuracy': 0.8069763804160801, 'precision (macro)': 0.6072276810630024, 'recall (macro)': 0.7930185930553887, 'f1-score (macro)': 0.6234759508363847, 'f1-score (weighted)': 0.8494902599109054, 'auc': np.float64(0.8799659077255797)}, 'SVM (Undersample)': {'accuracy': 0.8201157516033162, 'precision (macro)': 0.6108318096066492, 'recall (macro)': 0.7866677805478793, 'f1-score (macro)': 0.631631135597942, 'f1-score (weighted)': 0.858200113114133, 'auc': np.float64(0.8728042773038568)}, 'Random Forest (Undersample)': {'accuracy': 0.8337243860472392, 'precision (macro)': 0.6195686413077718, 'recall (macro)': 0.7950168208578637, 'f1-score (macro)': 0.6455647473581354, 'f1-score (weighted)': 0.8676157421786599, 'auc': np.float64(0.880761519283912)}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Modelos Avançados com TF-IDF (Undersampling) ---\n",
        "\n",
        "# 5. LightGBM (Gradient Boosting) (Undersampling)\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Treinando LightGBM com Undersampling-balanced data...\")\n",
        "\n",
        "# o número de classes será 2\n",
        "num_classes_lgbm_rus = len(np.unique(y_train_rus))\n",
        "print(f\"Número de classes em y_train_rus para LightGBM: {num_classes_lgbm_rus}\")\n",
        "\n",
        "lgb_model_rus = lgb.LGBMClassifier(objective='binary', random_state=42)\n",
        "lgb_model_rus.fit(X_train_rus, y_train_rus)\n",
        "lgb_predictions_rus = lgb_model_rus.predict(X_test)\n",
        "lgb_predictions_proba_rus = lgb_model_rus.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Avaliação do LightGBM (Undersampling):\")\n",
        "print(classification_report(y_test, lgb_predictions_rus))\n",
        "print(\"Acurácia: \", accuracy_score(y_test, lgb_predictions_rus))\n",
        "print(\"Weighted F1-score:\", f1_score(y_test, lgb_predictions_rus, average='weighted'))\n",
        "print(\"AUC:\", roc_auc_score(y_test, lgb_predictions_proba_rus))\n",
        "\n",
        "lgb_report_rus = classification_report(y_test, lgb_predictions_rus, output_dict=True)\n",
        "results['LightGBM (Undersample)'] = {\n",
        "    'accuracy': accuracy_score(y_test, lgb_predictions_rus),\n",
        "    'precision (macro)': lgb_report_rus['macro avg']['precision'],\n",
        "    'recall (macro)': lgb_report_rus['macro avg']['recall'],\n",
        "    'f1-score (macro)': lgb_report_rus['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': f1_score(y_test, lgb_predictions_rus, average='weighted'),\n",
        "    'auc': roc_auc_score(y_test, lgb_predictions_proba_rus)\n",
        "}\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 6. Rede Neural Densa (MLP) com TensorFlow/Keras (Undersampling)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "print(\"Treinando Rede Neural Densa (MLP) com Undersampling-balanced data...\")\n",
        "\n",
        "# o número de classes será 2\n",
        "num_classes_mlp_rus = len(np.unique(y_train_rus))\n",
        "print(f\"Número de classes em y_train_rus para MLP: {num_classes_mlp_rus}\")\n",
        "\n",
        "mlp_model_rus = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train_rus.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid') # saída sigmoid para classificação binária\n",
        "])\n",
        "\n",
        "mlp_model_rus.compile(optimizer='adam',\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "epochs_mlp_rus = 20\n",
        "batch_size_mlp_rus = 64\n",
        "\n",
        "print(\"\\nTreinando MLP (Undersampling)...\")\n",
        "history_mlp_rus = mlp_model_rus.fit(X_train_rus, y_train_rus,\n",
        "                                  epochs=epochs_mlp_rus,\n",
        "                                  batch_size=batch_size_mlp_rus,\n",
        "                                  validation_data=(X_test, y_test),\n",
        "                                  verbose=1)\n",
        "\n",
        "print(\"\\nResumo do Modelo MLP (Undersampling):\")\n",
        "mlp_model_rus.summary()\n",
        "\n",
        "print(\"\\nAvaliando MLP (Undersampling) no conjunto de teste...\")\n",
        "loss_mlp_rus, accuracy_mlp_rus = mlp_model_rus.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da MLP (Undersampling) no conjunto de teste: {accuracy_mlp_rus:.4f}\")\n",
        "\n",
        "mlp_predictions_proba_positive_rus = mlp_model_rus.predict(X_test)\n",
        "mlp_predictions_rus = (mlp_predictions_proba_positive_rus >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da MLP (Undersampling):\")\n",
        "print(classification_report(y_test, mlp_predictions_rus))\n",
        "\n",
        "mlp_weighted_f1_rus = f1_score(y_test, mlp_predictions_rus, average='weighted')\n",
        "print(\"Weighted F1-score:\", mlp_weighted_f1_rus)\n",
        "\n",
        "try:\n",
        "    mlp_auc_score_rus = roc_auc_score(y_test, mlp_predictions_proba_positive_rus)\n",
        "    print(\"AUC:\", mlp_auc_score_rus)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    mlp_auc_score_rus = None\n",
        "\n",
        "mlp_report_rus = classification_report(y_test, mlp_predictions_rus, output_dict=True)\n",
        "results['MLP (Undersample)'] = {\n",
        "    'accuracy': accuracy_mlp_rus,\n",
        "    'precision (macro)': mlp_report_rus['macro avg']['precision'],\n",
        "    'recall (macro)': mlp_report_rus['macro avg']['recall'],\n",
        "    'f1-score (macro)': mlp_report_rus['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': mlp_weighted_f1_rus,\n",
        "    'auc': mlp_auc_score_rus\n",
        "}\n",
        "\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LDWGIjC9Kj0j",
        "outputId": "0f1e6284-f032-44d2-ae9e-2f2d027e869a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Treinando LightGBM com Undersampling-balanced data...\n",
            "Número de classes em y_train_rus para LightGBM: 2\n",
            "[LightGBM] [Info] Number of positive: 1794, number of negative: 1794\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006447 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2174\n",
            "[LightGBM] [Info] Number of data points in the train set: 3588, number of used features: 152\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avaliação do LightGBM (Undersampling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.82      0.89      5945\n",
            "           1       0.21      0.63      0.32       448\n",
            "\n",
            "    accuracy                           0.81      6393\n",
            "   macro avg       0.59      0.73      0.60      6393\n",
            "weighted avg       0.91      0.81      0.85      6393\n",
            "\n",
            "Acurácia:  0.8077584858438918\n",
            "Weighted F1-score: 0.8480370681632683\n",
            "AUC: 0.8112100880091314\n",
            "--------------------------------------------------\n",
            "Treinando Rede Neural Densa (MLP) com Undersampling-balanced data...\n",
            "Número de classes em y_train_rus para MLP: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando MLP (Undersampling)...\n",
            "Epoch 1/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 119ms/step - accuracy: 0.5803 - loss: 0.6859 - val_accuracy: 0.8541 - val_loss: 0.5883\n",
            "Epoch 2/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 63ms/step - accuracy: 0.8337 - loss: 0.4965 - val_accuracy: 0.7209 - val_loss: 0.5340\n",
            "Epoch 3/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.8889 - loss: 0.2685 - val_accuracy: 0.8223 - val_loss: 0.4271\n",
            "Epoch 4/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.9457 - loss: 0.1469 - val_accuracy: 0.8270 - val_loss: 0.4403\n",
            "Epoch 5/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.9667 - loss: 0.0996 - val_accuracy: 0.8143 - val_loss: 0.5135\n",
            "Epoch 6/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.9755 - loss: 0.0749 - val_accuracy: 0.8211 - val_loss: 0.5367\n",
            "Epoch 7/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 71ms/step - accuracy: 0.9815 - loss: 0.0569 - val_accuracy: 0.8084 - val_loss: 0.6122\n",
            "Epoch 8/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.9856 - loss: 0.0470 - val_accuracy: 0.7865 - val_loss: 0.7454\n",
            "Epoch 9/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9829 - loss: 0.0458 - val_accuracy: 0.8115 - val_loss: 0.6711\n",
            "Epoch 10/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.9878 - loss: 0.0368 - val_accuracy: 0.7816 - val_loss: 0.8298\n",
            "Epoch 11/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.9808 - loss: 0.0423 - val_accuracy: 0.8029 - val_loss: 0.7592\n",
            "Epoch 12/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9885 - loss: 0.0335 - val_accuracy: 0.7744 - val_loss: 0.9424\n",
            "Epoch 13/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9869 - loss: 0.0310 - val_accuracy: 0.7896 - val_loss: 0.8925\n",
            "Epoch 14/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.9841 - loss: 0.0268 - val_accuracy: 0.8087 - val_loss: 0.8136\n",
            "Epoch 15/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - accuracy: 0.9850 - loss: 0.0308 - val_accuracy: 0.7993 - val_loss: 0.8726\n",
            "Epoch 16/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.9892 - loss: 0.0283 - val_accuracy: 0.7857 - val_loss: 0.9634\n",
            "Epoch 17/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9892 - loss: 0.0254 - val_accuracy: 0.7888 - val_loss: 0.9668\n",
            "Epoch 18/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9905 - loss: 0.0218 - val_accuracy: 0.7918 - val_loss: 0.9666\n",
            "Epoch 19/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.9880 - loss: 0.0284 - val_accuracy: 0.8131 - val_loss: 0.8703\n",
            "Epoch 20/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9901 - loss: 0.0260 - val_accuracy: 0.8137 - val_loss: 0.8583\n",
            "\n",
            "Resumo do Modelo MLP (Undersampling):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,280,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,939,845\u001b[0m (15.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,939,845</span> (15.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,281\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,281</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,626,564\u001b[0m (10.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,626,564</span> (10.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando MLP (Undersampling) no conjunto de teste...\n",
            "\n",
            "Acurácia da MLP (Undersampling) no conjunto de teste: 0.8137\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\n",
            "Avaliação completa da MLP (Undersampling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.82      0.89      5945\n",
            "           1       0.24      0.75      0.36       448\n",
            "\n",
            "    accuracy                           0.81      6393\n",
            "   macro avg       0.61      0.79      0.63      6393\n",
            "weighted avg       0.93      0.81      0.85      6393\n",
            "\n",
            "Weighted F1-score: 0.8538643318115757\n",
            "AUC: 0.8694502433016941\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- undersampling para Modelos DL (CNN/LSTM) ---\n",
        "\n",
        "# reutiliza RandomUnderSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\nDistribuição das classes no treino DL (Pré-Undersampling):\")\n",
        "print(pd.Series(y_train_pre_oversample).value_counts(normalize=True) * 100)\n",
        "\n",
        "\n",
        "# inicializa RandomUnderSampler\n",
        "# random_state para reprodutibilidade\n",
        "rus_dl = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "X_train_undersample_dl, y_train_undersample_dl = rus_dl.fit_resample(X_train_pre_oversample, y_train_pre_oversample)\n",
        "print(\"\\nForma dos dados de treino DL após Undersampling:\", X_train_undersample_dl.shape, y_train_undersample_dl.shape)\n",
        "\n",
        "# verificação da distribuição\n",
        "print(\"\\nDistribuição das classes no treino DL (Após Undersampling):\")\n",
        "print(pd.Series(y_train_undersample_dl).value_counts(normalize=True) * 100)\n",
        "\n",
        "\n",
        "# --- variáveis para treino/validação do Modelos DL com Undersampling ---\n",
        "X_train_dl_rus = X_train_undersample_dl\n",
        "y_train_dl_rus = y_train_undersample_dl\n",
        "\n",
        "X_val_dl = X_val_dl\n",
        "y_val_dl = y_val_dl\n",
        "\n",
        "\n",
        "# --- construção e treino do Modelo CNN (agora usando dados subamostrados para treino) ---\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Construindo e Treinando o Modelo CNN com Undersampling...\")\n",
        "\n",
        "# reutiliza parâmetros de embedding e CNN da célula anterior\n",
        "# max_words, embedding_dim, max_sequence_length, num_filters, filter_sizes\n",
        "\n",
        "model_cnn_undersample = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    Conv1D(filters=num_filters, kernel_size=filter_sizes[0], activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_cnn_undersample.compile(optimizer='adam',\n",
        "                              loss='binary_crossentropy',\n",
        "                              metrics=['accuracy'])\n",
        "\n",
        "epochs_cnn_rus = 10\n",
        "batch_size_cnn_rus = 32\n",
        "\n",
        "print(\"\\nTreinando CNN com dados subamostrados...\")\n",
        "history_cnn_undersample = model_cnn_undersample.fit(X_train_dl_rus, y_train_dl_rus,\n",
        "                                                 epochs=epochs_cnn_rus,\n",
        "                                                 batch_size=batch_size_cnn_rus,\n",
        "                                                 validation_data=(X_val_dl, y_val_dl),\n",
        "                                                 verbose=1)\n",
        "\n",
        "print(\"\\nResumo do Modelo CNN (Undersample):\")\n",
        "model_cnn_undersample.summary()\n",
        "\n",
        "# --- avaliação do modelo ---\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "print(\"\\nAvaliando CNN (Undersample) no conjunto de validação...\")\n",
        "loss_cnn_us_val, accuracy_cnn_us_val = model_cnn_undersample.evaluate(X_val_dl, y_val_dl, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da CNN (Undersample) no conjunto de validação: {accuracy_cnn_us_val:.4f}\")\n",
        "\n",
        "cnn_us_predictions_proba_val = model_cnn_undersample.predict(X_val_dl)\n",
        "cnn_us_predictions_val = (cnn_us_predictions_proba_val >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da CNN (Undersample) no conjunto de validação:\")\n",
        "print(classification_report(y_val_dl, cnn_us_predictions_val))\n",
        "\n",
        "cnn_us_weighted_f1_val = f1_score(y_val_dl, cnn_us_predictions_val, average='weighted')\n",
        "print(\"Weighted F1-score:\", cnn_us_weighted_f1_val)\n",
        "\n",
        "try:\n",
        "    cnn_us_auc_score_val = roc_auc_score(y_val_dl, cnn_us_predictions_proba_val)\n",
        "    print(\"AUC:\", cnn_us_auc_score_val)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    cnn_us_auc_score_val = None\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "# dicionário de resultados\n",
        "cnn_us_report_val = classification_report(y_val_dl, cnn_us_predictions_val, output_dict=True)\n",
        "results['CNN (Embedding+Seq) Undersample Validation'] = {\n",
        "    'accuracy': accuracy_cnn_us_val,\n",
        "    'precision (macro)': cnn_us_report_val['macro avg']['precision'],\n",
        "    'recall (macro)': cnn_us_report_val['macro avg']['recall'],\n",
        "    'f1-score (macro)': cnn_us_report_val['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': cnn_us_weighted_f1_val,\n",
        "    'auc': cnn_us_auc_score_val\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_-Lx__HvKrGN",
        "outputId": "9bd25648-98a5-4bf2-9d1d-8602c3aed7c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Distribuição das classes no treino DL (Pré-Undersampling):\n",
            "0    92.983691\n",
            "1     7.016309\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Forma dos dados de treino DL após Undersampling: (3588, 100) (3588,)\n",
            "\n",
            "Distribuição das classes no treino DL (Após Undersampling):\n",
            "0    50.0\n",
            "1    50.0\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "==================================================\n",
            "Construindo e Treinando o Modelo CNN com Undersampling...\n",
            "\n",
            "Treinando CNN com dados subamostrados...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step - accuracy: 0.6025 - loss: 0.6598 - val_accuracy: 0.8577 - val_loss: 0.3910\n",
            "Epoch 2/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - accuracy: 0.8908 - loss: 0.2927 - val_accuracy: 0.7364 - val_loss: 0.5829\n",
            "Epoch 3/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - accuracy: 0.9633 - loss: 0.1160 - val_accuracy: 0.7511 - val_loss: 0.7000\n",
            "Epoch 4/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9790 - loss: 0.0624 - val_accuracy: 0.8437 - val_loss: 0.4979\n",
            "Epoch 5/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - accuracy: 0.9912 - loss: 0.0306 - val_accuracy: 0.8439 - val_loss: 0.5611\n",
            "Epoch 6/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - accuracy: 0.9944 - loss: 0.0203 - val_accuracy: 0.7954 - val_loss: 0.8465\n",
            "Epoch 7/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 49ms/step - accuracy: 0.9909 - loss: 0.0195 - val_accuracy: 0.7879 - val_loss: 0.9246\n",
            "Epoch 8/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 73ms/step - accuracy: 0.9924 - loss: 0.0160 - val_accuracy: 0.7976 - val_loss: 0.9638\n",
            "Epoch 9/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - accuracy: 0.9920 - loss: 0.0177 - val_accuracy: 0.7666 - val_loss: 1.1375\n",
            "Epoch 10/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 65ms/step - accuracy: 0.9952 - loss: 0.0124 - val_accuracy: 0.7840 - val_loss: 1.1188\n",
            "\n",
            "Resumo do Modelo CNN (Undersample):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m49,280\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_2          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_2          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,037,765\u001b[0m (15.40 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,037,765</span> (15.40 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,345,921\u001b[0m (5.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,345,921</span> (5.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,691,844\u001b[0m (10.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,691,844</span> (10.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando CNN (Undersample) no conjunto de validação...\n",
            "\n",
            "Acurácia da CNN (Undersample) no conjunto de validação: 0.7840\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
            "\n",
            "Avaliação completa da CNN (Undersample) no conjunto de validação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.78      0.87      5945\n",
            "           1       0.22      0.80      0.34       448\n",
            "\n",
            "    accuracy                           0.78      6393\n",
            "   macro avg       0.60      0.79      0.61      6393\n",
            "weighted avg       0.93      0.78      0.83      6393\n",
            "\n",
            "Weighted F1-score: 0.8337273615327018\n",
            "AUC: 0.8774765709479755\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- construção e treino do Modelo LSTM (agora usando dados subamostrados para treino) ---\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Construindo e Treinando o Modelo LSTM com Undersampling...\")\n",
        "\n",
        "# reutiliza parâmetros de embedding e LSTM da célula anterior\n",
        "# max_words, embedding_dim, max_sequence_length\n",
        "\n",
        "model_lstm_undersample = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    LSTM(128),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_lstm_undersample.compile(optimizer='adam',\n",
        "                               loss='binary_crossentropy',\n",
        "                               metrics=['accuracy'])\n",
        "\n",
        "epochs_lstm_rus = 15\n",
        "batch_size_lstm_rus = 32\n",
        "\n",
        "print(\"\\nTreinando LSTM com dados subamostrados...\")\n",
        "history_lstm_undersample = model_lstm_undersample.fit(X_train_dl_rus, y_train_dl_rus,\n",
        "                                                   epochs=epochs_lstm_rus,\n",
        "                                                   batch_size=batch_size_lstm_rus,\n",
        "                                                   validation_data=(X_val_dl, y_val_dl),\n",
        "                                                   verbose=1)\n",
        "\n",
        "print(\"\\nResumo do Modelo LSTM (Undersampling):\")\n",
        "model_lstm_undersample.summary()\n",
        "\n",
        "# --- avaliação do Modelo LSTM (Undersampling) ---\n",
        "print(\"\\nAvaliando LSTM (Undersampling) no conjunto de validação...\")\n",
        "loss_lstm_us_val, accuracy_lstm_us_val = model_lstm_undersample.evaluate(X_val_dl, y_val_dl, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia da LSTM (Undersampling) no conjunto de validação: {accuracy_lstm_us_val:.4f}\")\n",
        "\n",
        "lstm_us_predictions_proba_val = model_lstm_undersample.predict(X_val_dl)\n",
        "lstm_us_predictions_val = (lstm_us_predictions_proba_val >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nAvaliação completa da LSTM (Undersampling) no conjunto de validação:\")\n",
        "print(classification_report(y_val_dl, lstm_us_predictions_val))\n",
        "\n",
        "lstm_us_weighted_f1_val = f1_score(y_val_dl, lstm_us_predictions_val, average='weighted')\n",
        "print(\"Weighted F1-score:\", lstm_us_weighted_f1_val)\n",
        "\n",
        "try:\n",
        "    lstm_us_auc_score_val = roc_auc_score(y_val_dl, lstm_us_predictions_proba_val)\n",
        "    print(\"AUC:\", lstm_us_auc_score_val)\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")\n",
        "    lstm_us_auc_score_val = None\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "# dicionário de resultados\n",
        "lstm_us_report_val = classification_report(y_val_dl, lstm_us_predictions_val, output_dict=True)\n",
        "results['LSTM (Embedding+Seq) Undersample Validation'] = {\n",
        "    'accuracy': accuracy_lstm_us_val,\n",
        "    'precision (macro)': lstm_us_report_val['macro avg']['precision'],\n",
        "    'recall (macro)': lstm_us_report_val['macro avg']['recall'],\n",
        "    'f1-score (macro)': lstm_us_report_val['macro avg']['f1-score'],\n",
        "    'f1-score (weighted)': lstm_us_weighted_f1_val,\n",
        "    'auc': lstm_us_auc_score_val\n",
        "}\n",
        "\n",
        "print(\"\\nResultados Finais Acumulados:\")\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OzAy8SmkKtoG",
        "outputId": "e5f93d73-7d68-4db8-fd95-39ed1b0f0d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Construindo e Treinando o Modelo LSTM com Undersampling...\n",
            "\n",
            "Treinando LSTM com dados subamostrados...\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 316ms/step - accuracy: 0.4868 - loss: 0.6950 - val_accuracy: 0.0701 - val_loss: 0.7040\n",
            "Epoch 2/15\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 369ms/step - accuracy: 0.4975 - loss: 0.6953 - val_accuracy: 0.9299 - val_loss: 0.6852\n",
            "Epoch 3/15\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 365ms/step - accuracy: 0.4862 - loss: 0.6944 - val_accuracy: 0.9299 - val_loss: 0.6761\n",
            "Epoch 4/15\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 373ms/step - accuracy: 0.4940 - loss: 0.6937 - val_accuracy: 0.9299 - val_loss: 0.6849\n",
            "Epoch 5/15\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 378ms/step - accuracy: 0.4852 - loss: 0.6945 - val_accuracy: 0.9299 - val_loss: 0.6761\n",
            "Epoch 6/15\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 292ms/step - accuracy: 0.4905 - loss: 0.6938 - val_accuracy: 0.9299 - val_loss: 0.6848\n",
            "Epoch 7/15\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 378ms/step - accuracy: 0.4843 - loss: 0.6939 - val_accuracy: 0.0701 - val_loss: 0.7409\n",
            "Epoch 8/15\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 382ms/step - accuracy: 0.4837 - loss: 0.6951 - val_accuracy: 0.9299 - val_loss: 0.6905\n",
            "Epoch 9/15\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 300ms/step - accuracy: 0.5055 - loss: 0.6931 - val_accuracy: 0.9299 - val_loss: 0.6872\n",
            "Epoch 10/15\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 367ms/step - accuracy: 0.5066 - loss: 0.6937 - val_accuracy: 0.7611 - val_loss: 0.6902\n",
            "Epoch 11/15\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 378ms/step - accuracy: 0.5172 - loss: 0.6980 - val_accuracy: 0.2550 - val_loss: 0.6932\n",
            "Epoch 12/15\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 292ms/step - accuracy: 0.4950 - loss: 0.6937 - val_accuracy: 0.9299 - val_loss: 0.6826\n",
            "Epoch 13/15\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 379ms/step - accuracy: 0.4890 - loss: 0.6937 - val_accuracy: 0.9299 - val_loss: 0.6757\n",
            "Epoch 14/15\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 383ms/step - accuracy: 0.4939 - loss: 0.6933 - val_accuracy: 0.9299 - val_loss: 0.6802\n",
            "Epoch 15/15\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 376ms/step - accuracy: 0.4942 - loss: 0.6937 - val_accuracy: 0.9299 - val_loss: 0.6672\n",
            "\n",
            "Resumo do Modelo LSTM (Undersampling):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,259,717\u001b[0m (16.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,259,717</span> (16.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,419,905\u001b[0m (5.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,419,905</span> (5.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,839,812\u001b[0m (10.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,839,812</span> (10.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliando LSTM (Undersampling) no conjunto de validação...\n",
            "\n",
            "Acurácia da LSTM (Undersampling) no conjunto de validação: 0.9299\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step\n",
            "\n",
            "Avaliação completa da LSTM (Undersampling) no conjunto de validação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      1.00      0.96      5945\n",
            "           1       0.00      0.00      0.00       448\n",
            "\n",
            "    accuracy                           0.93      6393\n",
            "   macro avg       0.46      0.50      0.48      6393\n",
            "weighted avg       0.86      0.93      0.90      6393\n",
            "\n",
            "Weighted F1-score: 0.8961572925201334\n",
            "AUC: 0.5000227156674275\n",
            "==================================================\n",
            "\n",
            "Resultados Finais Acumulados:\n",
            "{'Logistic Regression (Undersample)': {'accuracy': 0.8391991240419209, 'precision (macro)': 0.6224560521003524, 'recall (macro)': 0.7948645695662622, 'f1-score (macro)': 0.6502364468302245, 'f1-score (weighted)': 0.8712894020616981, 'auc': np.float64(0.8765397843325725)}, 'Multinomial NB (Undersample)': {'accuracy': 0.8069763804160801, 'precision (macro)': 0.6072276810630024, 'recall (macro)': 0.7930185930553887, 'f1-score (macro)': 0.6234759508363847, 'f1-score (weighted)': 0.8494902599109054, 'auc': np.float64(0.8799659077255797)}, 'SVM (Undersample)': {'accuracy': 0.8201157516033162, 'precision (macro)': 0.6108318096066492, 'recall (macro)': 0.7866677805478793, 'f1-score (macro)': 0.631631135597942, 'f1-score (weighted)': 0.858200113114133, 'auc': np.float64(0.8728042773038568)}, 'Random Forest (Undersample)': {'accuracy': 0.8337243860472392, 'precision (macro)': 0.6195686413077718, 'recall (macro)': 0.7950168208578637, 'f1-score (macro)': 0.6455647473581354, 'f1-score (weighted)': 0.8676157421786599, 'auc': np.float64(0.880761519283912)}, 'LightGBM (Undersample)': {'accuracy': 0.8077584858438918, 'precision (macro)': 0.5886986715535558, 'recall (macro)': 0.7263612504505588, 'f1-score (macro)': 0.601750786728772, 'f1-score (weighted)': 0.8480370681632683, 'auc': np.float64(0.8112100880091314)}, 'MLP (Undersample)': {'accuracy': 0.8137024641036987, 'precision (macro)': 0.6080404573124858, 'recall (macro)': 0.7863154061035684, 'f1-score (macro)': 0.6265011388089134, 'f1-score (weighted)': 0.8538643318115757, 'auc': np.float64(0.8694502433016941)}, 'CNN (Embedding+Seq) Undersample Validation': {'accuracy': 0.7839824557304382, 'precision (macro)': 0.599337760609791, 'recall (macro)': 0.792006901057311, 'f1-score (macro)': 0.6064226157500889, 'f1-score (weighted)': 0.8337273615327018, 'auc': np.float64(0.8774765709479755)}, 'LSTM (Embedding+Seq) Undersample Validation': {'accuracy': 0.9299233555793762, 'precision (macro)': 0.4649616768340372, 'recall (macro)': 0.5, 'f1-score (macro)': 0.4818447074080078, 'f1-score (weighted)': 0.8961572925201334, 'auc': np.float64(0.5000227156674275)}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIzkmiNXCC1L"
      },
      "source": [
        "### **Resultados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVK1y6MJCHoq"
      },
      "source": [
        "#### **Dataset Original**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Modelo                        | Acurácia | F1-score Ponderado | AUC (ponderado, OVR)             |\n",
        "| ----------------------------- | -------- | ------------------ | -------------------------------- |\n",
        "| Regressão Logística           | 0.9449   | 0.9295             | 0.8940                           |\n",
        "| Naive Bayes Multinomial       | 0.9457   | 0.9300             | 0.8659                           |\n",
        "| SVM (Kernel Linear)           | 0.9510   | 0.9409             | 0.8591                           |\n",
        "| Random Forest                 | 0.9550   | 0.9504             | 0.8849                           |\n",
        "| LightGBM                      | 0.9476   | 0.9378             | 0.8625                           |\n",
        "| MLP (Perceptron Multicamadas) | 0.9500   | 0.9439             | 0.8656                           |\n",
        "| CNN (Convolutional NN)        | 0.9400   | 0.9409             | 0.8631                           |\n",
        "| LSTM                          | 0.9300   | 0.8962             | 0.5004 *(classe 1 não prevista)* |\n"
      ],
      "metadata": {
        "id": "RUXwatekfG13"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP75jgq3CM-h"
      },
      "source": [
        "#### **Dataset Balanceado**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Quadro Resumo dos Modelos com `class_weight`**"
      ],
      "metadata": {
        "id": "-0zOgBw1sqAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Modelo                        | Acurácia | F1-score Ponderado    | AUC (ponderado, OVR)             |\n",
        "| ----------------------------- | -------- | --------------------- | -------------------------------- |\n",
        "| Regressão Logística           | 0.8911   | 0.9068                | 0.8984                           |\n",
        "| Naive Bayes Multinomial (\\*)  | 0.9457   | 0.9300                | 0.8659                           |\n",
        "| SVM (Kernel Linear)           | 0.8839   | 0.9013                | 0.8862                           |\n",
        "| Random Forest                 | 0.9554   | 0.9506                | 0.8904                           |\n",
        "| LightGBM                      | 0.8781   | 0.8964                | 0.8658                           |\n",
        "| MLP (Perceptron Multicamadas) | 0.9400   | 0.9394                | 0.8606                           |\n",
        "| CNN (Convolutional NN)        | 0.8900   | 0.9063  | 0.8799           |\n",
        "| LSTM                          | 0.0700   | 0.0092                | 0.5000 *(classe 0 não prevista)* |\n"
      ],
      "metadata": {
        "id": "Mb-Vx95FmC9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Quadro Resumo dos Modelos com Oversampling**\n"
      ],
      "metadata": {
        "id": "VSk7ikv1stuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Modelo                        | Acurácia | F1-score Ponderado | AUC (ponderado, OVR)             |\n",
        "| ----------------------------- | -------- | ------------------ | -------------------------------- |\n",
        "| Regressão Logística           | 0.8414   | 0.8723             | 0.8772                           |\n",
        "| Naive Bayes Multinomial       | 0.8816   | 0.9003             | 0.8926                           |\n",
        "| SVM (Kernel Linear)           | 0.8430   | 0.8729             | 0.8542                           |\n",
        "| Random Forest                 | 0.9162   | 0.9228             | 0.8840                           |\n",
        "| LightGBM                      | 0.8997   | 0.9105             | 0.8556                           |\n",
        "| MLP (Perceptron Multicamadas) | 0.9500   | 0.9461             | 0.8662                           |\n",
        "| CNN (Convolutional NN)        | 0.7600   | 0.8189             | 0.8038                           |\n",
        "| LSTM                          | 0.0700   | 0.0092             | 0.5001 *(classe 0 não prevista)* |\n"
      ],
      "metadata": {
        "id": "E_gDs8Ywjb18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Quadro Resumo dos Modelos com Undersampling**"
      ],
      "metadata": {
        "id": "9EnkDYwCs0X0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Modelo                        | Acurácia | F1-score Ponderado | AUC (ponderado, OVR)             |\n",
        "| ----------------------------- | -------- | ------------------ | -------------------------------- |\n",
        "| Regressão Logística           | 0.8392   | 0.8713             | 0.8765                           |\n",
        "| Naive Bayes Multinomial       | 0.8070   | 0.8495             | 0.8800                           |\n",
        "| SVM (Kernel Linear)           | 0.8201   | 0.8582             | 0.8728                           |\n",
        "| Random Forest                 | 0.8337   | 0.8676             | 0.8808                           |\n",
        "| LightGBM                      | 0.8078   | 0.8480             | 0.8112                           |\n",
        "| MLP (Perceptron Multicamadas) | 0.8100   | 0.8539             | 0.8695                           |\n",
        "| CNN (Convolutional NN)        | 0.7800   | 0.8337             | 0.8775                           |\n",
        "| LSTM                          | 0.9300   | 0.8962             | 0.5000 *(classe 1 não prevista)* |\n"
      ],
      "metadata": {
        "id": "GbfU59iqkBs3"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOkoy82K3nFaD+1qVY9f08W",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}